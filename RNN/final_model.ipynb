{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import TensorflowDataset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle('extracted_features\\\\features_test.pkl')\n",
    "test = tf.data.Dataset.from_tensor_slices((test_data, np.zeros(len(test_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_silence_data = pd.read_pickle('extracted_features\\\\features_test_silence.pkl')\n",
    "# test_silence = tf.data.Dataset.from_tensor_slices((test_silence_data, np.zeros(len(test_silence_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>2.370784</td>\n",
       "      <td>[2.370784044265747]</td>\n",
       "      <td>0.333063</td>\n",
       "      <td>[0.3330628573894501]</td>\n",
       "      <td>1.522492</td>\n",
       "      <td>[1.5224920511245728]</td>\n",
       "      <td>0.5584</td>\n",
       "      <td>[0.5583995580673218]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.660897</td>\n",
       "      <td>[2.427072286605835, 1.4517465829849243, 1.1519...</td>\n",
       "      <td>0.807175</td>\n",
       "      <td>[0.31317439675331116, 0.581358015537262, 0.668...</td>\n",
       "      <td>0.76055</td>\n",
       "      <td>[1.5653058290481567, 1.1923205852508545, 1.008...</td>\n",
       "      <td>0.795528</td>\n",
       "      <td>[0.546190083026886, 0.655339777469635, 0.71697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.600205</td>\n",
       "      <td>[2.488501787185669, 1.461273193359375, 1.12221...</td>\n",
       "      <td>0.823559</td>\n",
       "      <td>[0.30050238966941833, 0.5798560380935669, 0.67...</td>\n",
       "      <td>0.765887</td>\n",
       "      <td>[1.6190937757492065, 1.1751519441604614, 1.014...</td>\n",
       "      <td>0.790527</td>\n",
       "      <td>[0.5310385227203369, 0.6641659140586853, 0.712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.459687</td>\n",
       "      <td>[2.3511674404144287, 1.395878791809082, 1.1104...</td>\n",
       "      <td>0.865408</td>\n",
       "      <td>[0.3353935480117798, 0.5998998880386353, 0.681...</td>\n",
       "      <td>0.708278</td>\n",
       "      <td>[1.5098788738250732, 1.139453411102295, 1.0019...</td>\n",
       "      <td>0.81627</td>\n",
       "      <td>[0.559723436832428, 0.6802000403404236, 0.7131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.434148</td>\n",
       "      <td>[2.4646902084350586, 1.4764158725738525, 1.160...</td>\n",
       "      <td>0.870311</td>\n",
       "      <td>[0.3031783699989319, 0.5757989287376404, 0.666...</td>\n",
       "      <td>0.736218</td>\n",
       "      <td>[1.6360548734664917, 1.2466334104537964, 1.031...</td>\n",
       "      <td>0.816564</td>\n",
       "      <td>[0.5272138714790344, 0.6362165212631226, 0.707...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.922474</td>\n",
       "      <td>[2.8702659606933594, 1.9048563241958618, 1.527...</td>\n",
       "      <td>0.74233</td>\n",
       "      <td>[0.19589799642562866, 0.4504428207874298, 0.56...</td>\n",
       "      <td>0.943816</td>\n",
       "      <td>[2.0041897296905518, 1.4960237741470337, 1.257...</td>\n",
       "      <td>0.755075</td>\n",
       "      <td>[0.41791704297065735, 0.5720800161361694, 0.64...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.862444</td>\n",
       "      <td>[3.029785394668579, 1.9737733602523804, 1.5499...</td>\n",
       "      <td>0.757747</td>\n",
       "      <td>[0.16844776272773743, 0.4341798722743988, 0.55...</td>\n",
       "      <td>0.950969</td>\n",
       "      <td>[2.0705583095550537, 1.57509183883667, 1.30928...</td>\n",
       "      <td>0.755663</td>\n",
       "      <td>[0.38849660754203796, 0.5460429787635803, 0.62...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.695517</td>\n",
       "      <td>[2.872177839279175, 1.8928440809249878, 1.5408...</td>\n",
       "      <td>0.806105</td>\n",
       "      <td>[0.19755537807941437, 0.45203113555908203, 0.5...</td>\n",
       "      <td>0.809129</td>\n",
       "      <td>[1.9758355617523193, 1.4280952215194702, 1.273...</td>\n",
       "      <td>0.794793</td>\n",
       "      <td>[0.42586055397987366, 0.5837010741233826, 0.64...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.647044</td>\n",
       "      <td>[3.02612042427063, 1.9941836595535278, 1.55550...</td>\n",
       "      <td>0.815583</td>\n",
       "      <td>[0.1640971601009369, 0.42404571175575256, 0.55...</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>[2.168159008026123, 1.5454410314559937, 1.2715...</td>\n",
       "      <td>0.788026</td>\n",
       "      <td>[0.3620182275772095, 0.5489850044250488, 0.644...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3         4   \\\n",
       "0  64  0.2   1  32  2.370784   \n",
       "1  64  0.2  10  32  0.660897   \n",
       "2  64  0.2  10  64  0.600205   \n",
       "3  64  0.2  20  32  0.459687   \n",
       "4  64  0.2  20  64  0.434148   \n",
       "5  64  0.4  10  32  0.922474   \n",
       "6  64  0.4  10  64  0.862444   \n",
       "7  64  0.4  20  32  0.695517   \n",
       "8  64  0.4  20  64  0.647044   \n",
       "\n",
       "                                                  5         6   \\\n",
       "0                                [2.370784044265747]  0.333063   \n",
       "1  [2.427072286605835, 1.4517465829849243, 1.1519...  0.807175   \n",
       "2  [2.488501787185669, 1.461273193359375, 1.12221...  0.823559   \n",
       "3  [2.3511674404144287, 1.395878791809082, 1.1104...  0.865408   \n",
       "4  [2.4646902084350586, 1.4764158725738525, 1.160...  0.870311   \n",
       "5  [2.8702659606933594, 1.9048563241958618, 1.527...   0.74233   \n",
       "6  [3.029785394668579, 1.9737733602523804, 1.5499...  0.757747   \n",
       "7  [2.872177839279175, 1.8928440809249878, 1.5408...  0.806105   \n",
       "8  [3.02612042427063, 1.9941836595535278, 1.55550...  0.815583   \n",
       "\n",
       "                                                  7         8   \\\n",
       "0                               [0.3330628573894501]  1.522492   \n",
       "1  [0.31317439675331116, 0.581358015537262, 0.668...   0.76055   \n",
       "2  [0.30050238966941833, 0.5798560380935669, 0.67...  0.765887   \n",
       "3  [0.3353935480117798, 0.5998998880386353, 0.681...  0.708278   \n",
       "4  [0.3031783699989319, 0.5757989287376404, 0.666...  0.736218   \n",
       "5  [0.19589799642562866, 0.4504428207874298, 0.56...  0.943816   \n",
       "6  [0.16844776272773743, 0.4341798722743988, 0.55...  0.950969   \n",
       "7  [0.19755537807941437, 0.45203113555908203, 0.5...  0.809129   \n",
       "8  [0.1640971601009369, 0.42404571175575256, 0.55...  0.829114   \n",
       "\n",
       "                                                  9         10  \\\n",
       "0                               [1.5224920511245728]    0.5584   \n",
       "1  [1.5653058290481567, 1.1923205852508545, 1.008...  0.795528   \n",
       "2  [1.6190937757492065, 1.1751519441604614, 1.014...  0.790527   \n",
       "3  [1.5098788738250732, 1.139453411102295, 1.0019...   0.81627   \n",
       "4  [1.6360548734664917, 1.2466334104537964, 1.031...  0.816564   \n",
       "5  [2.0041897296905518, 1.4960237741470337, 1.257...  0.755075   \n",
       "6  [2.0705583095550537, 1.57509183883667, 1.30928...  0.755663   \n",
       "7  [1.9758355617523193, 1.4280952215194702, 1.273...  0.794793   \n",
       "8  [2.168159008026123, 1.5454410314559937, 1.2715...  0.788026   \n",
       "\n",
       "                                                  11  \n",
       "0                               [0.5583995580673218]  \n",
       "1  [0.546190083026886, 0.655339777469635, 0.71697...  \n",
       "2  [0.5310385227203369, 0.6641659140586853, 0.712...  \n",
       "3  [0.559723436832428, 0.6802000403404236, 0.7131...  \n",
       "4  [0.5272138714790344, 0.6362165212631226, 0.707...  \n",
       "5  [0.41791704297065735, 0.5720800161361694, 0.64...  \n",
       "6  [0.38849660754203796, 0.5460429787635803, 0.62...  \n",
       "7  [0.42586055397987366, 0.5837010741233826, 0.64...  \n",
       "8  [0.3620182275772095, 0.5489850044250488, 0.644...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('results/model_lstm_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.020137</td>\n",
       "      <td>2.234147</td>\n",
       "      <td>0.694923</td>\n",
       "      <td>0.695544</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>1.408424</td>\n",
       "      <td>0.765225</td>\n",
       "      <td>0.765225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.835371</td>\n",
       "      <td>2.106825</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.670023</td>\n",
       "      <td>1.220319</td>\n",
       "      <td>0.798176</td>\n",
       "      <td>0.798176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.836061</td>\n",
       "      <td>2.168659</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.697845</td>\n",
       "      <td>1.250656</td>\n",
       "      <td>0.782142</td>\n",
       "      <td>0.782142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.714789</td>\n",
       "      <td>2.103729</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.629499</td>\n",
       "      <td>1.196326</td>\n",
       "      <td>0.812298</td>\n",
       "      <td>0.820683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.258716</td>\n",
       "      <td>2.888041</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>1.075352</td>\n",
       "      <td>1.915189</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.694322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.282760</td>\n",
       "      <td>2.965580</td>\n",
       "      <td>0.632391</td>\n",
       "      <td>0.641438</td>\n",
       "      <td>1.102142</td>\n",
       "      <td>1.910666</td>\n",
       "      <td>0.684025</td>\n",
       "      <td>0.708444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.257138</td>\n",
       "      <td>3.018563</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>1.939173</td>\n",
       "      <td>0.722124</td>\n",
       "      <td>0.722124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.190626</td>\n",
       "      <td>3.028895</td>\n",
       "      <td>0.648482</td>\n",
       "      <td>0.671771</td>\n",
       "      <td>0.864657</td>\n",
       "      <td>1.711817</td>\n",
       "      <td>0.743307</td>\n",
       "      <td>0.754340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>2.257917</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.818004</td>\n",
       "      <td>1.401505</td>\n",
       "      <td>0.757134</td>\n",
       "      <td>0.758753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.772999</td>\n",
       "      <td>2.242363</td>\n",
       "      <td>0.769142</td>\n",
       "      <td>0.770644</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>1.372575</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>0.798029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>2.232953</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>1.369754</td>\n",
       "      <td>0.789056</td>\n",
       "      <td>0.789056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.765530</td>\n",
       "      <td>2.189961</td>\n",
       "      <td>0.771058</td>\n",
       "      <td>0.783022</td>\n",
       "      <td>0.631657</td>\n",
       "      <td>1.512975</td>\n",
       "      <td>0.808179</td>\n",
       "      <td>0.808620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.424297</td>\n",
       "      <td>3.053465</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>0.360106</td>\n",
       "      <td>0.528391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.165354</td>\n",
       "      <td>3.081251</td>\n",
       "      <td>0.667938</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>5.661652</td>\n",
       "      <td>6.137986</td>\n",
       "      <td>0.366578</td>\n",
       "      <td>0.633863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.248828</td>\n",
       "      <td>3.112274</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>1.113379</td>\n",
       "      <td>2.413799</td>\n",
       "      <td>0.676228</td>\n",
       "      <td>0.676228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.029160</td>\n",
       "      <td>3.145577</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>1.156267</td>\n",
       "      <td>2.116711</td>\n",
       "      <td>0.716681</td>\n",
       "      <td>0.716681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.941052</td>\n",
       "      <td>2.091297</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>1.723114</td>\n",
       "      <td>0.747426</td>\n",
       "      <td>0.768020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>2.084395</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.697317</td>\n",
       "      <td>1.354331</td>\n",
       "      <td>0.786996</td>\n",
       "      <td>0.813180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.825751</td>\n",
       "      <td>2.136443</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.662067</td>\n",
       "      <td>1.199146</td>\n",
       "      <td>0.796705</td>\n",
       "      <td>0.796705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>2.120008</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.790722</td>\n",
       "      <td>0.600954</td>\n",
       "      <td>1.342593</td>\n",
       "      <td>0.812886</td>\n",
       "      <td>0.812886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.353759</td>\n",
       "      <td>3.022973</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>1.023833</td>\n",
       "      <td>2.236273</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_heads  num_layers  dropout_rate  epoch  batch      loss  loss_max  \\\n",
       "0         2.0         1.0           0.2   10.0   32.0  1.020137  2.234147   \n",
       "1         2.0         1.0           0.2   20.0   32.0  0.835371  2.106825   \n",
       "2         2.0         1.0           0.2   10.0   64.0  0.836061  2.168659   \n",
       "3         2.0         1.0           0.2   20.0   64.0  0.714789  2.103729   \n",
       "4         2.0         1.0           0.4   10.0   32.0  1.258716  2.888041   \n",
       "5         2.0         1.0           0.4   20.0   32.0  1.282760  2.965580   \n",
       "6         2.0         1.0           0.4   10.0   64.0  1.257138  3.018563   \n",
       "7         2.0         1.0           0.4   20.0   64.0  1.190626  3.028895   \n",
       "8         2.0         2.0           0.2   10.0   32.0  0.954365  2.257917   \n",
       "9         2.0         2.0           0.2   20.0   32.0  0.772999  2.242363   \n",
       "10        2.0         2.0           0.2   10.0   64.0  0.864500  2.232953   \n",
       "11        2.0         2.0           0.2   20.0   64.0  0.765530  2.189961   \n",
       "12        2.0         2.0           0.4   10.0   32.0  1.424297  3.053465   \n",
       "13        2.0         2.0           0.4   20.0   32.0  1.165354  3.081251   \n",
       "14        2.0         2.0           0.4   10.0   64.0  1.248828  3.112274   \n",
       "15        2.0         2.0           0.4   20.0   64.0  1.029160  3.145577   \n",
       "16        4.0         1.0           0.2   10.0   32.0  0.941052  2.091297   \n",
       "17        4.0         1.0           0.2   20.0   32.0  0.723400  2.084395   \n",
       "18        4.0         1.0           0.2   10.0   64.0  0.825751  2.136443   \n",
       "19        4.0         1.0           0.2   20.0   64.0  0.690125  2.120008   \n",
       "20        4.0         1.0           0.4   10.0   32.0  1.353759  3.022973   \n",
       "\n",
       "    accuracy  accuracy_max  val_loss  val_loss_max  val_accuracy  \\\n",
       "0   0.694923      0.695544  0.749380      1.408424      0.765225   \n",
       "1   0.752862      0.752862  0.670023      1.220319      0.798176   \n",
       "2   0.748770      0.748770  0.697845      1.250656      0.782142   \n",
       "3   0.783126      0.783126  0.629499      1.196326      0.812298   \n",
       "4   0.630682      0.630682  1.075352      1.915189      0.681083   \n",
       "5   0.632391      0.641438  1.102142      1.910666      0.684025   \n",
       "6   0.627350      0.627350  0.929122      1.939173      0.722124   \n",
       "7   0.648482      0.671771  0.864657      1.711817      0.743307   \n",
       "8   0.715571      0.715571  0.818004      1.401505      0.757134   \n",
       "9   0.769142      0.770644  0.697234      1.372575      0.789938   \n",
       "10  0.741070      0.741070  0.686920      1.369754      0.789056   \n",
       "11  0.771058      0.783022  0.631657      1.512975      0.808179   \n",
       "12  0.588350      0.588350  4.966305      4.966305      0.360106   \n",
       "13  0.667938      0.683632  5.661652      6.137986      0.366578   \n",
       "14  0.634100      0.634100  1.113379      2.413799      0.676228   \n",
       "15  0.698030      0.698030  1.156267      2.116711      0.716681   \n",
       "16  0.718885      0.718885  0.824226      1.723114      0.747426   \n",
       "17  0.785215      0.785215  0.697317      1.354331      0.786996   \n",
       "18  0.751774      0.751774  0.662067      1.199146      0.796705   \n",
       "19  0.790032      0.790722  0.600954      1.342593      0.812886   \n",
       "20  0.602645      0.602645  1.023833      2.236273      0.696970   \n",
       "\n",
       "    val_accuracy_max  \n",
       "0           0.765225  \n",
       "1           0.798176  \n",
       "2           0.782142  \n",
       "3           0.820683  \n",
       "4           0.694322  \n",
       "5           0.708444  \n",
       "6           0.722124  \n",
       "7           0.754340  \n",
       "8           0.758753  \n",
       "9           0.798029  \n",
       "10          0.789056  \n",
       "11          0.808620  \n",
       "12          0.528391  \n",
       "13          0.633863  \n",
       "14          0.676228  \n",
       "15          0.716681  \n",
       "16          0.768020  \n",
       "17          0.813180  \n",
       "18          0.796705  \n",
       "19          0.812886  \n",
       "20          0.696970  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('results/model_transformer_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.746932</td>\n",
       "      <td>[2.665213108062744, 1.6146924495697021, 1.2631...</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>[0.2528011202812195, 0.5191202163696289, 0.629...</td>\n",
       "      <td>0.765207</td>\n",
       "      <td>[1.9333562850952148, 1.2438899278640747, 1.199...</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>[0.447925865650177, 0.6325389742851257, 0.6531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>[2.7639663219451904, 1.5994480848312378, 1.273...</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>[0.23180773854255676, 0.521537184715271, 0.624...</td>\n",
       "      <td>0.747728</td>\n",
       "      <td>[1.6140880584716797, 1.2849770784378052, 1.108...</td>\n",
       "      <td>0.789497</td>\n",
       "      <td>[0.5142688751220703, 0.610914945602417, 0.6681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.564152</td>\n",
       "      <td>[2.6195595264434814, 1.5511832237243652, 1.253...</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>[0.26476529240608215, 0.5392158627510071, 0.63...</td>\n",
       "      <td>0.648608</td>\n",
       "      <td>[1.784652590751648, 1.3517918586730957, 1.0938...</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>[0.44851428270339966, 0.596057653427124, 0.689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.538738</td>\n",
       "      <td>[2.697845935821533, 1.6010715961456299, 1.2770...</td>\n",
       "      <td>0.842222</td>\n",
       "      <td>[0.2499525249004364, 0.5225730538368225, 0.621...</td>\n",
       "      <td>0.641245</td>\n",
       "      <td>[1.5733102560043335, 1.1619383096694946, 1.054...</td>\n",
       "      <td>0.818035</td>\n",
       "      <td>[0.5483965873718262, 0.6631362438201904, 0.691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1.121856</td>\n",
       "      <td>[3.5523223876953125, 2.7618539333343506, 2.129...</td>\n",
       "      <td>0.687948</td>\n",
       "      <td>[0.052138183265924454, 0.1850905567407608, 0.3...</td>\n",
       "      <td>2.346172</td>\n",
       "      <td>[3.1456222534179688, 2.78471302986145, 2.16078...</td>\n",
       "      <td>0.31524</td>\n",
       "      <td>[0.08046483993530273, 0.10311856120824814, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1.185146</td>\n",
       "      <td>[3.6519148349761963, 3.016111135482788, 2.6201...</td>\n",
       "      <td>0.668422</td>\n",
       "      <td>[0.04808107390999794, 0.12409578263759613, 0.2...</td>\n",
       "      <td>2.227695</td>\n",
       "      <td>[3.3813705444335938, 3.035633087158203, 2.6361...</td>\n",
       "      <td>0.30759</td>\n",
       "      <td>[0.06663724780082703, 0.09546925872564316, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3         4   \\\n",
       "0  64  0.2  10  32  0.746932   \n",
       "1  64  0.2  10  64  0.722944   \n",
       "2  64  0.2  20  32  0.564152   \n",
       "3  64  0.2  20  64  0.538738   \n",
       "4  64  0.4  10  32  1.121856   \n",
       "5  64  0.4  10  64  1.185146   \n",
       "\n",
       "                                                  5         6   \\\n",
       "0  [2.665213108062744, 1.6146924495697021, 1.2631...  0.785215   \n",
       "1  [2.7639663219451904, 1.5994480848312378, 1.273...  0.791395   \n",
       "2  [2.6195595264434814, 1.5511832237243652, 1.253...  0.838389   \n",
       "3  [2.697845935821533, 1.6010715961456299, 1.2770...  0.842222   \n",
       "4  [3.5523223876953125, 2.7618539333343506, 2.129...  0.687948   \n",
       "5  [3.6519148349761963, 3.016111135482788, 2.6201...  0.668422   \n",
       "\n",
       "                                                  7         8   \\\n",
       "0  [0.2528011202812195, 0.5191202163696289, 0.629...  0.765207   \n",
       "1  [0.23180773854255676, 0.521537184715271, 0.624...  0.747728   \n",
       "2  [0.26476529240608215, 0.5392158627510071, 0.63...  0.648608   \n",
       "3  [0.2499525249004364, 0.5225730538368225, 0.621...  0.641245   \n",
       "4  [0.052138183265924454, 0.1850905567407608, 0.3...  2.346172   \n",
       "5  [0.04808107390999794, 0.12409578263759613, 0.2...  2.227695   \n",
       "\n",
       "                                                  9         10  \\\n",
       "0  [1.9333562850952148, 1.2438899278640747, 1.199...  0.789938   \n",
       "1  [1.6140880584716797, 1.2849770784378052, 1.108...  0.789497   \n",
       "2  [1.784652590751648, 1.3517918586730957, 1.0938...  0.830097   \n",
       "3  [1.5733102560043335, 1.1619383096694946, 1.054...  0.818035   \n",
       "4  [3.1456222534179688, 2.78471302986145, 2.16078...   0.31524   \n",
       "5  [3.3813705444335938, 3.035633087158203, 2.6361...   0.30759   \n",
       "\n",
       "                                                  11  \n",
       "0  [0.447925865650177, 0.6325389742851257, 0.6531...  \n",
       "1  [0.5142688751220703, 0.610914945602417, 0.6681...  \n",
       "2  [0.44851428270339966, 0.596057653427124, 0.689...  \n",
       "3  [0.5483965873718262, 0.6631362438201904, 0.691...  \n",
       "4  [0.08046483993530273, 0.10311856120824814, 0.2...  \n",
       "5  [0.06663724780082703, 0.09546925872564316, 0.2...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_pickle('results/model_gru_final_version.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import models\n",
    "from models import Lstm, Gru, Transformer, TransformerBlock\n",
    "from dataset import LABELS\n",
    "import pandas as pd\n",
    "\n",
    "class final_model:\n",
    "    def __init__(self, model, test, batch_size=32):\n",
    "\n",
    "        self.model = model\n",
    "        self.test = test\n",
    "        # self.test_silence = test_silence\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        LABELS_dict = {i:LABELS[i] for i in range(len(LABELS))}\n",
    "        LABELS_dict[10] = 'unknown'\n",
    "        self.LABELS_dict = LABELS_dict\n",
    "\n",
    "        self.predicted_labels = None\n",
    "        self.silence_labels = None\n",
    "\n",
    "    def predict_silence(self):#, model_path='models\\\\lstm_silence.h5'):\n",
    "        # model_sil = tf.keras.models.load_model(model_path)\n",
    "        # self.silence_labels = np.argmax(model_sil.predict(self.test.batch(self.batch_size)), axis=1)\n",
    "        self.silence_labels = lis = pd.read_pickle('silence_detected.pkl')\n",
    "    \n",
    "    def predict_label(self):\n",
    "        self.predicted_labels = np.argmax(self.model.predict(self.test.batch(self.batch_size)), axis=1)\n",
    "\n",
    "    def predict_if_known(self):\n",
    "        self.model_known = tf.keras.models.load_model('models/lstm_unknown_final.h5')\n",
    "        self.predicted_labels_known = np.argmax(self.model_known.predict(self.test.batch(self.batch_size)), axis=1)\n",
    "    \n",
    "    def predict_label_transformer(self):\n",
    "        model  = tf.keras.models.load_model('models\\\\transformer.h5')\n",
    "        self.predicted_labels = self.model.predict(self.test)\n",
    "\n",
    "    # def save_results(self, submission_name=''):\n",
    "    #     x = pd.concat([pd.DataFrame(self.predicted_labels), pd.DataFrame(self.silence_labels)], axis=1)\n",
    "    #     x.columns = ['label', 'silence']\n",
    "    #     res = np.where(x['silence'] == 0 , 'silence', x['label'].apply(lambda x: self.LABELS_dict[x]))\n",
    "    #     res = pd.DataFrame(res)\n",
    "    #     res.columns = ['label_prediction']\n",
    "    #     res.to_csv(f'results\\\\submission_{submission_name}.csv', index=False, header=True)\n",
    "\n",
    "    def save_results(self, name):\n",
    "        df = pd.read_csv('sample_submission.csv')\n",
    "        x = pd.concat([pd.DataFrame(self.predicted_labels), pd.DataFrame(self.silence_labels)], axis=1)\n",
    "        x.columns = ['label', 'silence']\n",
    "        res = np.where(x['silence'] == 0 , 'silence', x['label'].apply(lambda x: self.LABELS_dict[x]))\n",
    "        df['label'] = res\n",
    "        df.to_csv(f'results\\\\submission_{name}.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import labels_only_detection_training, labels_only_detection_validation, labels_only_detection_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 1.1662 - accuracy: 0.6014\n",
      "Epoch 1: accuracy improved from -inf to 0.60141, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 61s 98ms/step - loss: 1.1662 - accuracy: 0.6014\n",
      "Epoch 2/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.5090 - accuracy: 0.7535\n",
      "Epoch 2: accuracy improved from 0.60141 to 0.75354, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 41s 96ms/step - loss: 0.5090 - accuracy: 0.7535\n",
      "Epoch 3/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.7892\n",
      "Epoch 3: accuracy improved from 0.75354 to 0.78919, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 42s 99ms/step - loss: 0.4429 - accuracy: 0.7892\n",
      "Epoch 4/20\n",
      "426/427 [============================>.] - ETA: 0s - loss: 0.3955 - accuracy: 0.8182\n",
      "Epoch 4: accuracy improved from 0.78919 to 0.81816, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 42s 98ms/step - loss: 0.3956 - accuracy: 0.8182\n",
      "Epoch 5/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8362\n",
      "Epoch 5: accuracy improved from 0.81816 to 0.83621, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 41s 97ms/step - loss: 0.3642 - accuracy: 0.8362\n",
      "Epoch 6/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8454\n",
      "Epoch 6: accuracy improved from 0.83621 to 0.84545, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 38s 88ms/step - loss: 0.3444 - accuracy: 0.8454\n",
      "Epoch 7/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.8560\n",
      "Epoch 7: accuracy improved from 0.84545 to 0.85601, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 38s 89ms/step - loss: 0.3204 - accuracy: 0.8560\n",
      "Epoch 8/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.3060 - accuracy: 0.8617\n",
      "Epoch 8: accuracy improved from 0.85601 to 0.86173, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 39s 92ms/step - loss: 0.3060 - accuracy: 0.8617\n",
      "Epoch 9/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8718\n",
      "Epoch 9: accuracy improved from 0.86173 to 0.87178, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 39s 92ms/step - loss: 0.2879 - accuracy: 0.8718\n",
      "Epoch 10/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2678 - accuracy: 0.8820\n",
      "Epoch 10: accuracy improved from 0.87178 to 0.88198, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 39s 91ms/step - loss: 0.2678 - accuracy: 0.8820\n",
      "Epoch 11/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.8915\n",
      "Epoch 11: accuracy improved from 0.88198 to 0.89151, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 40s 93ms/step - loss: 0.2569 - accuracy: 0.8915\n",
      "Epoch 12/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2330 - accuracy: 0.9013\n",
      "Epoch 12: accuracy improved from 0.89151 to 0.90134, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 43s 100ms/step - loss: 0.2330 - accuracy: 0.9013\n",
      "Epoch 13/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9057\n",
      "Epoch 13: accuracy improved from 0.90134 to 0.90574, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 43s 100ms/step - loss: 0.2283 - accuracy: 0.9057\n",
      "Epoch 14/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.2133 - accuracy: 0.9099\n",
      "Epoch 14: accuracy improved from 0.90574 to 0.90992, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 42s 99ms/step - loss: 0.2133 - accuracy: 0.9099\n",
      "Epoch 15/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9181\n",
      "Epoch 15: accuracy improved from 0.90992 to 0.91807, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 42s 99ms/step - loss: 0.1940 - accuracy: 0.9181\n",
      "Epoch 16/20\n",
      "426/427 [============================>.] - ETA: 0s - loss: 0.1892 - accuracy: 0.9237\n",
      "Epoch 16: accuracy improved from 0.91807 to 0.92371, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 38s 90ms/step - loss: 0.1892 - accuracy: 0.9237\n",
      "Epoch 17/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9281\n",
      "Epoch 17: accuracy improved from 0.92371 to 0.92812, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 43s 101ms/step - loss: 0.1769 - accuracy: 0.9281\n",
      "Epoch 18/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9350\n",
      "Epoch 18: accuracy improved from 0.92812 to 0.93501, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 43s 101ms/step - loss: 0.1629 - accuracy: 0.9350\n",
      "Epoch 19/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.9399\n",
      "Epoch 19: accuracy improved from 0.93501 to 0.93985, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 42s 99ms/step - loss: 0.1532 - accuracy: 0.9399\n",
      "Epoch 20/20\n",
      "427/427 [==============================] - ETA: 0s - loss: 0.1409 - accuracy: 0.9434\n",
      "Epoch 20: accuracy improved from 0.93985 to 0.94345, saving model to models\\gru_only_labels_final.h5\n",
      "427/427 [==============================] - 44s 102ms/step - loss: 0.1409 - accuracy: 0.9434\n"
     ]
    }
   ],
   "source": [
    "model = Gru(gru_units=64, dropout_rate=0.1, learning_rate=0.001, num_classes=10, batch_size=32, epoch=20, model_path='models\\\\gru_only_labels_final.h5')\n",
    "model.train(labels_only_detection_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Gru(from_path = 'models\\\\gru_mod.h5')\n",
    "# model2 = Transformer(from_path = 'models\\\\transformer_mod.h5')\n",
    "# model3 = tf.keras.models.load_model('models/best_gru.h5')\n",
    "# Lstm(from_path = 'models/lstm_unknown_final.h5')\n",
    "d = final_model(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.predict_silence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4955/4955 [==============================] - 748s 150ms/step\n"
     ]
    }
   ],
   "source": [
    "d.predict_if_known()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 39, 44), found shape=(None, None, 39, 44)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\final_model.ipynb Cell 11\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/final_model.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m d\u001b[39m.\u001b[39;49mpredict_label()\n",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\final_model.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/final_model.ipynb#W6sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_label\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/final_model.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicted_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest\u001b[39m.\u001b[39;49mbatch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32md:\\deep\\Deep_Learning\\RNN\\models.py:149\u001b[0m, in \u001b[0;36mGru.predict\u001b[1;34m(self, test_Dataset)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, test_Dataset):\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(test_Dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39m10\u001b[39;49m)), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqq73tc1w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 39, 44), found shape=(None, None, 39, 44)\n"
     ]
    }
   ],
   "source": [
    "d.predict_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    92616\n",
       "0    65922\n",
       "dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(d.predicted_labels_known).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>silence</th>\n",
       "      <th>unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158520</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158521</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158522</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158525</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158533</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66583 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  silence  unknown\n",
       "1          10        1        1\n",
       "4          10        1        1\n",
       "5          10        1        1\n",
       "11         10        1        1\n",
       "16         10        1        1\n",
       "...       ...      ...      ...\n",
       "158520     10        1        1\n",
       "158521     10        1        1\n",
       "158522     10        1        1\n",
       "158525     10        1        1\n",
       "158533     10        1        1\n",
       "\n",
       "[66583 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[(x.silence==1) & (x.unknown==1) & (x.label==10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "x = pd.concat([pd.DataFrame(d.predicted_labels), pd.DataFrame(d.silence_labels),pd.DataFrame(d.predicted_labels_known)], axis=1)\n",
    "x.columns = ['label', 'silence','unknown']\n",
    "# unknown == 1 => unknown\n",
    "res = np.where(x['silence'] == 0 , 'silence', np.where((x['unknown'] == 1) , 'unknown', x['label'].apply(lambda x: d.LABELS_dict[x])))\n",
    "df['label'] = res\n",
    "df.to_csv(f'results\\\\submission_chuj2.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of silences: 801\n",
      "Total number of observations: 158538\n"
     ]
    }
   ],
   "source": [
    "print('Number of silences:', len(d.test) - d.silence_labels.sum())\n",
    "print('Total number of observations:', len(d.test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ensamble_model:\n",
    "    def __init__(self):\n",
    "        # train = pd.read_pickle('features_test_silence.pkl')\n",
    "        # y_train = np.array([x[1] for x in train])\n",
    "        # labels = list(np.unique(y_train))\n",
    "        # self.labels_silence = labels\n",
    "        # self.silence_det = TensorflowDataset('features_test_silence.pkl', labels=self.labels_silence).dataset\n",
    "        \n",
    "        train = pd.read_pickle('features_test.pkl')\n",
    "        y_train = np.array([x[1] for x in train])\n",
    "        labels = list(np.unique(y_train))\n",
    "        self.labels_label = labels\n",
    "        self.label_det = TensorflowDataset('features_test.pkl', labels=self.labels_label).dataset\n",
    "    \n",
    "        self.silence_model = tf.keras.models.load_model('models\\\\silence_model.h5')\n",
    "        self.final_model = 'dupa'#tf.keras.models.load_model('models\\\\final_model.h5')\n",
    "    \n",
    "    def predict_silence(self):\n",
    "        pred = self.silence_model.predict(self.silence_det)\n",
    "\n",
    "    def predict_if_known(self):\n",
    "        pred = self.final_model.predict(self.label_det)\n",
    "\n",
    "    def save():\n",
    "        # otwrz plik csv do zapisu\n",
    "        with open('predictions.csv', mode='w', newline='') as file:\n",
    "            # utwrz obiekt writer i ustaw separator na przecinek\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            \n",
    "            # zapisz nagwek\n",
    "            writer.writerow(['col1', 'col2', 'col3', 'col4', 'prediction'])\n",
    "            \n",
    "            # zapisz dane i przewidywania\n",
    "            for i in range(len(data)):\n",
    "                row = data[i] + [predictions[i]]\n",
    "                writer.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lis = pd.read_pickle('silence_detected.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['label'] + lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sample_submission.csv')\n",
    "df['label'] = 'unknown'\n",
    "df.to_csv('results\\\\unknown_only.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for i in range(len(lis)):\n",
    "    if lis[i] == 0:\n",
    "        new.append('silence')\n",
    "    else:\n",
    "        new.append('dupa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
