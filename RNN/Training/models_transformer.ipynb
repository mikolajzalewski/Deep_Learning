{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import LABELS, labels_only_detection_training, labels_only_detection_validation, labels_only_detection_full\n",
    "from models import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.3493 - accuracy: 0.5433\n",
      "Epoch 1: val_accuracy improved from -inf to 0.65192, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 35s 92ms/step - loss: 1.3493 - accuracy: 0.5433 - val_loss: 0.9982 - val_accuracy: 0.6519\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7936 - accuracy: 0.7263\n",
      "Epoch 2: val_accuracy improved from 0.65192 to 0.75010, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 32s 96ms/step - loss: 0.7936 - accuracy: 0.7263 - val_loss: 0.7327 - val_accuracy: 0.7501\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6911 - accuracy: 0.7642\n",
      "Epoch 3: val_accuracy improved from 0.75010 to 0.76174, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 35s 105ms/step - loss: 0.6911 - accuracy: 0.7642 - val_loss: 0.7013 - val_accuracy: 0.7617\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6074 - accuracy: 0.7929\n",
      "Epoch 4: val_accuracy improved from 0.76174 to 0.80326, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 34s 104ms/step - loss: 0.6074 - accuracy: 0.7929 - val_loss: 0.5783 - val_accuracy: 0.8033\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5673 - accuracy: 0.8037\n",
      "Epoch 5: val_accuracy did not improve from 0.80326\n",
      "330/330 [==============================] - 34s 102ms/step - loss: 0.5673 - accuracy: 0.8037 - val_loss: 0.6102 - val_accuracy: 0.7924\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.8127\n",
      "Epoch 6: val_accuracy did not improve from 0.80326\n",
      "330/330 [==============================] - 36s 108ms/step - loss: 0.5325 - accuracy: 0.8127 - val_loss: 0.6209 - val_accuracy: 0.7963\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5026 - accuracy: 0.8269\n",
      "Epoch 7: val_accuracy improved from 0.80326 to 0.81257, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 36s 110ms/step - loss: 0.5026 - accuracy: 0.8269 - val_loss: 0.5348 - val_accuracy: 0.8126\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8353\n",
      "Epoch 8: val_accuracy improved from 0.81257 to 0.81607, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 37s 111ms/step - loss: 0.4781 - accuracy: 0.8353 - val_loss: 0.5632 - val_accuracy: 0.8161\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4609 - accuracy: 0.8396\n",
      "Epoch 9: val_accuracy improved from 0.81607 to 0.82965, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 36s 109ms/step - loss: 0.4609 - accuracy: 0.8396 - val_loss: 0.5066 - val_accuracy: 0.8296\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4414 - accuracy: 0.8481\n",
      "Epoch 10: val_accuracy improved from 0.82965 to 0.83081, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 37s 112ms/step - loss: 0.4414 - accuracy: 0.8481 - val_loss: 0.4906 - val_accuracy: 0.8308\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4187 - accuracy: 0.8556\n",
      "Epoch 11: val_accuracy did not improve from 0.83081\n",
      "330/330 [==============================] - 38s 115ms/step - loss: 0.4187 - accuracy: 0.8556 - val_loss: 0.5155 - val_accuracy: 0.8285\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4156 - accuracy: 0.8546\n",
      "Epoch 12: val_accuracy improved from 0.83081 to 0.83741, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 37s 111ms/step - loss: 0.4156 - accuracy: 0.8546 - val_loss: 0.5090 - val_accuracy: 0.8374\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3883 - accuracy: 0.8651\n",
      "Epoch 13: val_accuracy improved from 0.83741 to 0.84983, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.3883 - accuracy: 0.8651 - val_loss: 0.4497 - val_accuracy: 0.8498\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3766 - accuracy: 0.8705\n",
      "Epoch 14: val_accuracy improved from 0.84983 to 0.85021, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 39s 117ms/step - loss: 0.3766 - accuracy: 0.8705 - val_loss: 0.4532 - val_accuracy: 0.8502\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3657 - accuracy: 0.8733\n",
      "Epoch 15: val_accuracy did not improve from 0.85021\n",
      "330/330 [==============================] - 38s 116ms/step - loss: 0.3657 - accuracy: 0.8733 - val_loss: 0.4897 - val_accuracy: 0.8397\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8707\n",
      "Epoch 16: val_accuracy did not improve from 0.85021\n",
      "330/330 [==============================] - 35s 107ms/step - loss: 0.3678 - accuracy: 0.8707 - val_loss: 0.5111 - val_accuracy: 0.8390\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3566 - accuracy: 0.8756\n",
      "Epoch 17: val_accuracy did not improve from 0.85021\n",
      "330/330 [==============================] - 37s 113ms/step - loss: 0.3566 - accuracy: 0.8756 - val_loss: 0.4702 - val_accuracy: 0.8417\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3439 - accuracy: 0.8782\n",
      "Epoch 18: val_accuracy did not improve from 0.85021\n",
      "330/330 [==============================] - 36s 108ms/step - loss: 0.3439 - accuracy: 0.8782 - val_loss: 0.4801 - val_accuracy: 0.8452\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.8867\n",
      "Epoch 19: val_accuracy did not improve from 0.85021\n",
      "330/330 [==============================] - 36s 108ms/step - loss: 0.3308 - accuracy: 0.8867 - val_loss: 0.4722 - val_accuracy: 0.8421\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.4405 - accuracy: 0.5165\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69422, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 39s 211ms/step - loss: 1.4405 - accuracy: 0.5165 - val_loss: 0.8701 - val_accuracy: 0.6942\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.7260\n",
      "Epoch 2: val_accuracy improved from 0.69422 to 0.76484, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 204ms/step - loss: 0.7877 - accuracy: 0.7260 - val_loss: 0.6924 - val_accuracy: 0.7648\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6573 - accuracy: 0.7731\n",
      "Epoch 3: val_accuracy improved from 0.76484 to 0.77804, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 207ms/step - loss: 0.6573 - accuracy: 0.7731 - val_loss: 0.6725 - val_accuracy: 0.7780\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5854 - accuracy: 0.7958\n",
      "Epoch 4: val_accuracy improved from 0.77804 to 0.79084, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 207ms/step - loss: 0.5854 - accuracy: 0.7958 - val_loss: 0.6217 - val_accuracy: 0.7908\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.8124\n",
      "Epoch 5: val_accuracy improved from 0.79084 to 0.81063, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 210ms/step - loss: 0.5366 - accuracy: 0.8124 - val_loss: 0.5721 - val_accuracy: 0.8106\n",
      "Epoch 6/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.8286\n",
      "Epoch 6: val_accuracy improved from 0.81063 to 0.81374, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 216ms/step - loss: 0.4920 - accuracy: 0.8286 - val_loss: 0.5782 - val_accuracy: 0.8137\n",
      "Epoch 7/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4534 - accuracy: 0.8408\n",
      "Epoch 7: val_accuracy improved from 0.81374 to 0.81801, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 218ms/step - loss: 0.4534 - accuracy: 0.8408 - val_loss: 0.5375 - val_accuracy: 0.8180\n",
      "Epoch 8/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.8471\n",
      "Epoch 8: val_accuracy did not improve from 0.81801\n",
      "165/165 [==============================] - 35s 213ms/step - loss: 0.4402 - accuracy: 0.8471 - val_loss: 0.5668 - val_accuracy: 0.8083\n",
      "Epoch 9/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4201 - accuracy: 0.8550\n",
      "Epoch 9: val_accuracy improved from 0.81801 to 0.82111, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 208ms/step - loss: 0.4201 - accuracy: 0.8550 - val_loss: 0.5626 - val_accuracy: 0.8211\n",
      "Epoch 10/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4017 - accuracy: 0.8593\n",
      "Epoch 10: val_accuracy improved from 0.82111 to 0.83275, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 212ms/step - loss: 0.4017 - accuracy: 0.8593 - val_loss: 0.5017 - val_accuracy: 0.8328\n",
      "Epoch 11/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8664\n",
      "Epoch 11: val_accuracy did not improve from 0.83275\n",
      "165/165 [==============================] - 35s 210ms/step - loss: 0.3831 - accuracy: 0.8664 - val_loss: 0.5137 - val_accuracy: 0.8316\n",
      "Epoch 12/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3704 - accuracy: 0.8696\n",
      "Epoch 12: val_accuracy improved from 0.83275 to 0.83508, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 211ms/step - loss: 0.3704 - accuracy: 0.8696 - val_loss: 0.5152 - val_accuracy: 0.8351\n",
      "Epoch 13/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3653 - accuracy: 0.8733\n",
      "Epoch 13: val_accuracy did not improve from 0.83508\n",
      "165/165 [==============================] - 35s 213ms/step - loss: 0.3653 - accuracy: 0.8733 - val_loss: 0.4892 - val_accuracy: 0.8351\n",
      "Epoch 14/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.8797\n",
      "Epoch 14: val_accuracy improved from 0.83508 to 0.83624, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 33s 199ms/step - loss: 0.3450 - accuracy: 0.8797 - val_loss: 0.5163 - val_accuracy: 0.8362\n",
      "Epoch 15/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8854\n",
      "Epoch 15: val_accuracy improved from 0.83624 to 0.84090, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 213ms/step - loss: 0.3314 - accuracy: 0.8854 - val_loss: 0.4811 - val_accuracy: 0.8409\n",
      "Epoch 16/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3190 - accuracy: 0.8885\n",
      "Epoch 16: val_accuracy did not improve from 0.84090\n",
      "165/165 [==============================] - 35s 209ms/step - loss: 0.3190 - accuracy: 0.8885 - val_loss: 0.5314 - val_accuracy: 0.8258\n",
      "Epoch 17/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3074 - accuracy: 0.8924\n",
      "Epoch 17: val_accuracy did not improve from 0.84090\n",
      "165/165 [==============================] - 33s 202ms/step - loss: 0.3074 - accuracy: 0.8924 - val_loss: 0.5063 - val_accuracy: 0.8397\n",
      "Epoch 18/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3062 - accuracy: 0.8914\n",
      "Epoch 18: val_accuracy improved from 0.84090 to 0.84323, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 204ms/step - loss: 0.3062 - accuracy: 0.8914 - val_loss: 0.5185 - val_accuracy: 0.8432\n",
      "Epoch 19/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.2994 - accuracy: 0.8943\n",
      "Epoch 19: val_accuracy improved from 0.84323 to 0.84594, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 211ms/step - loss: 0.2994 - accuracy: 0.8943 - val_loss: 0.4869 - val_accuracy: 0.8459\n",
      "Epoch 20/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.2917 - accuracy: 0.8972\n",
      "Epoch 20: val_accuracy did not improve from 0.84594\n",
      "165/165 [==============================] - 36s 215ms/step - loss: 0.2917 - accuracy: 0.8972 - val_loss: 0.5072 - val_accuracy: 0.8390\n",
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.5517 - accuracy: 0.4850\n",
      "Epoch 1: val_accuracy improved from -inf to 0.69461, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 42s 115ms/step - loss: 1.5517 - accuracy: 0.4850 - val_loss: 0.9152 - val_accuracy: 0.6946\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.9007 - accuracy: 0.6895\n",
      "Epoch 2: val_accuracy improved from 0.69461 to 0.74505, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.9007 - accuracy: 0.6895 - val_loss: 0.7260 - val_accuracy: 0.7451\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7435 - accuracy: 0.7401\n",
      "Epoch 3: val_accuracy improved from 0.74505 to 0.76562, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.7435 - accuracy: 0.7401 - val_loss: 0.6550 - val_accuracy: 0.7656\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6454 - accuracy: 0.7749\n",
      "Epoch 4: val_accuracy improved from 0.76562 to 0.77416, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 116ms/step - loss: 0.6454 - accuracy: 0.7749 - val_loss: 0.6646 - val_accuracy: 0.7742\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6008 - accuracy: 0.7941\n",
      "Epoch 5: val_accuracy improved from 0.77416 to 0.79084, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 37s 112ms/step - loss: 0.6008 - accuracy: 0.7941 - val_loss: 0.6279 - val_accuracy: 0.7908\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.8117\n",
      "Epoch 6: val_accuracy improved from 0.79084 to 0.80792, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.5496 - accuracy: 0.8117 - val_loss: 0.5722 - val_accuracy: 0.8079\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5176 - accuracy: 0.8185\n",
      "Epoch 7: val_accuracy improved from 0.80792 to 0.81180, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.5176 - accuracy: 0.8185 - val_loss: 0.5666 - val_accuracy: 0.8118\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4932 - accuracy: 0.8297\n",
      "Epoch 8: val_accuracy improved from 0.81180 to 0.82033, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 39s 117ms/step - loss: 0.4932 - accuracy: 0.8297 - val_loss: 0.5122 - val_accuracy: 0.8203\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.8381\n",
      "Epoch 9: val_accuracy improved from 0.82033 to 0.82150, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 34s 101ms/step - loss: 0.4702 - accuracy: 0.8381 - val_loss: 0.5041 - val_accuracy: 0.8215\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.8460\n",
      "Epoch 10: val_accuracy did not improve from 0.82150\n",
      "330/330 [==============================] - 34s 103ms/step - loss: 0.4532 - accuracy: 0.8460 - val_loss: 0.5407 - val_accuracy: 0.8172\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4249 - accuracy: 0.8534\n",
      "Epoch 11: val_accuracy improved from 0.82150 to 0.83392, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 34s 103ms/step - loss: 0.4249 - accuracy: 0.8534 - val_loss: 0.5043 - val_accuracy: 0.8339\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4105 - accuracy: 0.8564\n",
      "Epoch 12: val_accuracy did not improve from 0.83392\n",
      "330/330 [==============================] - 34s 104ms/step - loss: 0.4105 - accuracy: 0.8564 - val_loss: 0.4930 - val_accuracy: 0.8312\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3904 - accuracy: 0.8645\n",
      "Epoch 13: val_accuracy improved from 0.83392 to 0.83780, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 34s 102ms/step - loss: 0.3904 - accuracy: 0.8645 - val_loss: 0.4853 - val_accuracy: 0.8378\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3761 - accuracy: 0.8687\n",
      "Epoch 14: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 33s 100ms/step - loss: 0.3761 - accuracy: 0.8687 - val_loss: 0.5025 - val_accuracy: 0.8351\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3685 - accuracy: 0.8741\n",
      "Epoch 15: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 36s 109ms/step - loss: 0.3685 - accuracy: 0.8741 - val_loss: 0.4876 - val_accuracy: 0.8366\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3544 - accuracy: 0.8770\n",
      "Epoch 16: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 32s 98ms/step - loss: 0.3544 - accuracy: 0.8770 - val_loss: 0.4891 - val_accuracy: 0.8362\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3442 - accuracy: 0.8808\n",
      "Epoch 17: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 32s 98ms/step - loss: 0.3442 - accuracy: 0.8808 - val_loss: 0.5044 - val_accuracy: 0.8304\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.8876\n",
      "Epoch 18: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 0.3268 - accuracy: 0.8876 - val_loss: 0.5171 - val_accuracy: 0.8347\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.7594 - accuracy: 0.4156\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63523, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 39s 210ms/step - loss: 1.7594 - accuracy: 0.4156 - val_loss: 1.0823 - val_accuracy: 0.6352\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.6692\n",
      "Epoch 2: val_accuracy improved from 0.63523 to 0.74622, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 211ms/step - loss: 0.9567 - accuracy: 0.6692 - val_loss: 0.7387 - val_accuracy: 0.7462\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.7705 - accuracy: 0.7358\n",
      "Epoch 3: val_accuracy improved from 0.74622 to 0.76989, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 216ms/step - loss: 0.7705 - accuracy: 0.7358 - val_loss: 0.6708 - val_accuracy: 0.7699\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6565 - accuracy: 0.7741\n",
      "Epoch 4: val_accuracy improved from 0.76989 to 0.78774, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 214ms/step - loss: 0.6565 - accuracy: 0.7741 - val_loss: 0.6148 - val_accuracy: 0.7877\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5878 - accuracy: 0.7973\n",
      "Epoch 5: val_accuracy improved from 0.78774 to 0.81723, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 215ms/step - loss: 0.5878 - accuracy: 0.7973 - val_loss: 0.5609 - val_accuracy: 0.8172\n",
      "Epoch 6/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.8113\n",
      "Epoch 6: val_accuracy did not improve from 0.81723\n",
      "165/165 [==============================] - 36s 218ms/step - loss: 0.5465 - accuracy: 0.8113 - val_loss: 0.5539 - val_accuracy: 0.8130\n",
      "Epoch 7/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5031 - accuracy: 0.8247\n",
      "Epoch 7: val_accuracy improved from 0.81723 to 0.81762, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 210ms/step - loss: 0.5031 - accuracy: 0.8247 - val_loss: 0.5380 - val_accuracy: 0.8176\n",
      "Epoch 8/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4725 - accuracy: 0.8339\n",
      "Epoch 8: val_accuracy did not improve from 0.81762\n",
      "165/165 [==============================] - 36s 215ms/step - loss: 0.4725 - accuracy: 0.8339 - val_loss: 0.5548 - val_accuracy: 0.8149\n",
      "Epoch 9/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4484 - accuracy: 0.8446\n",
      "Epoch 9: val_accuracy improved from 0.81762 to 0.82266, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 217ms/step - loss: 0.4484 - accuracy: 0.8446 - val_loss: 0.5412 - val_accuracy: 0.8227\n",
      "Epoch 10/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4196 - accuracy: 0.8556\n",
      "Epoch 10: val_accuracy did not improve from 0.82266\n",
      "165/165 [==============================] - 36s 220ms/step - loss: 0.4196 - accuracy: 0.8556 - val_loss: 0.5317 - val_accuracy: 0.8215\n",
      "Epoch 11/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4074 - accuracy: 0.8575\n",
      "Epoch 11: val_accuracy improved from 0.82266 to 0.83314, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 36s 216ms/step - loss: 0.4074 - accuracy: 0.8575 - val_loss: 0.4954 - val_accuracy: 0.8331\n",
      "Epoch 12/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3811 - accuracy: 0.8674\n",
      "Epoch 12: val_accuracy improved from 0.83314 to 0.84323, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 37s 223ms/step - loss: 0.3811 - accuracy: 0.8674 - val_loss: 0.5012 - val_accuracy: 0.8432\n",
      "Epoch 13/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3635 - accuracy: 0.8718\n",
      "Epoch 13: val_accuracy did not improve from 0.84323\n",
      "165/165 [==============================] - 35s 213ms/step - loss: 0.3635 - accuracy: 0.8718 - val_loss: 0.5497 - val_accuracy: 0.8289\n",
      "Epoch 14/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3497 - accuracy: 0.8777\n",
      "Epoch 14: val_accuracy improved from 0.84323 to 0.84517, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 210ms/step - loss: 0.3497 - accuracy: 0.8777 - val_loss: 0.5089 - val_accuracy: 0.8452\n",
      "Epoch 15/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8793\n",
      "Epoch 15: val_accuracy improved from 0.84517 to 0.84672, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 212ms/step - loss: 0.3440 - accuracy: 0.8793 - val_loss: 0.4776 - val_accuracy: 0.8467\n",
      "Epoch 16/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.8847\n",
      "Epoch 16: val_accuracy did not improve from 0.84672\n",
      "165/165 [==============================] - 37s 223ms/step - loss: 0.3307 - accuracy: 0.8847 - val_loss: 0.5022 - val_accuracy: 0.8421\n",
      "Epoch 17/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3126 - accuracy: 0.8897\n",
      "Epoch 17: val_accuracy did not improve from 0.84672\n",
      "165/165 [==============================] - 34s 207ms/step - loss: 0.3126 - accuracy: 0.8897 - val_loss: 0.4871 - val_accuracy: 0.8428\n",
      "Epoch 18/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8929\n",
      "Epoch 18: val_accuracy did not improve from 0.84672\n",
      "165/165 [==============================] - 36s 216ms/step - loss: 0.3072 - accuracy: 0.8929 - val_loss: 0.4948 - val_accuracy: 0.8436\n",
      "Epoch 19/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.8941\n",
      "Epoch 19: val_accuracy did not improve from 0.84672\n",
      "165/165 [==============================] - 35s 210ms/step - loss: 0.2892 - accuracy: 0.8941 - val_loss: 0.5047 - val_accuracy: 0.8413\n",
      "Epoch 20/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.2873 - accuracy: 0.9006\n",
      "Epoch 20: val_accuracy did not improve from 0.84672\n",
      "165/165 [==============================] - 34s 209ms/step - loss: 0.2873 - accuracy: 0.9006 - val_loss: 0.5053 - val_accuracy: 0.8428\n",
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.2886 - accuracy: 0.1687\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10400, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 41s 113ms/step - loss: 2.2886 - accuracy: 0.1687 - val_loss: 3.6267 - val_accuracy: 0.1040\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.2636 - accuracy: 0.1529\n",
      "Epoch 2: val_accuracy did not improve from 0.10400\n",
      "330/330 [==============================] - 37s 111ms/step - loss: 2.2636 - accuracy: 0.1529 - val_loss: 3.3073 - val_accuracy: 0.0920\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.1426 - accuracy: 0.1959\n",
      "Epoch 3: val_accuracy improved from 0.10400 to 0.17346, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 116ms/step - loss: 2.1426 - accuracy: 0.1959 - val_loss: 2.8970 - val_accuracy: 0.1735\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.0515 - accuracy: 0.2325\n",
      "Epoch 4: val_accuracy improved from 0.17346 to 0.17811, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 39s 117ms/step - loss: 2.0515 - accuracy: 0.2325 - val_loss: 2.5408 - val_accuracy: 0.1781\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.9063 - accuracy: 0.2976\n",
      "Epoch 5: val_accuracy did not improve from 0.17811\n",
      "330/330 [==============================] - 40s 120ms/step - loss: 1.9063 - accuracy: 0.2976 - val_loss: 2.9841 - val_accuracy: 0.1288\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6919 - accuracy: 0.3746\n",
      "Epoch 6: val_accuracy improved from 0.17811 to 0.25495, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 42s 127ms/step - loss: 1.6919 - accuracy: 0.3746 - val_loss: 2.0837 - val_accuracy: 0.2549\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6286 - accuracy: 0.4057\n",
      "Epoch 7: val_accuracy improved from 0.25495 to 0.27163, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 40s 121ms/step - loss: 1.6286 - accuracy: 0.4057 - val_loss: 1.8847 - val_accuracy: 0.2716\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6106 - accuracy: 0.4141\n",
      "Epoch 8: val_accuracy did not improve from 0.27163\n",
      "330/330 [==============================] - 43s 129ms/step - loss: 1.6106 - accuracy: 0.4141 - val_loss: 3.0716 - val_accuracy: 0.2596\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.5632 - accuracy: 0.4345\n",
      "Epoch 9: val_accuracy did not improve from 0.27163\n",
      "330/330 [==============================] - 41s 125ms/step - loss: 1.5632 - accuracy: 0.4345 - val_loss: 3.0232 - val_accuracy: 0.2076\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6702 - accuracy: 0.3958\n",
      "Epoch 10: val_accuracy improved from 0.27163 to 0.30501, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 40s 120ms/step - loss: 1.6702 - accuracy: 0.3958 - val_loss: 1.9345 - val_accuracy: 0.3050\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.7373 - accuracy: 0.3724\n",
      "Epoch 11: val_accuracy did not improve from 0.30501\n",
      "330/330 [==============================] - 40s 120ms/step - loss: 1.7373 - accuracy: 0.3724 - val_loss: 1.9979 - val_accuracy: 0.2945\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6990 - accuracy: 0.3784\n",
      "Epoch 12: val_accuracy improved from 0.30501 to 0.38106, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 43s 129ms/step - loss: 1.6990 - accuracy: 0.3784 - val_loss: 1.6756 - val_accuracy: 0.3811\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.7269 - accuracy: 0.3747\n",
      "Epoch 13: val_accuracy did not improve from 0.38106\n",
      "330/330 [==============================] - 39s 117ms/step - loss: 1.7269 - accuracy: 0.3747 - val_loss: 2.2985 - val_accuracy: 0.1312\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.1136 - accuracy: 0.2180\n",
      "Epoch 14: val_accuracy did not improve from 0.38106\n",
      "330/330 [==============================] - 45s 136ms/step - loss: 2.1136 - accuracy: 0.2180 - val_loss: 2.6639 - val_accuracy: 0.1378\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.9877 - accuracy: 0.2732\n",
      "Epoch 15: val_accuracy did not improve from 0.38106\n",
      "330/330 [==============================] - 43s 129ms/step - loss: 1.9877 - accuracy: 0.2732 - val_loss: 2.2271 - val_accuracy: 0.2088\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.0783 - accuracy: 0.2256\n",
      "Epoch 16: val_accuracy did not improve from 0.38106\n",
      "330/330 [==============================] - 39s 119ms/step - loss: 2.0783 - accuracy: 0.2256 - val_loss: 2.0928 - val_accuracy: 0.2041\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.0816 - accuracy: 0.2236\n",
      "Epoch 17: val_accuracy did not improve from 0.38106\n",
      "330/330 [==============================] - 38s 114ms/step - loss: 2.0816 - accuracy: 0.2236 - val_loss: 2.7250 - val_accuracy: 0.2243\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.5947 - accuracy: 0.4441\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50990, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 61s 344ms/step - loss: 1.5947 - accuracy: 0.4441 - val_loss: 1.7951 - val_accuracy: 0.5099\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.1299 - accuracy: 0.6065\n",
      "Epoch 2: val_accuracy improved from 0.50990 to 0.56888, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 55s 333ms/step - loss: 1.1299 - accuracy: 0.6065 - val_loss: 1.3525 - val_accuracy: 0.5689\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.0241 - accuracy: 0.6405\n",
      "Epoch 3: val_accuracy improved from 0.56888 to 0.57315, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 56s 341ms/step - loss: 1.0241 - accuracy: 0.6405 - val_loss: 1.2829 - val_accuracy: 0.5731\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.4713 - accuracy: 0.4850\n",
      "Epoch 4: val_accuracy did not improve from 0.57315\n",
      "165/165 [==============================] - 55s 335ms/step - loss: 1.4713 - accuracy: 0.4850 - val_loss: 2.1374 - val_accuracy: 0.2383\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.8663 - accuracy: 0.3221\n",
      "Epoch 5: val_accuracy did not improve from 0.57315\n",
      "165/165 [==============================] - 56s 338ms/step - loss: 1.8663 - accuracy: 0.3221 - val_loss: 1.7664 - val_accuracy: 0.3558\n",
      "Epoch 6/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.5754 - accuracy: 0.4354\n",
      "Epoch 6: val_accuracy did not improve from 0.57315\n",
      "165/165 [==============================] - 56s 341ms/step - loss: 1.5754 - accuracy: 0.4354 - val_loss: 1.3957 - val_accuracy: 0.5017\n",
      "Epoch 7/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.4833 - accuracy: 0.4706\n",
      "Epoch 7: val_accuracy did not improve from 0.57315\n",
      "165/165 [==============================] - 57s 346ms/step - loss: 1.4833 - accuracy: 0.4706 - val_loss: 1.8736 - val_accuracy: 0.3648\n",
      "Epoch 8/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.5704 - accuracy: 0.4452\n",
      "Epoch 8: val_accuracy did not improve from 0.57315\n",
      "165/165 [==============================] - 57s 342ms/step - loss: 1.5704 - accuracy: 0.4452 - val_loss: 1.6937 - val_accuracy: 0.4439\n",
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.4859 - accuracy: 0.4984\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66783, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 100s 277ms/step - loss: 1.4859 - accuracy: 0.4984 - val_loss: 1.0041 - val_accuracy: 0.6678\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.8451 - accuracy: 0.7056\n",
      "Epoch 2: val_accuracy improved from 0.66783 to 0.73923, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 88s 268ms/step - loss: 0.8451 - accuracy: 0.7056 - val_loss: 0.7287 - val_accuracy: 0.7392\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7178 - accuracy: 0.7517\n",
      "Epoch 3: val_accuracy improved from 0.73923 to 0.76135, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 90s 272ms/step - loss: 0.7178 - accuracy: 0.7517 - val_loss: 0.6648 - val_accuracy: 0.7614\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6442 - accuracy: 0.7794\n",
      "Epoch 4: val_accuracy improved from 0.76135 to 0.76368, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 89s 268ms/step - loss: 0.6442 - accuracy: 0.7794 - val_loss: 0.6517 - val_accuracy: 0.7637\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6133 - accuracy: 0.7911\n",
      "Epoch 5: val_accuracy improved from 0.76368 to 0.79977, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 86s 261ms/step - loss: 0.6133 - accuracy: 0.7911 - val_loss: 0.6002 - val_accuracy: 0.7998\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.8010\n",
      "Epoch 6: val_accuracy did not improve from 0.79977\n",
      "330/330 [==============================] - 95s 289ms/step - loss: 0.5825 - accuracy: 0.8010 - val_loss: 0.6346 - val_accuracy: 0.7908\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5315 - accuracy: 0.8172\n",
      "Epoch 7: val_accuracy did not improve from 0.79977\n",
      "330/330 [==============================] - 97s 295ms/step - loss: 0.5315 - accuracy: 0.8172 - val_loss: 0.7096 - val_accuracy: 0.7656\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5359 - accuracy: 0.8140\n",
      "Epoch 8: val_accuracy improved from 0.79977 to 0.81102, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 101s 305ms/step - loss: 0.5359 - accuracy: 0.8140 - val_loss: 0.5622 - val_accuracy: 0.8110\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5069 - accuracy: 0.8253\n",
      "Epoch 9: val_accuracy did not improve from 0.81102\n",
      "330/330 [==============================] - 96s 292ms/step - loss: 0.5069 - accuracy: 0.8253 - val_loss: 0.5958 - val_accuracy: 0.8060\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4982 - accuracy: 0.8256\n",
      "Epoch 10: val_accuracy did not improve from 0.81102\n",
      "330/330 [==============================] - 101s 305ms/step - loss: 0.4982 - accuracy: 0.8256 - val_loss: 0.5950 - val_accuracy: 0.8048\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4758 - accuracy: 0.8358\n",
      "Epoch 11: val_accuracy did not improve from 0.81102\n",
      "330/330 [==============================] - 108s 327ms/step - loss: 0.4758 - accuracy: 0.8358 - val_loss: 0.6833 - val_accuracy: 0.7831\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8399\n",
      "Epoch 12: val_accuracy did not improve from 0.81102\n",
      "330/330 [==============================] - 162s 492ms/step - loss: 0.4713 - accuracy: 0.8399 - val_loss: 0.6079 - val_accuracy: 0.7971\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4611 - accuracy: 0.8420\n",
      "Epoch 13: val_accuracy improved from 0.81102 to 0.81490, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 160s 484ms/step - loss: 0.4611 - accuracy: 0.8420 - val_loss: 0.5376 - val_accuracy: 0.8149\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4354 - accuracy: 0.8521\n",
      "Epoch 14: val_accuracy did not improve from 0.81490\n",
      "330/330 [==============================] - 140s 424ms/step - loss: 0.4354 - accuracy: 0.8521 - val_loss: 0.5980 - val_accuracy: 0.8060\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8518\n",
      "Epoch 15: val_accuracy did not improve from 0.81490\n",
      "330/330 [==============================] - 132s 398ms/step - loss: 0.4255 - accuracy: 0.8518 - val_loss: 0.6805 - val_accuracy: 0.7776\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8573\n",
      "Epoch 16: val_accuracy improved from 0.81490 to 0.82538, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 136s 411ms/step - loss: 0.4164 - accuracy: 0.8573 - val_loss: 0.5245 - val_accuracy: 0.8254\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4090 - accuracy: 0.8583\n",
      "Epoch 17: val_accuracy improved from 0.82538 to 0.82693, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 126s 382ms/step - loss: 0.4090 - accuracy: 0.8583 - val_loss: 0.5405 - val_accuracy: 0.8269\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.8611\n",
      "Epoch 18: val_accuracy improved from 0.82693 to 0.83392, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 137s 414ms/step - loss: 0.4036 - accuracy: 0.8611 - val_loss: 0.5192 - val_accuracy: 0.8339\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3864 - accuracy: 0.8660\n",
      "Epoch 19: val_accuracy did not improve from 0.83392\n",
      "330/330 [==============================] - 133s 402ms/step - loss: 0.3864 - accuracy: 0.8660 - val_loss: 0.5556 - val_accuracy: 0.8250\n",
      "Epoch 20/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3771 - accuracy: 0.8683\n",
      "Epoch 20: val_accuracy did not improve from 0.83392\n",
      "330/330 [==============================] - 134s 407ms/step - loss: 0.3771 - accuracy: 0.8683 - val_loss: 0.5475 - val_accuracy: 0.8227\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.5120 - accuracy: 0.4875\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68374, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 140s 687ms/step - loss: 1.5120 - accuracy: 0.4875 - val_loss: 0.8887 - val_accuracy: 0.6837\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.8268 - accuracy: 0.7132\n",
      "Epoch 2: val_accuracy improved from 0.68374 to 0.74078, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 111s 676ms/step - loss: 0.8268 - accuracy: 0.7132 - val_loss: 0.7458 - val_accuracy: 0.7408\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6837 - accuracy: 0.7606\n",
      "Epoch 3: val_accuracy did not improve from 0.74078\n",
      "165/165 [==============================] - 107s 646ms/step - loss: 0.6837 - accuracy: 0.7606 - val_loss: 0.8190 - val_accuracy: 0.7392\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6091 - accuracy: 0.7893\n",
      "Epoch 4: val_accuracy did not improve from 0.74078\n",
      "165/165 [==============================] - 109s 659ms/step - loss: 0.6091 - accuracy: 0.7893 - val_loss: 1.0222 - val_accuracy: 0.6927\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5525 - accuracy: 0.8065\n",
      "Epoch 5: val_accuracy improved from 0.74078 to 0.79395, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 113s 686ms/step - loss: 0.5525 - accuracy: 0.8065 - val_loss: 0.6273 - val_accuracy: 0.7939\n",
      "Epoch 6/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5214 - accuracy: 0.8168\n",
      "Epoch 6: val_accuracy improved from 0.79395 to 0.80093, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 112s 680ms/step - loss: 0.5214 - accuracy: 0.8168 - val_loss: 0.5742 - val_accuracy: 0.8009\n",
      "Epoch 7/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4903 - accuracy: 0.8310\n",
      "Epoch 7: val_accuracy improved from 0.80093 to 0.80636, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 101s 614ms/step - loss: 0.4903 - accuracy: 0.8310 - val_loss: 0.6198 - val_accuracy: 0.8064\n",
      "Epoch 8/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8385\n",
      "Epoch 8: val_accuracy did not improve from 0.80636\n",
      "165/165 [==============================] - 108s 652ms/step - loss: 0.4602 - accuracy: 0.8385 - val_loss: 0.6267 - val_accuracy: 0.7974\n",
      "Epoch 9/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.8463\n",
      "Epoch 9: val_accuracy improved from 0.80636 to 0.81374, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 117s 707ms/step - loss: 0.4421 - accuracy: 0.8463 - val_loss: 0.5757 - val_accuracy: 0.8137\n",
      "Epoch 10/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.8515\n",
      "Epoch 10: val_accuracy did not improve from 0.81374\n",
      "165/165 [==============================] - 108s 654ms/step - loss: 0.4224 - accuracy: 0.8515 - val_loss: 0.6074 - val_accuracy: 0.8087\n",
      "Epoch 11/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4043 - accuracy: 0.8597\n",
      "Epoch 11: val_accuracy did not improve from 0.81374\n",
      "165/165 [==============================] - 77s 463ms/step - loss: 0.4043 - accuracy: 0.8597 - val_loss: 0.6574 - val_accuracy: 0.7955\n",
      "Epoch 12/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4019 - accuracy: 0.8608\n",
      "Epoch 12: val_accuracy did not improve from 0.81374\n",
      "165/165 [==============================] - 84s 506ms/step - loss: 0.4019 - accuracy: 0.8608 - val_loss: 0.7073 - val_accuracy: 0.7889\n",
      "Epoch 13/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3845 - accuracy: 0.8660\n",
      "Epoch 13: val_accuracy improved from 0.81374 to 0.82072, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 80s 487ms/step - loss: 0.3845 - accuracy: 0.8660 - val_loss: 0.5455 - val_accuracy: 0.8207\n",
      "Epoch 14/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.8753\n",
      "Epoch 14: val_accuracy improved from 0.82072 to 0.82383, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 80s 486ms/step - loss: 0.3686 - accuracy: 0.8753 - val_loss: 0.5420 - val_accuracy: 0.8238\n",
      "Epoch 15/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3533 - accuracy: 0.8775\n",
      "Epoch 15: val_accuracy improved from 0.82383 to 0.83741, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 83s 503ms/step - loss: 0.3533 - accuracy: 0.8775 - val_loss: 0.5279 - val_accuracy: 0.8374\n",
      "Epoch 16/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3473 - accuracy: 0.8793\n",
      "Epoch 16: val_accuracy did not improve from 0.83741\n",
      "165/165 [==============================] - 82s 497ms/step - loss: 0.3473 - accuracy: 0.8793 - val_loss: 0.5430 - val_accuracy: 0.8269\n",
      "Epoch 17/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3401 - accuracy: 0.8818\n",
      "Epoch 17: val_accuracy did not improve from 0.83741\n",
      "165/165 [==============================] - 80s 483ms/step - loss: 0.3401 - accuracy: 0.8818 - val_loss: 0.5545 - val_accuracy: 0.8289\n",
      "Epoch 18/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3364 - accuracy: 0.8825\n",
      "Epoch 18: val_accuracy improved from 0.83741 to 0.84439, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 82s 494ms/step - loss: 0.3364 - accuracy: 0.8825 - val_loss: 0.5485 - val_accuracy: 0.8444\n",
      "Epoch 19/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3213 - accuracy: 0.8873\n",
      "Epoch 19: val_accuracy did not improve from 0.84439\n",
      "165/165 [==============================] - 82s 494ms/step - loss: 0.3213 - accuracy: 0.8873 - val_loss: 0.5366 - val_accuracy: 0.8335\n",
      "Epoch 20/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.8960\n",
      "Epoch 20: val_accuracy improved from 0.84439 to 0.84905, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 85s 514ms/step - loss: 0.3019 - accuracy: 0.8960 - val_loss: 0.4927 - val_accuracy: 0.8490\n",
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.5971 - accuracy: 0.4717\n",
      "Epoch 1: val_accuracy improved from -inf to 0.66705, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 141s 394ms/step - loss: 1.5971 - accuracy: 0.4717 - val_loss: 0.9679 - val_accuracy: 0.6671\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.9092 - accuracy: 0.6887\n",
      "Epoch 2: val_accuracy improved from 0.66705 to 0.73147, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 122s 369ms/step - loss: 0.9092 - accuracy: 0.6887 - val_loss: 0.7709 - val_accuracy: 0.7315\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7374 - accuracy: 0.7473\n",
      "Epoch 3: val_accuracy improved from 0.73147 to 0.78580, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 122s 370ms/step - loss: 0.7374 - accuracy: 0.7473 - val_loss: 0.6115 - val_accuracy: 0.7858\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.7817\n",
      "Epoch 4: val_accuracy did not improve from 0.78580\n",
      "330/330 [==============================] - 123s 372ms/step - loss: 0.6361 - accuracy: 0.7817 - val_loss: 0.6553 - val_accuracy: 0.7858\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.8013\n",
      "Epoch 5: val_accuracy improved from 0.78580 to 0.79201, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 123s 373ms/step - loss: 0.5776 - accuracy: 0.8013 - val_loss: 0.6301 - val_accuracy: 0.7920\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5458 - accuracy: 0.8113\n",
      "Epoch 6: val_accuracy improved from 0.79201 to 0.81180, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 123s 373ms/step - loss: 0.5458 - accuracy: 0.8113 - val_loss: 0.5588 - val_accuracy: 0.8118\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4969 - accuracy: 0.8266\n",
      "Epoch 7: val_accuracy did not improve from 0.81180\n",
      "330/330 [==============================] - 119s 362ms/step - loss: 0.4969 - accuracy: 0.8266 - val_loss: 0.5705 - val_accuracy: 0.8025\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.8345\n",
      "Epoch 8: val_accuracy improved from 0.81180 to 0.82111, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 125s 377ms/step - loss: 0.4825 - accuracy: 0.8345 - val_loss: 0.5385 - val_accuracy: 0.8211\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4460 - accuracy: 0.8492\n",
      "Epoch 9: val_accuracy improved from 0.82111 to 0.82654, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 122s 369ms/step - loss: 0.4460 - accuracy: 0.8492 - val_loss: 0.5297 - val_accuracy: 0.8265\n",
      "Epoch 10/20\n",
      " 82/330 [======>.......................] - ETA: 1:36 - loss: 0.4445 - accuracy: 0.8436"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m [\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m]:\n\u001b[0;32m      7\u001b[0m     model \u001b[39m=\u001b[39m Transformer(num_heads\u001b[39m=\u001b[39mnum_heads, num_layers\u001b[39m=\u001b[39mnum_layer, dropout_rate\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, learning_rate\u001b[39m=\u001b[39mlr, num_classes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch, epoch\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(labels_only_detection_training, labels_only_detection_validation)\n\u001b[0;32m      9\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((res, pd\u001b[39m.\u001b[39mDataFrame([[num_heads,\u001b[39m0.1\u001b[39m, lr, \u001b[39m20\u001b[39m, batch,num_layer, model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]], \n\u001b[0;32m     10\u001b[0m                                                             columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_layer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\models.py:245\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, train_Dataset, val_Dataset)\u001b[0m\n\u001b[0;32m    243\u001b[0m ])\n\u001b[0;32m    244\u001b[0m self.layernorm1 = LayerNormalization(epsilon=1e-6)\n\u001b[1;32m--> 245\u001b[0m self.layernorm2 = LayerNormalization(epsilon=1e-6)\n\u001b[0;32m    246\u001b[0m self.dropout1 = Dropout(self.dropout_rate)\n\u001b[0;32m    247\u001b[0m self.dropout2 = Dropout(self.dropout_rate)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])\n",
    "\n",
    "for num_heads in [2,4]:\n",
    "    for num_layer in [2,4]:\n",
    "        for lr in [0.001, 0.0005, 0.005]:\n",
    "            for batch in [64,128]:\n",
    "                model = Transformer(num_heads=num_heads, num_layers=num_layer, dropout_rate=0.1, learning_rate=lr, num_classes=10, batch_size=batch, epoch=20)\n",
    "                model.train(labels_only_detection_training, labels_only_detection_validation)\n",
    "                res = np.concatenate((res, pd.DataFrame([[num_heads,0.1, lr, 20, batch,num_layer, model.history.history['loss'][-1], model.history.history['accuracy'][-1], model.history.history['val_loss'][-1], model.history.history['val_accuracy'][-1]]], \n",
    "                                                                        columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res,columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max']).to_pickle('..\\\\results\\\\transformer_wyniki_only_known.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results\\\\model_transformer_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>2.120008</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.790722</td>\n",
       "      <td>0.600954</td>\n",
       "      <td>1.342593</td>\n",
       "      <td>0.812886</td>\n",
       "      <td>0.812886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_heads  num_layers  dropout_rate  epoch  batch      loss  loss_max  \\\n",
       "19        4.0         1.0           0.2   20.0   64.0  0.690125  2.120008   \n",
       "\n",
       "    accuracy  accuracy_max  val_loss  val_loss_max  val_accuracy  \\\n",
       "19  0.790032      0.790722  0.600954      1.342593      0.812886   \n",
       "\n",
       "    val_accuracy_max  \n",
       "19          0.812886  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.val_accuracy == results.val_accuracy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(4, 1, 0.2, 20, 64, 0.001, (39,44), 11, model_path='models\\\\transformer_mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.4390 - accuracy: 0.6051\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62386, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 64ms/step - loss: 1.4361 - accuracy: 0.6059 - val_loss: 1.2966 - val_accuracy: 0.6239\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.2717 - accuracy: 0.6354\n",
      "Epoch 2: val_accuracy improved from 0.62386 to 0.68123, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.2717 - accuracy: 0.6354 - val_loss: 1.0730 - val_accuracy: 0.6812\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.1600 - accuracy: 0.6546\n",
      "Epoch 3: val_accuracy did not improve from 0.68123\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.1600 - accuracy: 0.6546 - val_loss: 1.2423 - val_accuracy: 0.6174\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.0674 - accuracy: 0.6781\n",
      "Epoch 4: val_accuracy did not improve from 0.68123\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.0674 - accuracy: 0.6781 - val_loss: 1.0686 - val_accuracy: 0.6736\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.6976\n",
      "Epoch 5: val_accuracy improved from 0.68123 to 0.70359, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 58ms/step - loss: 0.9702 - accuracy: 0.6976 - val_loss: 0.9470 - val_accuracy: 0.7036\n",
      "Epoch 6/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9419 - accuracy: 0.7089\n",
      "Epoch 6: val_accuracy improved from 0.70359 to 0.72286, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 0.9403 - accuracy: 0.7090 - val_loss: 0.8850 - val_accuracy: 0.7229\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.7252\n",
      "Epoch 7: val_accuracy improved from 0.72286 to 0.75081, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 0.8779 - accuracy: 0.7252 - val_loss: 0.8093 - val_accuracy: 0.7508\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8337 - accuracy: 0.7391\n",
      "Epoch 8: val_accuracy improved from 0.75081 to 0.75919, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.8337 - accuracy: 0.7391 - val_loss: 0.7818 - val_accuracy: 0.7592\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.7511\n",
      "Epoch 9: val_accuracy did not improve from 0.75919\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.7798 - accuracy: 0.7511 - val_loss: 0.7994 - val_accuracy: 0.7470\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7608 - accuracy: 0.7498\n",
      "Epoch 10: val_accuracy improved from 0.75919 to 0.76361, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 61ms/step - loss: 0.7608 - accuracy: 0.7498 - val_loss: 0.7453 - val_accuracy: 0.7636\n",
      "Epoch 11/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.7196 - accuracy: 0.7644\n",
      "Epoch 11: val_accuracy did not improve from 0.76361\n",
      "107/107 [==============================] - 7s 63ms/step - loss: 0.7207 - accuracy: 0.7637 - val_loss: 0.7486 - val_accuracy: 0.7599\n",
      "Epoch 12/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6988 - accuracy: 0.7748\n",
      "Epoch 12: val_accuracy did not improve from 0.76361\n",
      "107/107 [==============================] - 6s 61ms/step - loss: 0.6979 - accuracy: 0.7753 - val_loss: 0.7384 - val_accuracy: 0.7633\n",
      "Epoch 13/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.7761\n",
      "Epoch 13: val_accuracy improved from 0.76361 to 0.78155, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 64ms/step - loss: 0.6844 - accuracy: 0.7769 - val_loss: 0.6976 - val_accuracy: 0.7816\n",
      "Epoch 14/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7913\n",
      "Epoch 14: val_accuracy improved from 0.78155 to 0.78376, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.6378 - accuracy: 0.7909 - val_loss: 0.6855 - val_accuracy: 0.7838\n",
      "Epoch 15/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.7941\n",
      "Epoch 15: val_accuracy did not improve from 0.78376\n",
      "107/107 [==============================] - 7s 62ms/step - loss: 0.6264 - accuracy: 0.7940 - val_loss: 0.7411 - val_accuracy: 0.7682\n",
      "Epoch 16/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.7957\n",
      "Epoch 16: val_accuracy did not improve from 0.78376\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.6217 - accuracy: 0.7958 - val_loss: 0.7235 - val_accuracy: 0.7780\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.8013\n",
      "Epoch 17: val_accuracy improved from 0.78376 to 0.78773, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 62ms/step - loss: 0.6095 - accuracy: 0.8013 - val_loss: 0.6695 - val_accuracy: 0.7877\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.8050\n",
      "Epoch 18: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.5933 - accuracy: 0.8050 - val_loss: 0.7116 - val_accuracy: 0.7721\n",
      "Epoch 19/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.5547 - accuracy: 0.8187\n",
      "Epoch 19: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 7s 61ms/step - loss: 0.5546 - accuracy: 0.8189 - val_loss: 0.6890 - val_accuracy: 0.7855\n",
      "Epoch 20/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.8174\n",
      "Epoch 20: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 6s 58ms/step - loss: 0.5425 - accuracy: 0.8173 - val_loss: 0.7654 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "model.train(label_detection_training, label_detection_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import Lstm, Gru, Transformer\n",
    "from dataset import LABELS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('extracted_features\\\\features_test.pkl')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((test, np.zeros(len(test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15854/15854 [==============================] - 58s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "x = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    97594\n",
       "7      9068\n",
       "2      8487\n",
       "5      7625\n",
       "4      7303\n",
       "9      6912\n",
       "0      6628\n",
       "8      4575\n",
       "6      4372\n",
       "1      4179\n",
       "3      1795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import label_detection_training, label_detection_validation, silence_detection_training, silence_detection_validation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from dataset import TensorflowDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "# val = pd.read_pickle('extracted_features\\\\features_validation.pkl')\n",
    "\n",
    "# X_train = np.array([x[0] for x in train])\n",
    "# y_train = np.array([x[1] for x in train])\n",
    "# X_val = np.array([x[0] for x in val])\n",
    "# y_val = np.array([x[1] for x in val])\n",
    "\n",
    "# # Create a label encoder object\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# # Fit the label encoder using your labels (combine both train and val labels if they have different classes)\n",
    "# label_encoder.fit(np.concatenate((y_train, y_val)))\n",
    "\n",
    "# # Transform your string labels to integer labels for both training and validation sets\n",
    "# y_train_encoded = label_encoder.transform(y_train)\n",
    "# y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "y_train = np.array([x[1] for x in train])\n",
    "labels = list(np.unique(y_train))\n",
    "train = TensorflowDataset('extracted_features\\\\features_training.pkl', labels=labels).dataset\n",
    "train = train.shuffle(len(train), reshuffle_each_iteration=True)\n",
    "val = TensorflowDataset('extracted_features\\\\features_validation.pkl', labels=labels).dataset\n",
    "val = val.shuffle(len(val), reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 10\n",
      "Epoch 1/10\n",
      "1811/1811 - 49s - loss: 2.2341 - accuracy: 0.3708 - val_loss: 1.4084 - val_accuracy: 0.5799 - 49s/epoch - 27ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 45s - loss: 1.5168 - accuracy: 0.5522 - val_loss: 1.0448 - val_accuracy: 0.6921 - 45s/epoch - 25ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 48s - loss: 1.3047 - accuracy: 0.6113 - val_loss: 1.1216 - val_accuracy: 0.6602 - 48s/epoch - 27ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 49s - loss: 1.2991 - accuracy: 0.6142 - val_loss: 0.9259 - val_accuracy: 0.7234 - 49s/epoch - 27ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 49s - loss: 1.1588 - accuracy: 0.6563 - val_loss: 0.8972 - val_accuracy: 0.7286 - 49s/epoch - 27ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 48s - loss: 1.1611 - accuracy: 0.6545 - val_loss: 0.8835 - val_accuracy: 0.7343 - 48s/epoch - 26ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 50s - loss: 1.1062 - accuracy: 0.6720 - val_loss: 0.8617 - val_accuracy: 0.7398 - 50s/epoch - 28ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 51s - loss: 1.0632 - accuracy: 0.6827 - val_loss: 0.8325 - val_accuracy: 0.7505 - 51s/epoch - 28ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 50s - loss: 1.0183 - accuracy: 0.6955 - val_loss: 0.7855 - val_accuracy: 0.7580 - 50s/epoch - 28ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 52s - loss: 1.0201 - accuracy: 0.6949 - val_loss: 0.7494 - val_accuracy: 0.7652 - 52s/epoch - 29ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 66s - loss: 2.1068 - accuracy: 0.4024 - val_loss: 1.2203 - val_accuracy: 0.6320 - 66s/epoch - 37ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 60s - loss: 1.3975 - accuracy: 0.5842 - val_loss: 1.0267 - val_accuracy: 0.6958 - 60s/epoch - 33ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 61s - loss: 1.2557 - accuracy: 0.6253 - val_loss: 0.9898 - val_accuracy: 0.7030 - 61s/epoch - 34ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 60s - loss: 1.1790 - accuracy: 0.6498 - val_loss: 0.9405 - val_accuracy: 0.7162 - 60s/epoch - 33ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 59s - loss: 1.1051 - accuracy: 0.6706 - val_loss: 0.8465 - val_accuracy: 0.7433 - 59s/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 59s - loss: 1.0588 - accuracy: 0.6832 - val_loss: 0.8125 - val_accuracy: 0.7501 - 59s/epoch - 33ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 59s - loss: 1.0260 - accuracy: 0.6957 - val_loss: 0.7454 - val_accuracy: 0.7695 - 59s/epoch - 33ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 59s - loss: 1.0234 - accuracy: 0.6941 - val_loss: 0.8140 - val_accuracy: 0.7492 - 59s/epoch - 32ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 59s - loss: 0.9964 - accuracy: 0.7015 - val_loss: 0.7606 - val_accuracy: 0.7705 - 59s/epoch - 33ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 60s - loss: 0.9706 - accuracy: 0.7115 - val_loss: 0.7610 - val_accuracy: 0.7742 - 60s/epoch - 33ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 59s - loss: 0.9567 - accuracy: 0.7159 - val_loss: 0.7523 - val_accuracy: 0.7711 - 59s/epoch - 32ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 59s - loss: 0.9381 - accuracy: 0.7184 - val_loss: 0.7059 - val_accuracy: 0.7799 - 59s/epoch - 33ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 59s - loss: 0.9173 - accuracy: 0.7251 - val_loss: 0.7533 - val_accuracy: 0.7652 - 59s/epoch - 33ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 59s - loss: 0.9095 - accuracy: 0.7280 - val_loss: 0.6973 - val_accuracy: 0.7905 - 59s/epoch - 32ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 58s - loss: 0.8989 - accuracy: 0.7309 - val_loss: 0.7127 - val_accuracy: 0.7792 - 58s/epoch - 32ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 55s - loss: 0.8772 - accuracy: 0.7378 - val_loss: 0.6731 - val_accuracy: 0.7945 - 55s/epoch - 30ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 63s - loss: 0.8873 - accuracy: 0.7348 - val_loss: 0.6656 - val_accuracy: 0.7970 - 63s/epoch - 35ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 62s - loss: 0.8480 - accuracy: 0.7459 - val_loss: 0.6658 - val_accuracy: 0.7948 - 62s/epoch - 34ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 64s - loss: 0.8532 - accuracy: 0.7436 - val_loss: 0.6740 - val_accuracy: 0.7949 - 64s/epoch - 35ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 62s - loss: 0.8354 - accuracy: 0.7529 - val_loss: 0.6700 - val_accuracy: 0.7982 - 62s/epoch - 34ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 69s - loss: 2.1687 - accuracy: 0.3928 - val_loss: 1.2507 - val_accuracy: 0.6231 - 69s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 59s - loss: 1.3432 - accuracy: 0.6033 - val_loss: 0.9850 - val_accuracy: 0.6986 - 59s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 59s - loss: 1.1516 - accuracy: 0.6575 - val_loss: 0.9089 - val_accuracy: 0.7239 - 59s/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 50s - loss: 1.0466 - accuracy: 0.6855 - val_loss: 0.8129 - val_accuracy: 0.7557 - 50s/epoch - 55ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 59s - loss: 0.9956 - accuracy: 0.7029 - val_loss: 0.8737 - val_accuracy: 0.7315 - 59s/epoch - 65ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 60s - loss: 0.9615 - accuracy: 0.7107 - val_loss: 0.8457 - val_accuracy: 0.7429 - 60s/epoch - 67ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 58s - loss: 0.9409 - accuracy: 0.7180 - val_loss: 0.7405 - val_accuracy: 0.7764 - 58s/epoch - 64ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 61s - loss: 0.8800 - accuracy: 0.7343 - val_loss: 0.7344 - val_accuracy: 0.7732 - 61s/epoch - 67ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 60s - loss: 0.8790 - accuracy: 0.7355 - val_loss: 0.7138 - val_accuracy: 0.7804 - 60s/epoch - 66ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 59s - loss: 0.8361 - accuracy: 0.7488 - val_loss: 0.6978 - val_accuracy: 0.7821 - 59s/epoch - 65ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 118s - loss: 2.1037 - accuracy: 0.4103 - val_loss: 1.1963 - val_accuracy: 0.6392 - 118s/epoch - 130ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 101s - loss: 1.3822 - accuracy: 0.5919 - val_loss: 1.0613 - val_accuracy: 0.6759 - 101s/epoch - 112ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 94s - loss: 1.1698 - accuracy: 0.6513 - val_loss: 0.8555 - val_accuracy: 0.7427 - 94s/epoch - 104ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 90s - loss: 1.0694 - accuracy: 0.6805 - val_loss: 0.8836 - val_accuracy: 0.7293 - 90s/epoch - 99ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 93s - loss: 1.0819 - accuracy: 0.6767 - val_loss: 0.8426 - val_accuracy: 0.7489 - 93s/epoch - 103ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 92s - loss: 0.9971 - accuracy: 0.6995 - val_loss: 0.7443 - val_accuracy: 0.7718 - 92s/epoch - 101ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 93s - loss: 0.9367 - accuracy: 0.7204 - val_loss: 0.7408 - val_accuracy: 0.7795 - 93s/epoch - 102ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 93s - loss: 0.9282 - accuracy: 0.7207 - val_loss: 0.6834 - val_accuracy: 0.7945 - 93s/epoch - 103ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 94s - loss: 0.9016 - accuracy: 0.7295 - val_loss: 0.6862 - val_accuracy: 0.7939 - 94s/epoch - 104ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 92s - loss: 0.8396 - accuracy: 0.7475 - val_loss: 0.6798 - val_accuracy: 0.7910 - 92s/epoch - 102ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 88s - loss: 0.8189 - accuracy: 0.7537 - val_loss: 0.6650 - val_accuracy: 0.7970 - 88s/epoch - 97ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 87s - loss: 0.8193 - accuracy: 0.7542 - val_loss: 0.6548 - val_accuracy: 0.8020 - 87s/epoch - 96ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 89s - loss: 0.7878 - accuracy: 0.7629 - val_loss: 0.6748 - val_accuracy: 0.7946 - 89s/epoch - 98ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 88s - loss: 0.7794 - accuracy: 0.7640 - val_loss: 0.6947 - val_accuracy: 0.7917 - 88s/epoch - 97ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 90s - loss: 0.7724 - accuracy: 0.7663 - val_loss: 0.6232 - val_accuracy: 0.8076 - 90s/epoch - 99ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 88s - loss: 0.7560 - accuracy: 0.7723 - val_loss: 0.6265 - val_accuracy: 0.8094 - 88s/epoch - 97ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 90s - loss: 0.7304 - accuracy: 0.7782 - val_loss: 0.6092 - val_accuracy: 0.8167 - 90s/epoch - 99ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 89s - loss: 0.7648 - accuracy: 0.7683 - val_loss: 0.6040 - val_accuracy: 0.8091 - 89s/epoch - 99ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 90s - loss: 0.7440 - accuracy: 0.7758 - val_loss: 0.5918 - val_accuracy: 0.8207 - 90s/epoch - 99ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 89s - loss: 0.7148 - accuracy: 0.7831 - val_loss: 0.6295 - val_accuracy: 0.8123 - 89s/epoch - 99ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 88s - loss: 2.8880 - accuracy: 0.2114 - val_loss: 1.9152 - val_accuracy: 0.4313 - 88s/epoch - 49ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 75s - loss: 1.9606 - accuracy: 0.4265 - val_loss: 1.5135 - val_accuracy: 0.5418 - 75s/epoch - 41ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 75s - loss: 1.7141 - accuracy: 0.4964 - val_loss: 1.3216 - val_accuracy: 0.6167 - 75s/epoch - 41ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 74s - loss: 1.6374 - accuracy: 0.5199 - val_loss: 1.2035 - val_accuracy: 0.6568 - 74s/epoch - 41ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 74s - loss: 1.4951 - accuracy: 0.5615 - val_loss: 1.1292 - val_accuracy: 0.6808 - 74s/epoch - 41ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 73s - loss: 1.4249 - accuracy: 0.5817 - val_loss: 1.1265 - val_accuracy: 0.6914 - 73s/epoch - 40ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 73s - loss: 1.3747 - accuracy: 0.5957 - val_loss: 1.0877 - val_accuracy: 0.6942 - 73s/epoch - 40ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 74s - loss: 1.3169 - accuracy: 0.6136 - val_loss: 1.0696 - val_accuracy: 0.6943 - 74s/epoch - 41ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 73s - loss: 1.2841 - accuracy: 0.6216 - val_loss: 1.0865 - val_accuracy: 0.6871 - 73s/epoch - 40ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 74s - loss: 1.2587 - accuracy: 0.6307 - val_loss: 1.0754 - val_accuracy: 0.6811 - 74s/epoch - 41ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 70s - loss: 2.9656 - accuracy: 0.1883 - val_loss: 1.9107 - val_accuracy: 0.4328 - 70s/epoch - 38ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 58s - loss: 2.1559 - accuracy: 0.3685 - val_loss: 1.5600 - val_accuracy: 0.5294 - 58s/epoch - 32ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 58s - loss: 1.9294 - accuracy: 0.4344 - val_loss: 1.6689 - val_accuracy: 0.5040 - 58s/epoch - 32ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 57s - loss: 1.8102 - accuracy: 0.4710 - val_loss: 1.3358 - val_accuracy: 0.5978 - 57s/epoch - 31ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 58s - loss: 1.6944 - accuracy: 0.5047 - val_loss: 1.3843 - val_accuracy: 0.5861 - 58s/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 59s - loss: 1.7247 - accuracy: 0.4969 - val_loss: 1.2546 - val_accuracy: 0.6280 - 59s/epoch - 32ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 58s - loss: 1.5653 - accuracy: 0.5430 - val_loss: 1.1360 - val_accuracy: 0.6608 - 58s/epoch - 32ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 58s - loss: 1.5096 - accuracy: 0.5603 - val_loss: 1.1550 - val_accuracy: 0.6565 - 58s/epoch - 32ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 59s - loss: 1.4759 - accuracy: 0.5693 - val_loss: 1.1402 - val_accuracy: 0.6642 - 59s/epoch - 32ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 59s - loss: 1.4318 - accuracy: 0.5816 - val_loss: 1.0620 - val_accuracy: 0.6856 - 59s/epoch - 33ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 57s - loss: 1.4061 - accuracy: 0.5920 - val_loss: 1.1076 - val_accuracy: 0.6686 - 57s/epoch - 31ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 58s - loss: 1.3788 - accuracy: 0.6013 - val_loss: 1.0828 - val_accuracy: 0.6802 - 58s/epoch - 32ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 59s - loss: 1.3507 - accuracy: 0.6056 - val_loss: 1.1181 - val_accuracy: 0.6677 - 59s/epoch - 33ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 58s - loss: 1.3661 - accuracy: 0.6036 - val_loss: 1.0943 - val_accuracy: 0.6799 - 58s/epoch - 32ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 60s - loss: 1.3172 - accuracy: 0.6178 - val_loss: 1.0533 - val_accuracy: 0.6904 - 60s/epoch - 33ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 58s - loss: 1.2815 - accuracy: 0.6282 - val_loss: 1.2224 - val_accuracy: 0.6518 - 58s/epoch - 32ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 59s - loss: 1.2834 - accuracy: 0.6276 - val_loss: 1.0122 - val_accuracy: 0.7084 - 59s/epoch - 32ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 58s - loss: 1.2610 - accuracy: 0.6341 - val_loss: 1.0368 - val_accuracy: 0.7048 - 58s/epoch - 32ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 58s - loss: 1.2425 - accuracy: 0.6414 - val_loss: 1.0248 - val_accuracy: 0.7037 - 58s/epoch - 32ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 59s - loss: 1.2828 - accuracy: 0.6324 - val_loss: 1.1021 - val_accuracy: 0.6840 - 59s/epoch - 32ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 59s - loss: 3.0186 - accuracy: 0.1848 - val_loss: 1.9392 - val_accuracy: 0.4317 - 59s/epoch - 65ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 49s - loss: 1.9983 - accuracy: 0.4152 - val_loss: 1.3500 - val_accuracy: 0.6011 - 49s/epoch - 54ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 49s - loss: 1.7194 - accuracy: 0.4952 - val_loss: 1.3456 - val_accuracy: 0.5971 - 49s/epoch - 54ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 49s - loss: 1.5301 - accuracy: 0.5480 - val_loss: 1.1968 - val_accuracy: 0.6480 - 49s/epoch - 55ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 48s - loss: 1.6161 - accuracy: 0.5242 - val_loss: 1.0982 - val_accuracy: 0.6731 - 48s/epoch - 53ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 49s - loss: 1.4156 - accuracy: 0.5828 - val_loss: 1.0176 - val_accuracy: 0.6956 - 49s/epoch - 55ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 49s - loss: 1.3523 - accuracy: 0.6023 - val_loss: 0.9910 - val_accuracy: 0.7017 - 49s/epoch - 54ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 50s - loss: 1.3352 - accuracy: 0.6055 - val_loss: 0.9973 - val_accuracy: 0.6979 - 50s/epoch - 55ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 50s - loss: 1.2867 - accuracy: 0.6225 - val_loss: 1.0096 - val_accuracy: 0.6964 - 50s/epoch - 55ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 49s - loss: 1.2571 - accuracy: 0.6274 - val_loss: 0.9291 - val_accuracy: 0.7221 - 49s/epoch - 54ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 55s - loss: 3.0289 - accuracy: 0.1875 - val_loss: 1.7118 - val_accuracy: 0.4871 - 55s/epoch - 61ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 49s - loss: 1.9964 - accuracy: 0.4133 - val_loss: 1.4481 - val_accuracy: 0.5818 - 49s/epoch - 54ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 48s - loss: 1.7583 - accuracy: 0.4846 - val_loss: 1.2385 - val_accuracy: 0.6255 - 48s/epoch - 53ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 48s - loss: 1.5753 - accuracy: 0.5354 - val_loss: 1.1838 - val_accuracy: 0.6458 - 48s/epoch - 53ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 48s - loss: 1.4876 - accuracy: 0.5596 - val_loss: 1.1234 - val_accuracy: 0.6578 - 48s/epoch - 53ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 48s - loss: 1.4348 - accuracy: 0.5775 - val_loss: 1.0618 - val_accuracy: 0.6806 - 48s/epoch - 53ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 48s - loss: 1.3826 - accuracy: 0.5938 - val_loss: 1.0658 - val_accuracy: 0.6767 - 48s/epoch - 53ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 48s - loss: 1.3360 - accuracy: 0.6068 - val_loss: 1.0519 - val_accuracy: 0.6902 - 48s/epoch - 53ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 47s - loss: 1.3375 - accuracy: 0.6056 - val_loss: 0.9634 - val_accuracy: 0.7123 - 47s/epoch - 52ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 48s - loss: 1.2639 - accuracy: 0.6273 - val_loss: 0.9261 - val_accuracy: 0.7207 - 48s/epoch - 53ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 48s - loss: 1.2710 - accuracy: 0.6247 - val_loss: 0.8898 - val_accuracy: 0.7296 - 48s/epoch - 53ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 47s - loss: 1.2072 - accuracy: 0.6434 - val_loss: 0.9147 - val_accuracy: 0.7280 - 47s/epoch - 52ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 47s - loss: 1.2005 - accuracy: 0.6451 - val_loss: 0.8638 - val_accuracy: 0.7392 - 47s/epoch - 52ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 46s - loss: 1.1800 - accuracy: 0.6514 - val_loss: 0.8215 - val_accuracy: 0.7518 - 46s/epoch - 51ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 44s - loss: 1.1668 - accuracy: 0.6561 - val_loss: 0.8747 - val_accuracy: 0.7360 - 44s/epoch - 48ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 49s - loss: 1.1667 - accuracy: 0.6550 - val_loss: 0.8772 - val_accuracy: 0.7352 - 49s/epoch - 54ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 50s - loss: 1.1722 - accuracy: 0.6541 - val_loss: 0.8474 - val_accuracy: 0.7543 - 50s/epoch - 55ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 49s - loss: 1.1178 - accuracy: 0.6718 - val_loss: 0.8514 - val_accuracy: 0.7404 - 49s/epoch - 54ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 49s - loss: 1.1298 - accuracy: 0.6689 - val_loss: 0.8762 - val_accuracy: 0.7339 - 49s/epoch - 54ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 49s - loss: 1.1906 - accuracy: 0.6485 - val_loss: 0.8647 - val_accuracy: 0.7433 - 49s/epoch - 54ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 179s - loss: 2.2579 - accuracy: 0.3634 - val_loss: 1.4015 - val_accuracy: 0.5800 - 179s/epoch - 99ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 162s - loss: 1.4520 - accuracy: 0.5709 - val_loss: 1.2543 - val_accuracy: 0.6231 - 162s/epoch - 89ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 159s - loss: 1.3048 - accuracy: 0.6151 - val_loss: 1.0973 - val_accuracy: 0.6667 - 159s/epoch - 88ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 166s - loss: 1.1778 - accuracy: 0.6499 - val_loss: 1.0563 - val_accuracy: 0.6786 - 166s/epoch - 91ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 166s - loss: 1.1349 - accuracy: 0.6622 - val_loss: 0.8829 - val_accuracy: 0.7346 - 166s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 166s - loss: 1.0741 - accuracy: 0.6824 - val_loss: 0.8584 - val_accuracy: 0.7411 - 166s/epoch - 92ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 165s - loss: 1.0262 - accuracy: 0.6956 - val_loss: 0.8829 - val_accuracy: 0.7345 - 165s/epoch - 91ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 165s - loss: 1.0391 - accuracy: 0.6915 - val_loss: 0.8633 - val_accuracy: 0.7407 - 165s/epoch - 91ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 167s - loss: 1.0289 - accuracy: 0.6960 - val_loss: 0.8151 - val_accuracy: 0.7588 - 167s/epoch - 92ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 166s - loss: 0.9544 - accuracy: 0.7156 - val_loss: 0.8180 - val_accuracy: 0.7571 - 166s/epoch - 91ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 151s - loss: 2.2424 - accuracy: 0.3632 - val_loss: 1.3726 - val_accuracy: 0.5940 - 151s/epoch - 83ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 127s - loss: 1.4735 - accuracy: 0.5626 - val_loss: 1.1462 - val_accuracy: 0.6574 - 127s/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 126s - loss: 1.2847 - accuracy: 0.6187 - val_loss: 0.9925 - val_accuracy: 0.6899 - 126s/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 125s - loss: 1.2017 - accuracy: 0.6431 - val_loss: 0.9956 - val_accuracy: 0.7020 - 125s/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 124s - loss: 1.1085 - accuracy: 0.6695 - val_loss: 0.8904 - val_accuracy: 0.7258 - 124s/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 125s - loss: 1.0697 - accuracy: 0.6825 - val_loss: 0.9343 - val_accuracy: 0.7218 - 125s/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 124s - loss: 1.0222 - accuracy: 0.6956 - val_loss: 0.8401 - val_accuracy: 0.7457 - 124s/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 125s - loss: 1.0112 - accuracy: 0.6996 - val_loss: 0.7633 - val_accuracy: 0.7643 - 125s/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 113s - loss: 0.9630 - accuracy: 0.7143 - val_loss: 0.7986 - val_accuracy: 0.7561 - 113s/epoch - 62ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 117s - loss: 0.9457 - accuracy: 0.7180 - val_loss: 0.7780 - val_accuracy: 0.7627 - 117s/epoch - 64ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 114s - loss: 0.9119 - accuracy: 0.7296 - val_loss: 0.7844 - val_accuracy: 0.7692 - 114s/epoch - 63ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 115s - loss: 0.8779 - accuracy: 0.7400 - val_loss: 0.7566 - val_accuracy: 0.7757 - 115s/epoch - 64ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 115s - loss: 0.8485 - accuracy: 0.7462 - val_loss: 0.7334 - val_accuracy: 0.7804 - 115s/epoch - 64ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 114s - loss: 0.8292 - accuracy: 0.7523 - val_loss: 0.7023 - val_accuracy: 0.7870 - 114s/epoch - 63ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 115s - loss: 0.8503 - accuracy: 0.7474 - val_loss: 0.7596 - val_accuracy: 0.7714 - 115s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 114s - loss: 0.8598 - accuracy: 0.7432 - val_loss: 0.6589 - val_accuracy: 0.7980 - 114s/epoch - 63ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 114s - loss: 0.8087 - accuracy: 0.7596 - val_loss: 0.8116 - val_accuracy: 0.7555 - 114s/epoch - 63ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 113s - loss: 0.8040 - accuracy: 0.7629 - val_loss: 0.6994 - val_accuracy: 0.7946 - 113s/epoch - 62ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 113s - loss: 0.7684 - accuracy: 0.7706 - val_loss: 0.7407 - val_accuracy: 0.7879 - 113s/epoch - 62ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 111s - loss: 0.7730 - accuracy: 0.7691 - val_loss: 0.6972 - val_accuracy: 0.7899 - 111s/epoch - 62ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 97s - loss: 2.2330 - accuracy: 0.3803 - val_loss: 1.3698 - val_accuracy: 0.5887 - 97s/epoch - 107ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 82s - loss: 1.3428 - accuracy: 0.6049 - val_loss: 0.9779 - val_accuracy: 0.7026 - 82s/epoch - 91ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 85s - loss: 1.1531 - accuracy: 0.6585 - val_loss: 0.8739 - val_accuracy: 0.7332 - 85s/epoch - 93ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 85s - loss: 1.0473 - accuracy: 0.6895 - val_loss: 0.8710 - val_accuracy: 0.7324 - 85s/epoch - 94ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.0117 - accuracy: 0.6992 - val_loss: 0.8407 - val_accuracy: 0.7429 - 83s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 83s - loss: 0.9324 - accuracy: 0.7220 - val_loss: 0.7856 - val_accuracy: 0.7610 - 83s/epoch - 91ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 84s - loss: 0.8938 - accuracy: 0.7318 - val_loss: 0.7594 - val_accuracy: 0.7658 - 84s/epoch - 92ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 79s - loss: 1.0309 - accuracy: 0.6950 - val_loss: 0.8318 - val_accuracy: 0.7436 - 79s/epoch - 87ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 79s - loss: 0.8684 - accuracy: 0.7410 - val_loss: 0.7589 - val_accuracy: 0.7732 - 79s/epoch - 87ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 77s - loss: 0.8645 - accuracy: 0.7411 - val_loss: 0.6869 - val_accuracy: 0.7891 - 77s/epoch - 85ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 124s - loss: 2.1900 - accuracy: 0.3838 - val_loss: 1.5130 - val_accuracy: 0.5768 - 124s/epoch - 137ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 114s - loss: 1.3573 - accuracy: 0.5967 - val_loss: 1.1052 - val_accuracy: 0.6712 - 114s/epoch - 126ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 115s - loss: 1.1634 - accuracy: 0.6522 - val_loss: 1.0279 - val_accuracy: 0.6877 - 115s/epoch - 127ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 116s - loss: 1.0644 - accuracy: 0.6838 - val_loss: 0.9607 - val_accuracy: 0.7045 - 116s/epoch - 128ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 116s - loss: 1.0069 - accuracy: 0.6996 - val_loss: 0.9109 - val_accuracy: 0.7255 - 116s/epoch - 128ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 114s - loss: 0.9642 - accuracy: 0.7115 - val_loss: 0.7714 - val_accuracy: 0.7632 - 114s/epoch - 126ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 113s - loss: 0.9625 - accuracy: 0.7128 - val_loss: 0.8002 - val_accuracy: 0.7583 - 113s/epoch - 125ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 113s - loss: 0.8855 - accuracy: 0.7346 - val_loss: 0.7384 - val_accuracy: 0.7766 - 113s/epoch - 125ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 116s - loss: 0.8643 - accuracy: 0.7434 - val_loss: 0.7466 - val_accuracy: 0.7754 - 116s/epoch - 128ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 120s - loss: 0.8752 - accuracy: 0.7362 - val_loss: 0.7177 - val_accuracy: 0.7833 - 120s/epoch - 132ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 122s - loss: 0.8409 - accuracy: 0.7489 - val_loss: 0.7037 - val_accuracy: 0.7901 - 122s/epoch - 135ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 120s - loss: 0.7961 - accuracy: 0.7611 - val_loss: 0.7923 - val_accuracy: 0.7646 - 120s/epoch - 133ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 121s - loss: 0.8429 - accuracy: 0.7467 - val_loss: 0.6841 - val_accuracy: 0.7924 - 121s/epoch - 133ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 122s - loss: 0.7736 - accuracy: 0.7683 - val_loss: 0.6431 - val_accuracy: 0.8033 - 122s/epoch - 135ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 120s - loss: 0.7438 - accuracy: 0.7769 - val_loss: 0.7293 - val_accuracy: 0.7818 - 120s/epoch - 133ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 120s - loss: 0.7844 - accuracy: 0.7632 - val_loss: 0.6508 - val_accuracy: 0.8011 - 120s/epoch - 132ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 122s - loss: 0.7223 - accuracy: 0.7830 - val_loss: 0.6574 - val_accuracy: 0.8032 - 122s/epoch - 135ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 121s - loss: 0.7328 - accuracy: 0.7800 - val_loss: 0.6249 - val_accuracy: 0.8086 - 121s/epoch - 133ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 123s - loss: 0.7363 - accuracy: 0.7790 - val_loss: 0.6709 - val_accuracy: 0.7954 - 123s/epoch - 136ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 124s - loss: 0.7655 - accuracy: 0.7711 - val_loss: 0.6317 - val_accuracy: 0.8082 - 124s/epoch - 137ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 117s - loss: 3.0535 - accuracy: 0.1625 - val_loss: 2.7620 - val_accuracy: 0.2698 - 117s/epoch - 64ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 108s - loss: 2.1239 - accuracy: 0.3713 - val_loss: 2.3919 - val_accuracy: 0.3658 - 108s/epoch - 60ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 112s - loss: 1.9530 - accuracy: 0.4224 - val_loss: 2.4522 - val_accuracy: 0.3970 - 112s/epoch - 62ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 98s - loss: 1.7526 - accuracy: 0.4826 - val_loss: 2.4380 - val_accuracy: 0.4485 - 98s/epoch - 54ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 100s - loss: 1.6485 - accuracy: 0.5162 - val_loss: 2.4183 - val_accuracy: 0.5046 - 100s/epoch - 55ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 101s - loss: 1.5742 - accuracy: 0.5394 - val_loss: 2.7948 - val_accuracy: 0.4716 - 101s/epoch - 56ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 99s - loss: 1.5039 - accuracy: 0.5608 - val_loss: 2.4881 - val_accuracy: 0.5284 - 99s/epoch - 55ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 99s - loss: 1.4862 - accuracy: 0.5687 - val_loss: 3.1226 - val_accuracy: 0.4710 - 99s/epoch - 55ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 99s - loss: 1.4460 - accuracy: 0.5825 - val_loss: 4.4008 - val_accuracy: 0.4173 - 99s/epoch - 54ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 100s - loss: 1.4243 - accuracy: 0.5884 - val_loss: 4.9663 - val_accuracy: 0.3601 - 100s/epoch - 55ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 94s - loss: 3.0813 - accuracy: 0.1578 - val_loss: 3.2880 - val_accuracy: 0.2062 - 94s/epoch - 52ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 83s - loss: 2.1317 - accuracy: 0.3713 - val_loss: 2.3476 - val_accuracy: 0.3644 - 83s/epoch - 46ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 83s - loss: 1.8721 - accuracy: 0.4501 - val_loss: 2.0342 - val_accuracy: 0.4794 - 83s/epoch - 46ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 82s - loss: 1.7157 - accuracy: 0.4986 - val_loss: 2.2179 - val_accuracy: 0.4548 - 82s/epoch - 45ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 83s - loss: 1.5992 - accuracy: 0.5313 - val_loss: 2.2584 - val_accuracy: 0.4891 - 83s/epoch - 46ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 82s - loss: 1.5171 - accuracy: 0.5571 - val_loss: 1.8729 - val_accuracy: 0.5357 - 82s/epoch - 45ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 83s - loss: 1.4528 - accuracy: 0.5796 - val_loss: 1.6923 - val_accuracy: 0.5993 - 83s/epoch - 46ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 84s - loss: 1.3931 - accuracy: 0.5973 - val_loss: 2.0158 - val_accuracy: 0.5949 - 84s/epoch - 46ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 84s - loss: 1.3533 - accuracy: 0.6103 - val_loss: 2.3544 - val_accuracy: 0.5563 - 84s/epoch - 46ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 83s - loss: 1.2931 - accuracy: 0.6259 - val_loss: 1.9286 - val_accuracy: 0.6159 - 83s/epoch - 46ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 83s - loss: 1.2578 - accuracy: 0.6364 - val_loss: 1.9597 - val_accuracy: 0.6339 - 83s/epoch - 46ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 83s - loss: 1.2317 - accuracy: 0.6415 - val_loss: 2.5244 - val_accuracy: 0.5783 - 83s/epoch - 46ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 83s - loss: 1.1952 - accuracy: 0.6567 - val_loss: 2.8713 - val_accuracy: 0.5465 - 83s/epoch - 46ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 83s - loss: 1.2054 - accuracy: 0.6555 - val_loss: 3.0115 - val_accuracy: 0.5147 - 83s/epoch - 46ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 83s - loss: 1.1550 - accuracy: 0.6689 - val_loss: 3.0945 - val_accuracy: 0.5024 - 83s/epoch - 46ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 83s - loss: 1.1417 - accuracy: 0.6716 - val_loss: 3.5205 - val_accuracy: 0.4940 - 83s/epoch - 46ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 86s - loss: 1.1388 - accuracy: 0.6729 - val_loss: 4.2846 - val_accuracy: 0.4020 - 86s/epoch - 48ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 86s - loss: 1.1270 - accuracy: 0.6772 - val_loss: 4.3633 - val_accuracy: 0.4061 - 86s/epoch - 47ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 86s - loss: 1.1022 - accuracy: 0.6836 - val_loss: 6.1380 - val_accuracy: 0.3235 - 86s/epoch - 48ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 85s - loss: 1.1654 - accuracy: 0.6679 - val_loss: 5.6617 - val_accuracy: 0.3666 - 85s/epoch - 47ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 91s - loss: 3.1123 - accuracy: 0.1646 - val_loss: 2.4138 - val_accuracy: 0.3257 - 91s/epoch - 100ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 84s - loss: 2.1049 - accuracy: 0.3760 - val_loss: 1.8780 - val_accuracy: 0.4726 - 84s/epoch - 92ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 84s - loss: 1.7657 - accuracy: 0.4753 - val_loss: 1.7721 - val_accuracy: 0.5051 - 84s/epoch - 93ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 84s - loss: 1.6150 - accuracy: 0.5218 - val_loss: 1.4239 - val_accuracy: 0.5933 - 84s/epoch - 92ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.5067 - accuracy: 0.5527 - val_loss: 1.5120 - val_accuracy: 0.5716 - 83s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 84s - loss: 1.4321 - accuracy: 0.5781 - val_loss: 1.2837 - val_accuracy: 0.6364 - 84s/epoch - 93ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 84s - loss: 1.3546 - accuracy: 0.6006 - val_loss: 1.2469 - val_accuracy: 0.6453 - 84s/epoch - 93ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 84s - loss: 1.3080 - accuracy: 0.6120 - val_loss: 1.2488 - val_accuracy: 0.6337 - 84s/epoch - 93ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 85s - loss: 1.2928 - accuracy: 0.6189 - val_loss: 1.2006 - val_accuracy: 0.6581 - 85s/epoch - 94ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 85s - loss: 1.2488 - accuracy: 0.6341 - val_loss: 1.1134 - val_accuracy: 0.6762 - 85s/epoch - 94ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 156s - loss: 3.1456 - accuracy: 0.1565 - val_loss: 1.8228 - val_accuracy: 0.4700 - 156s/epoch - 172ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 142s - loss: 1.9936 - accuracy: 0.4098 - val_loss: 2.1167 - val_accuracy: 0.4406 - 142s/epoch - 157ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 142s - loss: 1.7639 - accuracy: 0.4768 - val_loss: 1.8694 - val_accuracy: 0.4994 - 142s/epoch - 156ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 143s - loss: 1.5857 - accuracy: 0.5288 - val_loss: 1.8316 - val_accuracy: 0.5278 - 143s/epoch - 157ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 145s - loss: 1.4678 - accuracy: 0.5672 - val_loss: 1.7980 - val_accuracy: 0.5446 - 145s/epoch - 160ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 144s - loss: 1.3794 - accuracy: 0.5925 - val_loss: 1.5471 - val_accuracy: 0.5908 - 144s/epoch - 159ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 145s - loss: 1.3246 - accuracy: 0.6120 - val_loss: 1.3590 - val_accuracy: 0.6355 - 145s/epoch - 160ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 145s - loss: 1.2829 - accuracy: 0.6223 - val_loss: 1.3267 - val_accuracy: 0.6478 - 145s/epoch - 160ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 145s - loss: 1.2494 - accuracy: 0.6312 - val_loss: 1.4039 - val_accuracy: 0.6322 - 145s/epoch - 160ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 146s - loss: 1.2262 - accuracy: 0.6406 - val_loss: 1.3439 - val_accuracy: 0.6400 - 146s/epoch - 161ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 143s - loss: 1.1947 - accuracy: 0.6506 - val_loss: 1.3316 - val_accuracy: 0.6648 - 143s/epoch - 158ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 146s - loss: 1.1621 - accuracy: 0.6603 - val_loss: 1.3027 - val_accuracy: 0.6598 - 146s/epoch - 161ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 144s - loss: 1.1500 - accuracy: 0.6644 - val_loss: 1.2143 - val_accuracy: 0.6842 - 144s/epoch - 159ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 144s - loss: 1.1411 - accuracy: 0.6677 - val_loss: 1.1371 - val_accuracy: 0.7079 - 144s/epoch - 159ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 144s - loss: 1.0896 - accuracy: 0.6828 - val_loss: 1.2513 - val_accuracy: 0.6977 - 144s/epoch - 159ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 140s - loss: 1.0583 - accuracy: 0.6912 - val_loss: 1.2410 - val_accuracy: 0.7005 - 140s/epoch - 154ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 142s - loss: 1.0568 - accuracy: 0.6904 - val_loss: 1.4178 - val_accuracy: 0.6845 - 142s/epoch - 157ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 141s - loss: 1.0535 - accuracy: 0.6928 - val_loss: 1.2665 - val_accuracy: 0.6883 - 141s/epoch - 156ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 140s - loss: 1.0699 - accuracy: 0.6881 - val_loss: 1.1796 - val_accuracy: 0.7157 - 140s/epoch - 155ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 140s - loss: 1.0292 - accuracy: 0.6980 - val_loss: 1.1563 - val_accuracy: 0.7167 - 140s/epoch - 154ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 116s - loss: 2.0913 - accuracy: 0.4069 - val_loss: 1.7231 - val_accuracy: 0.4875 - 116s/epoch - 64ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 110s - loss: 1.4050 - accuracy: 0.5861 - val_loss: 1.1407 - val_accuracy: 0.6573 - 110s/epoch - 61ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 112s - loss: 1.2536 - accuracy: 0.6258 - val_loss: 0.9992 - val_accuracy: 0.7074 - 112s/epoch - 62ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 111s - loss: 1.2020 - accuracy: 0.6442 - val_loss: 0.8997 - val_accuracy: 0.7268 - 111s/epoch - 61ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 110s - loss: 1.1406 - accuracy: 0.6608 - val_loss: 0.8691 - val_accuracy: 0.7342 - 110s/epoch - 61ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 112s - loss: 1.0528 - accuracy: 0.6893 - val_loss: 0.8170 - val_accuracy: 0.7496 - 112s/epoch - 62ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 111s - loss: 1.0217 - accuracy: 0.6946 - val_loss: 0.8714 - val_accuracy: 0.7292 - 111s/epoch - 61ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 110s - loss: 1.0073 - accuracy: 0.7004 - val_loss: 0.7972 - val_accuracy: 0.7583 - 110s/epoch - 61ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 111s - loss: 0.9558 - accuracy: 0.7143 - val_loss: 0.7493 - val_accuracy: 0.7680 - 111s/epoch - 61ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 111s - loss: 0.9411 - accuracy: 0.7189 - val_loss: 0.8242 - val_accuracy: 0.7474 - 111s/epoch - 61ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 108s - loss: 2.0844 - accuracy: 0.4110 - val_loss: 1.3543 - val_accuracy: 0.5866 - 108s/epoch - 60ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 102s - loss: 1.3399 - accuracy: 0.6043 - val_loss: 1.0398 - val_accuracy: 0.6852 - 102s/epoch - 56ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 103s - loss: 1.1925 - accuracy: 0.6460 - val_loss: 0.9425 - val_accuracy: 0.7124 - 103s/epoch - 57ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 104s - loss: 1.0934 - accuracy: 0.6750 - val_loss: 0.9392 - val_accuracy: 0.7142 - 104s/epoch - 58ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 102s - loss: 1.0317 - accuracy: 0.6914 - val_loss: 0.9200 - val_accuracy: 0.7171 - 102s/epoch - 56ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 110s - loss: 0.9809 - accuracy: 0.7061 - val_loss: 0.8001 - val_accuracy: 0.7563 - 110s/epoch - 61ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 109s - loss: 0.9275 - accuracy: 0.7221 - val_loss: 0.7608 - val_accuracy: 0.7676 - 109s/epoch - 60ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 111s - loss: 0.9197 - accuracy: 0.7261 - val_loss: 0.7782 - val_accuracy: 0.7632 - 111s/epoch - 61ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 109s - loss: 0.8878 - accuracy: 0.7348 - val_loss: 0.7798 - val_accuracy: 0.7523 - 109s/epoch - 60ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 112s - loss: 0.8541 - accuracy: 0.7459 - val_loss: 0.7193 - val_accuracy: 0.7788 - 112s/epoch - 62ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 109s - loss: 0.8355 - accuracy: 0.7503 - val_loss: 0.6982 - val_accuracy: 0.7876 - 109s/epoch - 60ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 109s - loss: 0.8194 - accuracy: 0.7553 - val_loss: 0.7066 - val_accuracy: 0.7852 - 109s/epoch - 60ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 112s - loss: 0.7894 - accuracy: 0.7653 - val_loss: 0.7040 - val_accuracy: 0.7842 - 112s/epoch - 62ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 109s - loss: 0.7944 - accuracy: 0.7629 - val_loss: 0.6959 - val_accuracy: 0.7896 - 109s/epoch - 60ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 111s - loss: 0.7945 - accuracy: 0.7625 - val_loss: 0.6345 - val_accuracy: 0.8132 - 111s/epoch - 61ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 109s - loss: 0.7555 - accuracy: 0.7728 - val_loss: 0.6470 - val_accuracy: 0.8070 - 109s/epoch - 60ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 111s - loss: 0.7824 - accuracy: 0.7666 - val_loss: 0.6245 - val_accuracy: 0.8071 - 111s/epoch - 61ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 111s - loss: 0.7727 - accuracy: 0.7673 - val_loss: 0.6770 - val_accuracy: 0.7971 - 111s/epoch - 61ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 110s - loss: 0.7437 - accuracy: 0.7755 - val_loss: 0.6581 - val_accuracy: 0.7992 - 110s/epoch - 60ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 112s - loss: 0.7234 - accuracy: 0.7852 - val_loss: 0.6973 - val_accuracy: 0.7870 - 112s/epoch - 62ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 91s - loss: 2.1364 - accuracy: 0.4027 - val_loss: 1.1991 - val_accuracy: 0.6402 - 91s/epoch - 100ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 82s - loss: 1.3895 - accuracy: 0.5904 - val_loss: 1.0185 - val_accuracy: 0.6884 - 82s/epoch - 90ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 79s - loss: 1.1817 - accuracy: 0.6511 - val_loss: 0.9145 - val_accuracy: 0.7164 - 79s/epoch - 88ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 81s - loss: 1.0897 - accuracy: 0.6749 - val_loss: 0.8276 - val_accuracy: 0.7435 - 81s/epoch - 89ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.0082 - accuracy: 0.6996 - val_loss: 0.7773 - val_accuracy: 0.7643 - 83s/epoch - 91ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 77s - loss: 0.9596 - accuracy: 0.7139 - val_loss: 0.7566 - val_accuracy: 0.7724 - 77s/epoch - 85ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 80s - loss: 0.9627 - accuracy: 0.7117 - val_loss: 0.7643 - val_accuracy: 0.7661 - 80s/epoch - 88ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 80s - loss: 0.8922 - accuracy: 0.7325 - val_loss: 0.7254 - val_accuracy: 0.7779 - 80s/epoch - 88ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 77s - loss: 0.8604 - accuracy: 0.7412 - val_loss: 0.6892 - val_accuracy: 0.7901 - 77s/epoch - 85ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 80s - loss: 0.8258 - accuracy: 0.7518 - val_loss: 0.6621 - val_accuracy: 0.7967 - 80s/epoch - 88ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 82s - loss: 2.1200 - accuracy: 0.4042 - val_loss: 1.3426 - val_accuracy: 0.6040 - 82s/epoch - 90ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 78s - loss: 1.3128 - accuracy: 0.6113 - val_loss: 1.0047 - val_accuracy: 0.6995 - 78s/epoch - 86ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 78s - loss: 1.1334 - accuracy: 0.6615 - val_loss: 0.9386 - val_accuracy: 0.7204 - 78s/epoch - 86ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 74s - loss: 1.0405 - accuracy: 0.6899 - val_loss: 0.8517 - val_accuracy: 0.7457 - 74s/epoch - 82ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 78s - loss: 1.0004 - accuracy: 0.7024 - val_loss: 0.7929 - val_accuracy: 0.7596 - 78s/epoch - 86ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 79s - loss: 0.9272 - accuracy: 0.7219 - val_loss: 0.8056 - val_accuracy: 0.7515 - 79s/epoch - 87ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 75s - loss: 0.9008 - accuracy: 0.7295 - val_loss: 0.7642 - val_accuracy: 0.7624 - 75s/epoch - 83ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 78s - loss: 0.8635 - accuracy: 0.7394 - val_loss: 0.7446 - val_accuracy: 0.7729 - 78s/epoch - 86ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 78s - loss: 0.8480 - accuracy: 0.7444 - val_loss: 0.7687 - val_accuracy: 0.7665 - 78s/epoch - 86ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 75s - loss: 0.8852 - accuracy: 0.7358 - val_loss: 0.6602 - val_accuracy: 0.7988 - 75s/epoch - 83ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 78s - loss: 0.7884 - accuracy: 0.7633 - val_loss: 0.6625 - val_accuracy: 0.7974 - 78s/epoch - 86ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 78s - loss: 0.7569 - accuracy: 0.7724 - val_loss: 0.6631 - val_accuracy: 0.7983 - 78s/epoch - 86ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 77s - loss: 0.7507 - accuracy: 0.7730 - val_loss: 0.6863 - val_accuracy: 0.7888 - 77s/epoch - 86ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 79s - loss: 0.7365 - accuracy: 0.7759 - val_loss: 0.6411 - val_accuracy: 0.8049 - 79s/epoch - 88ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 77s - loss: 0.7231 - accuracy: 0.7824 - val_loss: 0.6658 - val_accuracy: 0.7970 - 77s/epoch - 85ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 79s - loss: 0.8101 - accuracy: 0.7572 - val_loss: 0.6893 - val_accuracy: 0.7891 - 79s/epoch - 87ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 78s - loss: 0.7076 - accuracy: 0.7870 - val_loss: 0.6175 - val_accuracy: 0.8071 - 78s/epoch - 86ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 76s - loss: 0.7199 - accuracy: 0.7820 - val_loss: 0.6488 - val_accuracy: 0.8071 - 76s/epoch - 84ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 78s - loss: 0.6955 - accuracy: 0.7907 - val_loss: 0.6826 - val_accuracy: 0.7908 - 78s/epoch - 86ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 77s - loss: 0.6901 - accuracy: 0.7900 - val_loss: 0.6010 - val_accuracy: 0.8129 - 77s/epoch - 85ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 145s - loss: 3.0230 - accuracy: 0.1724 - val_loss: 2.2363 - val_accuracy: 0.3248 - 145s/epoch - 80ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 144s - loss: 2.1690 - accuracy: 0.3603 - val_loss: 1.7557 - val_accuracy: 0.4821 - 144s/epoch - 79ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 146s - loss: 1.8697 - accuracy: 0.4496 - val_loss: 1.5132 - val_accuracy: 0.5363 - 146s/epoch - 81ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 147s - loss: 1.7377 - accuracy: 0.4873 - val_loss: 1.3509 - val_accuracy: 0.5971 - 147s/epoch - 81ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 137s - loss: 1.6301 - accuracy: 0.5221 - val_loss: 1.2486 - val_accuracy: 0.6305 - 137s/epoch - 75ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 146s - loss: 1.5492 - accuracy: 0.5439 - val_loss: 1.1941 - val_accuracy: 0.6427 - 146s/epoch - 81ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 145s - loss: 1.4713 - accuracy: 0.5674 - val_loss: 1.0975 - val_accuracy: 0.6752 - 145s/epoch - 80ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 144s - loss: 1.4047 - accuracy: 0.5864 - val_loss: 1.1295 - val_accuracy: 0.6639 - 144s/epoch - 80ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 147s - loss: 1.3826 - accuracy: 0.5951 - val_loss: 1.0712 - val_accuracy: 0.6842 - 147s/epoch - 81ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 139s - loss: 1.3538 - accuracy: 0.6026 - val_loss: 1.0238 - val_accuracy: 0.6970 - 139s/epoch - 77ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 143s - loss: 3.0259 - accuracy: 0.1751 - val_loss: 2.1750 - val_accuracy: 0.3420 - 143s/epoch - 79ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 133s - loss: 2.1249 - accuracy: 0.3700 - val_loss: 1.6614 - val_accuracy: 0.4957 - 133s/epoch - 74ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 132s - loss: 1.9152 - accuracy: 0.4344 - val_loss: 1.4920 - val_accuracy: 0.5559 - 132s/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 132s - loss: 1.7713 - accuracy: 0.4804 - val_loss: 1.3447 - val_accuracy: 0.6025 - 132s/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 125s - loss: 1.6620 - accuracy: 0.5112 - val_loss: 1.3313 - val_accuracy: 0.5989 - 125s/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 133s - loss: 1.5956 - accuracy: 0.5324 - val_loss: 1.2086 - val_accuracy: 0.6365 - 133s/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 132s - loss: 1.5860 - accuracy: 0.5338 - val_loss: 1.2962 - val_accuracy: 0.6162 - 132s/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 133s - loss: 1.5435 - accuracy: 0.5471 - val_loss: 1.2003 - val_accuracy: 0.6392 - 133s/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 131s - loss: 1.4947 - accuracy: 0.5631 - val_loss: 1.1578 - val_accuracy: 0.6558 - 131s/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 125s - loss: 1.4756 - accuracy: 0.5685 - val_loss: 1.1465 - val_accuracy: 0.6583 - 125s/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 132s - loss: 1.4358 - accuracy: 0.5818 - val_loss: 1.1074 - val_accuracy: 0.6671 - 132s/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 134s - loss: 1.4027 - accuracy: 0.5900 - val_loss: 1.0852 - val_accuracy: 0.6767 - 134s/epoch - 74ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 133s - loss: 1.3661 - accuracy: 0.6016 - val_loss: 1.1139 - val_accuracy: 0.6646 - 133s/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 131s - loss: 1.3677 - accuracy: 0.6011 - val_loss: 1.0974 - val_accuracy: 0.6667 - 131s/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 116s - loss: 1.3811 - accuracy: 0.5963 - val_loss: 1.0212 - val_accuracy: 0.6929 - 116s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 92s - loss: 1.3347 - accuracy: 0.6102 - val_loss: 1.0739 - val_accuracy: 0.6851 - 92s/epoch - 51ms/step\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\models_transformer.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Train the model with your training set and validate it with your validation set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mbatch(batch_size), epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_data\u001b[39m=\u001b[39;49mval\u001b[39m.\u001b[39;49mbatch(batch_size), verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                 \u001b[39m# Record the results for the current configuration\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m row \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m: num_heads,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m: num_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mmax(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (MultiHeadAttention, Dropout, LayerNormalization, Dense, TimeDistributed,\n",
    "                                     BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "results = pd.DataFrame(columns=['num_heads', 'num_layers', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(embed_dim, activation='relu'),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "for num_heads in [2, 4]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for dropout_rate in [0.2, 0.4]:\n",
    "            for batch_size in [32, 64]:\n",
    "                for epochs in [ 10, 20]:\n",
    "                    print(f'num_heads: {num_heads}, num_layers: {num_layers}, dropout_rate: {dropout_rate}, batch_size: {batch_size}, epochs: {epochs}')\n",
    "                    num_classes = len(np.unique(labels))  # Number of unique classes in your dataset\n",
    "                    input_shape = (39, 44)\n",
    "                    embed_dim = 128\n",
    "\n",
    "                    inputs = tf.keras.Input(shape=input_shape)\n",
    "                    x = Conv1D(filters=128, kernel_size=3, activation='relu')(inputs)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = MaxPooling1D(pool_size=2)(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "                    for _ in range(num_layers):\n",
    "                        x = TransformerBlock(embed_dim, num_heads, dropout_rate)(x)\n",
    "\n",
    "                    x = GlobalMaxPooling1D()(x)\n",
    "                    x = Dense(512, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(256, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(128, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "                    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "                    # Compile the model\n",
    "                    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                    # Train the model with your training set and validate it with your validation set\n",
    "                    history = model.fit(train.batch(batch_size), epochs=epochs, batch_size=batch_size, validation_data=val.batch(batch_size), verbose=2)\n",
    "                                    # Record the results for the current configuration\n",
    "                    row = {\n",
    "                        'num_heads': num_heads,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'epoch': epochs,\n",
    "                        'batch': batch_size,\n",
    "                        'loss': history.history['loss'][-1],\n",
    "                        'loss_max': np.max(history.history['loss']),\n",
    "                        'accuracy': history.history['accuracy'][-1],\n",
    "                        'accuracy_max': np.max(history.history['accuracy']),\n",
    "                        'val_loss': history.history['val_loss'][-1],\n",
    "                        'val_loss_max': np.max(history.history['val_loss']),\n",
    "                        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "                        'val_accuracy_max': np.max(history.history['val_accuracy']),\n",
    "                    }\n",
    "\n",
    "                    results = results.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.020137</td>\n",
       "      <td>2.234147</td>\n",
       "      <td>0.694923</td>\n",
       "      <td>0.695544</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>1.408424</td>\n",
       "      <td>0.765225</td>\n",
       "      <td>0.765225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.835371</td>\n",
       "      <td>2.106825</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.670023</td>\n",
       "      <td>1.220319</td>\n",
       "      <td>0.798176</td>\n",
       "      <td>0.798176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.836061</td>\n",
       "      <td>2.168659</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.697845</td>\n",
       "      <td>1.250656</td>\n",
       "      <td>0.782142</td>\n",
       "      <td>0.782142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.714789</td>\n",
       "      <td>2.103729</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.629499</td>\n",
       "      <td>1.196326</td>\n",
       "      <td>0.812298</td>\n",
       "      <td>0.820683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.258716</td>\n",
       "      <td>2.888041</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>1.075352</td>\n",
       "      <td>1.915189</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.694322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.282760</td>\n",
       "      <td>2.965580</td>\n",
       "      <td>0.632391</td>\n",
       "      <td>0.641438</td>\n",
       "      <td>1.102142</td>\n",
       "      <td>1.910666</td>\n",
       "      <td>0.684025</td>\n",
       "      <td>0.708444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.257138</td>\n",
       "      <td>3.018563</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>1.939173</td>\n",
       "      <td>0.722124</td>\n",
       "      <td>0.722124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.190626</td>\n",
       "      <td>3.028895</td>\n",
       "      <td>0.648482</td>\n",
       "      <td>0.671771</td>\n",
       "      <td>0.864657</td>\n",
       "      <td>1.711817</td>\n",
       "      <td>0.743307</td>\n",
       "      <td>0.754340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>2.257917</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.818004</td>\n",
       "      <td>1.401505</td>\n",
       "      <td>0.757134</td>\n",
       "      <td>0.758753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.772999</td>\n",
       "      <td>2.242363</td>\n",
       "      <td>0.769142</td>\n",
       "      <td>0.770644</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>1.372575</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>0.798029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>2.232953</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>1.369754</td>\n",
       "      <td>0.789056</td>\n",
       "      <td>0.789056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.765530</td>\n",
       "      <td>2.189961</td>\n",
       "      <td>0.771058</td>\n",
       "      <td>0.783022</td>\n",
       "      <td>0.631657</td>\n",
       "      <td>1.512975</td>\n",
       "      <td>0.808179</td>\n",
       "      <td>0.808620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.424297</td>\n",
       "      <td>3.053465</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>0.360106</td>\n",
       "      <td>0.528391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.165354</td>\n",
       "      <td>3.081251</td>\n",
       "      <td>0.667938</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>5.661652</td>\n",
       "      <td>6.137986</td>\n",
       "      <td>0.366578</td>\n",
       "      <td>0.633863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.248828</td>\n",
       "      <td>3.112274</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>1.113379</td>\n",
       "      <td>2.413799</td>\n",
       "      <td>0.676228</td>\n",
       "      <td>0.676228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.029160</td>\n",
       "      <td>3.145577</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>1.156267</td>\n",
       "      <td>2.116711</td>\n",
       "      <td>0.716681</td>\n",
       "      <td>0.716681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.941052</td>\n",
       "      <td>2.091297</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>1.723114</td>\n",
       "      <td>0.747426</td>\n",
       "      <td>0.768020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>2.084395</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.697317</td>\n",
       "      <td>1.354331</td>\n",
       "      <td>0.786996</td>\n",
       "      <td>0.813180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.825751</td>\n",
       "      <td>2.136443</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.662067</td>\n",
       "      <td>1.199146</td>\n",
       "      <td>0.796705</td>\n",
       "      <td>0.796705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>2.120008</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.790722</td>\n",
       "      <td>0.600954</td>\n",
       "      <td>1.342593</td>\n",
       "      <td>0.812886</td>\n",
       "      <td>0.812886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.353759</td>\n",
       "      <td>3.022973</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>1.023833</td>\n",
       "      <td>2.236273</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_heads  num_layers  dropout_rate  epoch  batch      loss  loss_max  \\\n",
       "0         2.0         1.0           0.2   10.0   32.0  1.020137  2.234147   \n",
       "1         2.0         1.0           0.2   20.0   32.0  0.835371  2.106825   \n",
       "2         2.0         1.0           0.2   10.0   64.0  0.836061  2.168659   \n",
       "3         2.0         1.0           0.2   20.0   64.0  0.714789  2.103729   \n",
       "4         2.0         1.0           0.4   10.0   32.0  1.258716  2.888041   \n",
       "5         2.0         1.0           0.4   20.0   32.0  1.282760  2.965580   \n",
       "6         2.0         1.0           0.4   10.0   64.0  1.257138  3.018563   \n",
       "7         2.0         1.0           0.4   20.0   64.0  1.190626  3.028895   \n",
       "8         2.0         2.0           0.2   10.0   32.0  0.954365  2.257917   \n",
       "9         2.0         2.0           0.2   20.0   32.0  0.772999  2.242363   \n",
       "10        2.0         2.0           0.2   10.0   64.0  0.864500  2.232953   \n",
       "11        2.0         2.0           0.2   20.0   64.0  0.765530  2.189961   \n",
       "12        2.0         2.0           0.4   10.0   32.0  1.424297  3.053465   \n",
       "13        2.0         2.0           0.4   20.0   32.0  1.165354  3.081251   \n",
       "14        2.0         2.0           0.4   10.0   64.0  1.248828  3.112274   \n",
       "15        2.0         2.0           0.4   20.0   64.0  1.029160  3.145577   \n",
       "16        4.0         1.0           0.2   10.0   32.0  0.941052  2.091297   \n",
       "17        4.0         1.0           0.2   20.0   32.0  0.723400  2.084395   \n",
       "18        4.0         1.0           0.2   10.0   64.0  0.825751  2.136443   \n",
       "19        4.0         1.0           0.2   20.0   64.0  0.690125  2.120008   \n",
       "20        4.0         1.0           0.4   10.0   32.0  1.353759  3.022973   \n",
       "\n",
       "    accuracy  accuracy_max  val_loss  val_loss_max  val_accuracy  \\\n",
       "0   0.694923      0.695544  0.749380      1.408424      0.765225   \n",
       "1   0.752862      0.752862  0.670023      1.220319      0.798176   \n",
       "2   0.748770      0.748770  0.697845      1.250656      0.782142   \n",
       "3   0.783126      0.783126  0.629499      1.196326      0.812298   \n",
       "4   0.630682      0.630682  1.075352      1.915189      0.681083   \n",
       "5   0.632391      0.641438  1.102142      1.910666      0.684025   \n",
       "6   0.627350      0.627350  0.929122      1.939173      0.722124   \n",
       "7   0.648482      0.671771  0.864657      1.711817      0.743307   \n",
       "8   0.715571      0.715571  0.818004      1.401505      0.757134   \n",
       "9   0.769142      0.770644  0.697234      1.372575      0.789938   \n",
       "10  0.741070      0.741070  0.686920      1.369754      0.789056   \n",
       "11  0.771058      0.783022  0.631657      1.512975      0.808179   \n",
       "12  0.588350      0.588350  4.966305      4.966305      0.360106   \n",
       "13  0.667938      0.683632  5.661652      6.137986      0.366578   \n",
       "14  0.634100      0.634100  1.113379      2.413799      0.676228   \n",
       "15  0.698030      0.698030  1.156267      2.116711      0.716681   \n",
       "16  0.718885      0.718885  0.824226      1.723114      0.747426   \n",
       "17  0.785215      0.785215  0.697317      1.354331      0.786996   \n",
       "18  0.751774      0.751774  0.662067      1.199146      0.796705   \n",
       "19  0.790032      0.790722  0.600954      1.342593      0.812886   \n",
       "20  0.602645      0.602645  1.023833      2.236273      0.696970   \n",
       "\n",
       "    val_accuracy_max  \n",
       "0           0.765225  \n",
       "1           0.798176  \n",
       "2           0.782142  \n",
       "3           0.820683  \n",
       "4           0.694322  \n",
       "5           0.708444  \n",
       "6           0.722124  \n",
       "7           0.754340  \n",
       "8           0.758753  \n",
       "9           0.798029  \n",
       "10          0.789056  \n",
       "11          0.808620  \n",
       "12          0.528391  \n",
       "13          0.633863  \n",
       "14          0.676228  \n",
       "15          0.716681  \n",
       "16          0.768020  \n",
       "17          0.813180  \n",
       "18          0.796705  \n",
       "19          0.812886  \n",
       "20          0.696970  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results\\\\model_transformer_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
