{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import LABELS, labels_only_detection_training, labels_only_detection_validation, labels_only_detection_full\n",
    "from models import Gru\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6770 - accuracy: 0.4347\n",
      "Epoch 1: val_accuracy improved from -inf to 0.59915, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 68s 156ms/step - loss: 1.6770 - accuracy: 0.4347 - val_loss: 1.1807 - val_accuracy: 0.5991\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.9312 - accuracy: 0.6749\n",
      "Epoch 2: val_accuracy improved from 0.59915 to 0.73962, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 44s 132ms/step - loss: 0.9312 - accuracy: 0.6749 - val_loss: 0.7194 - val_accuracy: 0.7396\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7665 - accuracy: 0.7355\n",
      "Epoch 3: val_accuracy improved from 0.73962 to 0.76368, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 46s 138ms/step - loss: 0.7665 - accuracy: 0.7355 - val_loss: 0.6605 - val_accuracy: 0.7637\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.7715\n",
      "Epoch 4: val_accuracy improved from 0.76368 to 0.77765, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 40s 121ms/step - loss: 0.6555 - accuracy: 0.7715 - val_loss: 0.6368 - val_accuracy: 0.7776\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5908 - accuracy: 0.7975\n",
      "Epoch 5: val_accuracy improved from 0.77765 to 0.80287, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 43s 130ms/step - loss: 0.5908 - accuracy: 0.7975 - val_loss: 0.5822 - val_accuracy: 0.8029\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.8154\n",
      "Epoch 6: val_accuracy improved from 0.80287 to 0.81218, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 49s 147ms/step - loss: 0.5438 - accuracy: 0.8154 - val_loss: 0.5213 - val_accuracy: 0.8122\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.8271\n",
      "Epoch 7: val_accuracy improved from 0.81218 to 0.82693, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 43s 131ms/step - loss: 0.5075 - accuracy: 0.8271 - val_loss: 0.5095 - val_accuracy: 0.8269\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4713 - accuracy: 0.8395\n",
      "Epoch 8: val_accuracy improved from 0.82693 to 0.83042, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 42s 126ms/step - loss: 0.4713 - accuracy: 0.8395 - val_loss: 0.5170 - val_accuracy: 0.8304\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4357 - accuracy: 0.8511\n",
      "Epoch 9: val_accuracy improved from 0.83042 to 0.83780, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 51s 154ms/step - loss: 0.4357 - accuracy: 0.8511 - val_loss: 0.4649 - val_accuracy: 0.8378\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8607\n",
      "Epoch 10: val_accuracy did not improve from 0.83780\n",
      "330/330 [==============================] - 48s 145ms/step - loss: 0.4117 - accuracy: 0.8607 - val_loss: 0.5392 - val_accuracy: 0.8246\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3922 - accuracy: 0.8660\n",
      "Epoch 11: val_accuracy improved from 0.83780 to 0.84711, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 43s 131ms/step - loss: 0.3922 - accuracy: 0.8660 - val_loss: 0.4683 - val_accuracy: 0.8471\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3676 - accuracy: 0.8751\n",
      "Epoch 12: val_accuracy did not improve from 0.84711\n",
      "330/330 [==============================] - 40s 122ms/step - loss: 0.3676 - accuracy: 0.8751 - val_loss: 0.5179 - val_accuracy: 0.8308\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3501 - accuracy: 0.8818\n",
      "Epoch 13: val_accuracy improved from 0.84711 to 0.85603, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 45s 135ms/step - loss: 0.3501 - accuracy: 0.8818 - val_loss: 0.4401 - val_accuracy: 0.8560\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3318 - accuracy: 0.8871\n",
      "Epoch 14: val_accuracy improved from 0.85603 to 0.85720, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 45s 136ms/step - loss: 0.3318 - accuracy: 0.8871 - val_loss: 0.4416 - val_accuracy: 0.8572\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8937\n",
      "Epoch 15: val_accuracy did not improve from 0.85720\n",
      "330/330 [==============================] - 47s 142ms/step - loss: 0.3206 - accuracy: 0.8937 - val_loss: 0.4933 - val_accuracy: 0.8490\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3035 - accuracy: 0.8977\n",
      "Epoch 16: val_accuracy improved from 0.85720 to 0.86806, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 52s 156ms/step - loss: 0.3035 - accuracy: 0.8977 - val_loss: 0.4280 - val_accuracy: 0.8681\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.2975 - accuracy: 0.8975\n",
      "Epoch 17: val_accuracy did not improve from 0.86806\n",
      "330/330 [==============================] - 49s 147ms/step - loss: 0.2975 - accuracy: 0.8975 - val_loss: 0.4428 - val_accuracy: 0.8525\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.2789 - accuracy: 0.9064\n",
      "Epoch 18: val_accuracy did not improve from 0.86806\n",
      "330/330 [==============================] - 48s 146ms/step - loss: 0.2789 - accuracy: 0.9064 - val_loss: 0.4567 - val_accuracy: 0.8545\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9126\n",
      "Epoch 19: val_accuracy did not improve from 0.86806\n",
      "330/330 [==============================] - 46s 139ms/step - loss: 0.2585 - accuracy: 0.9126 - val_loss: 0.4788 - val_accuracy: 0.8553\n",
      "Epoch 20/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.2510 - accuracy: 0.9156\n",
      "Epoch 20: val_accuracy did not improve from 0.86806\n",
      "330/330 [==============================] - 47s 141ms/step - loss: 0.2510 - accuracy: 0.9156 - val_loss: 0.4775 - val_accuracy: 0.8572\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m lr \u001b[39min\u001b[39;00m [\u001b[39m0.001\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m [\u001b[39m64\u001b[39m, \u001b[39m32\u001b[39m, \u001b[39m128\u001b[39m]:\n\u001b[1;32m----> 7\u001b[0m         model \u001b[39m=\u001b[39m Gru(gru_units\u001b[39m=\u001b[39;49mgru_unit, dropout_rate\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49mlr, num_classes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[0;32m      8\u001b[0m         model\u001b[39m.\u001b[39mtrain(labels_only_detection_training, labels_only_detection_validation)\n\u001b[0;32m      9\u001b[0m         res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((res, pd\u001b[39m.\u001b[39mDataFrame([[gru_unit,\u001b[39m0.1\u001b[39m, lr, epoch, batch, model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]], \n\u001b[0;32m     10\u001b[0m                                                                 columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgru_units\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\models.py:90\u001b[0m, in \u001b[0;36mGru.__init__\u001b[1;34m(self, gru_units, dropout_rate, epoch, batch_size, learning_rate, input_shape, num_classes, model_path, from_path)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfrom_path)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_model()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\models.py:93\u001b[0m, in \u001b[0;36mGru.build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_model\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m     94\u001b[0m                     Conv1D(filters\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m, input_shape\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_shape),\n\u001b[0;32m     95\u001b[0m                     BatchNormalization(),\n\u001b[0;32m     96\u001b[0m                     MaxPooling1D(pool_size\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m),\n\u001b[0;32m     97\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m     98\u001b[0m                     Bidirectional(GRU(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_units, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)),\n\u001b[0;32m     99\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    100\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    101\u001b[0m                     Bidirectional(GRU(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_units, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)),\n\u001b[0;32m    102\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    103\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    104\u001b[0m                     GRU(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_units, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m    105\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    106\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    107\u001b[0m                     GRU(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru_units, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[0;32m    108\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    109\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    110\u001b[0m                     TimeDistributed(Dense(\u001b[39m256\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[0;32m    111\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    112\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    113\u001b[0m                     TimeDistributed(Dense(\u001b[39m128\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[0;32m    114\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    115\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    116\u001b[0m                     TimeDistributed(Dense(\u001b[39m64\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m)),\n\u001b[0;32m    117\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    118\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    119\u001b[0m                     GlobalMaxPooling1D(),\n\u001b[0;32m    120\u001b[0m                     Dense(\u001b[39m512\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    121\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    122\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    123\u001b[0m                     Dense(\u001b[39m256\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    124\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    125\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    126\u001b[0m                     Dense(\u001b[39m128\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    127\u001b[0m                     BatchNormalization(),\n\u001b[0;32m    128\u001b[0m                     Dropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout_rate),\n\u001b[0;32m    129\u001b[0m                     Dense(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msoftmax\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    130\u001b[0m                 ])\n\u001b[0;32m    132\u001b[0m     model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(), optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate), metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\sequential.py:143\u001b[0m, in \u001b[0;36mSequential.__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    141\u001b[0m     layers \u001b[39m=\u001b[39m [layers]\n\u001b[0;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m layers:\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(layer)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\sequential.py:237\u001b[0m, in \u001b[0;36mSequential.add\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_explicit_input_shape \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs:\n\u001b[0;32m    235\u001b[0m     \u001b[39m# If the model is being built continuously on top of an input layer:\u001b[39;00m\n\u001b[0;32m    236\u001b[0m     \u001b[39m# refresh its output.\u001b[39;00m\n\u001b[1;32m--> 237\u001b[0m     output_tensor \u001b[39m=\u001b[39m layer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutputs[\u001b[39m0\u001b[39;49m])\n\u001b[0;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(output_tensor)) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    239\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(SINGLE_LAYER_OUTPUT_ERROR_MSG)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py:278\u001b[0m, in \u001b[0;36mBidirectional.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     inputs \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    280\u001b[0m \u001b[39m# Applies the same workaround as in `RNN.__call__`\u001b[39;00m\n\u001b[0;32m    281\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:1045\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39m# Functional Model construction mode is invoked when `Layer`s are called\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[39m# on symbolic `KerasTensor`s, i.e.:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39m# >> inputs = tf.keras.Input(10)\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39m# >> outputs = MyLayer()(inputs)  # Functional construction mode.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39m# >> model = tf.keras.Model(inputs, outputs)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m _in_functional_construction_mode(\n\u001b[0;32m   1043\u001b[0m     \u001b[39mself\u001b[39m, inputs, args, kwargs, input_list\n\u001b[0;32m   1044\u001b[0m ):\n\u001b[1;32m-> 1045\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_functional_construction_call(\n\u001b[0;32m   1046\u001b[0m         inputs, args, kwargs, input_list\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1049\u001b[0m \u001b[39m# Maintains info about the `Layer.call` stack.\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m call_context \u001b[39m=\u001b[39m base_layer_utils\u001b[39m.\u001b[39mcall_context()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:2535\u001b[0m, in \u001b[0;36mLayer._functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   2528\u001b[0m         training_arg_passed_by_framework \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m \u001b[39mwith\u001b[39;00m call_context\u001b[39m.\u001b[39menter(\n\u001b[0;32m   2531\u001b[0m     layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, inputs\u001b[39m=\u001b[39minputs, build_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39mtraining_value\n\u001b[0;32m   2532\u001b[0m ):\n\u001b[0;32m   2533\u001b[0m     \u001b[39m# Check input assumptions set after layer building, e.g. input\u001b[39;00m\n\u001b[0;32m   2534\u001b[0m     \u001b[39m# shape.\u001b[39;00m\n\u001b[1;32m-> 2535\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keras_tensor_symbolic_call(\n\u001b[0;32m   2536\u001b[0m         inputs, input_masks, args, kwargs\n\u001b[0;32m   2537\u001b[0m     )\n\u001b[0;32m   2539\u001b[0m     \u001b[39mif\u001b[39;00m outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2541\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA layer\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms `call` method should return a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2542\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTensor or a list of Tensors, not None \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2543\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(layer: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2544\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:2382\u001b[0m, in \u001b[0;36mLayer._keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m   2378\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m   2379\u001b[0m         keras_tensor\u001b[39m.\u001b[39mKerasTensor, output_signature\n\u001b[0;32m   2380\u001b[0m     )\n\u001b[0;32m   2381\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2382\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_infer_output_signature(\n\u001b[0;32m   2383\u001b[0m         inputs, args, kwargs, input_masks\n\u001b[0;32m   2384\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:2441\u001b[0m, in \u001b[0;36mLayer._infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m   2439\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_build(inputs)\n\u001b[0;32m   2440\u001b[0m         inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs)\n\u001b[1;32m-> 2441\u001b[0m         outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   2443\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n\u001b[0;32m   2444\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_mask_metadata(\n\u001b[0;32m   2445\u001b[0m     inputs, outputs, input_masks, build_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2446\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\bidirectional.py:408\u001b[0m, in \u001b[0;36mBidirectional.call\u001b[1;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[0;32m    403\u001b[0m         forward_state, backward_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_layer(\n\u001b[0;32m    406\u001b[0m         forward_inputs, initial_state\u001b[39m=\u001b[39mforward_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    407\u001b[0m     )\n\u001b[1;32m--> 408\u001b[0m     y_rev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward_layer(\n\u001b[0;32m    409\u001b[0m         backward_inputs, initial_state\u001b[39m=\u001b[39mbackward_state, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    411\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    412\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_layer(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[0;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[0;32m    553\u001b[0m )\n\u001b[0;32m    555\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1129\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1131\u001b[0m ):\n\u001b[1;32m-> 1132\u001b[0m     outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\gru.py:669\u001b[0m, in \u001b[0;36mGRU.call\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    667\u001b[0m     runtime \u001b[39m=\u001b[39m gru_lstm_utils\u001b[39m.\u001b[39mruntime(gru_lstm_utils\u001b[39m.\u001b[39mRUNTIME_UNKNOWN)\n\u001b[0;32m    668\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 669\u001b[0m     last_output, outputs, runtime, states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_defun_gru_call(\n\u001b[0;32m    670\u001b[0m         inputs, initial_state, training, mask, row_lengths\n\u001b[0;32m    671\u001b[0m     )\n\u001b[0;32m    673\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstateful:\n\u001b[0;32m    674\u001b[0m     updates \u001b[39m=\u001b[39m [\n\u001b[0;32m    675\u001b[0m         tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39massign(\n\u001b[0;32m    676\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates[\u001b[39m0\u001b[39m], tf\u001b[39m.\u001b[39mcast(states[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    677\u001b[0m         )\n\u001b[0;32m    678\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\gru.py:902\u001b[0m, in \u001b[0;36mGRU._defun_gru_call\u001b[1;34m(self, inputs, initial_state, training, mask, sequence_lengths)\u001b[0m\n\u001b[0;32m    893\u001b[0m             last_output, outputs, new_h, runtime \u001b[39m=\u001b[39m standard_gru(\n\u001b[0;32m    894\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnormal_gru_kwargs\n\u001b[0;32m    895\u001b[0m             )\n\u001b[0;32m    896\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    897\u001b[0m         (\n\u001b[0;32m    898\u001b[0m             last_output,\n\u001b[0;32m    899\u001b[0m             outputs,\n\u001b[0;32m    900\u001b[0m             new_h,\n\u001b[0;32m    901\u001b[0m             runtime,\n\u001b[1;32m--> 902\u001b[0m         ) \u001b[39m=\u001b[39m gru_with_backend_selection(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnormal_gru_kwargs)\n\u001b[0;32m    904\u001b[0m states \u001b[39m=\u001b[39m [new_h]\n\u001b[0;32m    905\u001b[0m \u001b[39mreturn\u001b[39;00m last_output, outputs, runtime, states\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\gru.py:1309\u001b[0m, in \u001b[0;36mgru_with_backend_selection\u001b[1;34m(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask, return_sequences)\u001b[0m\n\u001b[0;32m   1306\u001b[0m     \u001b[39m# Call the normal GRU impl and register the cuDNN impl function. The\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m     \u001b[39m# grappler will kick in during session execution to optimize the graph.\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     last_output, outputs, new_h, runtime \u001b[39m=\u001b[39m defun_standard_gru(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m-> 1309\u001b[0m     gru_lstm_utils\u001b[39m.\u001b[39mfunction_register(defun_gpu_gru, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m   1311\u001b[0m \u001b[39mreturn\u001b[39;00m last_output, outputs, new_h, runtime\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\layers\\rnn\\gru_lstm_utils.py:259\u001b[0m, in \u001b[0;36mfunction_register\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m concrete_func \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39mget_concrete_function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m concrete_func\u001b[39m.\u001b[39madd_to_graph()\n\u001b[1;32m--> 259\u001b[0m concrete_func\u001b[39m.\u001b[39;49madd_gradient_functions_to_graph()\n\u001b[0;32m    260\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_func\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1998\u001b[0m, in \u001b[0;36mConcreteFunction.add_gradient_functions_to_graph\u001b[1;34m(self, g)\u001b[0m\n\u001b[0;32m   1995\u001b[0m   g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[0;32m   1996\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_delayed_rewrite_functions\u001b[39m.\u001b[39mforward()\u001b[39m.\u001b[39madd_to_graph(g)\n\u001b[0;32m   1997\u001b[0m forward_function, backward_function \u001b[39m=\u001b[39m (\n\u001b[1;32m-> 1998\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_delayed_rewrite_functions\u001b[39m.\u001b[39;49mforward_backward())\n\u001b[0;32m   1999\u001b[0m forward_function\u001b[39m.\u001b[39madd_to_graph(g)\n\u001b[0;32m   2000\u001b[0m backward_function\u001b[39m.\u001b[39madd_to_graph(g)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:480\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions.forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m forward_backward \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    479\u001b[0m   \u001b[39mreturn\u001b[39;00m forward_backward\n\u001b[1;32m--> 480\u001b[0m forward, backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_forward_backward(num_doutputs)\n\u001b[0;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_function_pairs[num_doutputs] \u001b[39m=\u001b[39m (forward, backward)\n\u001b[0;32m    482\u001b[0m \u001b[39mreturn\u001b[39;00m forward, backward\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:521\u001b[0m, in \u001b[0;36m_DelayedRewriteGradientFunctions._construct_forward_backward\u001b[1;34m(self, num_doutputs)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m gradients_util\u001b[39m.\u001b[39m_GradientsHelper(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    515\u001b[0m         trainable_outputs,\n\u001b[0;32m    516\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39minputs,\n\u001b[0;32m    517\u001b[0m         grad_ys\u001b[39m=\u001b[39mgrad_ys,\n\u001b[0;32m    518\u001b[0m         src_graph\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph)\n\u001b[0;32m    520\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 521\u001b[0m   backwards_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mFuncGraph(\n\u001b[0;32m    522\u001b[0m       _backward_name(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func_graph\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m    523\u001b[0m   func_graph_module\u001b[39m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    524\u001b[0m       name\u001b[39m=\u001b[39mbackwards_graph\u001b[39m.\u001b[39mname,\n\u001b[0;32m    525\u001b[0m       python_func\u001b[39m=\u001b[39m_backprop_function,\n\u001b[0;32m    526\u001b[0m       args\u001b[39m=\u001b[39m[], kwargs\u001b[39m=\u001b[39m{},\n\u001b[0;32m    527\u001b[0m       signature\u001b[39m=\u001b[39msignature,\n\u001b[0;32m    528\u001b[0m       func_graph\u001b[39m=\u001b[39mbackwards_graph)\n\u001b[0;32m    529\u001b[0m   backwards_graph_captures \u001b[39m=\u001b[39m backwards_graph\u001b[39m.\u001b[39mexternal_captures\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:238\u001b[0m, in \u001b[0;36mFuncGraph.__init__\u001b[1;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[0;32m    210\u001b[0m              name,\n\u001b[0;32m    211\u001b[0m              collections\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    212\u001b[0m              capture_by_value\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    213\u001b[0m              structured_input_signature\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    214\u001b[0m              structured_outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Construct a new FuncGraph.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \n\u001b[0;32m    217\u001b[0m \u001b[39m  The graph will inherit its graph key, collections, seed, and distribution\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m      information.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 238\u001b[0m   \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m()\n\u001b[0;32m    239\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m name\n\u001b[0;32m    240\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3212\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reduced_shape_cache \u001b[39m=\u001b[39m {}\n\u001b[0;32m   3210\u001b[0m \u001b[39m# TODO(skyewm): fold as much of the above as possible into the C\u001b[39;00m\n\u001b[0;32m   3211\u001b[0m \u001b[39m# implementation\u001b[39;00m\n\u001b[1;32m-> 3212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_graph \u001b[39m=\u001b[39m c_api_util\u001b[39m.\u001b[39;49mScopedTFGraph(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph_key)\n\u001b[0;32m   3213\u001b[0m \u001b[39m# The C API requires all ops to have shape functions. Disable this\u001b[39;00m\n\u001b[0;32m   3214\u001b[0m \u001b[39m# requirement (many custom ops do not have shape functions, and we don't\u001b[39;00m\n\u001b[0;32m   3215\u001b[0m \u001b[39m# want to break these existing cases).\u001b[39;00m\n\u001b[0;32m   3216\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_graph\u001b[39m.\u001b[39mget() \u001b[39mas\u001b[39;00m c_graph:\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py:97\u001b[0m, in \u001b[0;36mScopedTFGraph.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m     96\u001b[0m   \u001b[39msuper\u001b[39m(ScopedTFGraph, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m---> 97\u001b[0m       name, obj\u001b[39m=\u001b[39mc_api\u001b[39m.\u001b[39;49mTF_NewGraph(), deleter\u001b[39m=\u001b[39mc_api\u001b[39m.\u001b[39mTF_DeleteGraph)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])\n",
    "\n",
    "for gru_unit in [64,128]:\n",
    "    for epoch in [20, 30]:\n",
    "        for lr in [0.001, 0.01, 0.1]:\n",
    "            for batch in [64, 32, 128]:\n",
    "                model = Gru(gru_units=gru_unit, dropout_rate=0.1, learning_rate=lr, num_classes=10, batch_size=batch, epoch=epoch)\n",
    "                model.train(labels_only_detection_training, labels_only_detection_validation)\n",
    "                res = np.concatenate((res, pd.DataFrame([[gru_unit,0.1, lr, epoch, batch, model.history.history['loss'][-1], model.history.history['accuracy'][-1], model.history.history['val_loss'][-1], model.history.history['val_accuracy'][-1]]], \n",
    "                                                                        columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res,columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max']).to_pickle('..\\\\results\\\\gru_wyniki_only_known.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.9617 - accuracy: 0.5392\n",
      "Epoch 1: accuracy improved from -inf to 0.53913, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 22s 155ms/step - loss: 1.9615 - accuracy: 0.5391\n",
      "Epoch 2/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.4505 - accuracy: 0.6169\n",
      "Epoch 2: accuracy improved from 0.53913 to 0.61695, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 15s 141ms/step - loss: 1.4497 - accuracy: 0.6169\n",
      "Epoch 3/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.2981 - accuracy: 0.6288\n",
      "Epoch 3: accuracy improved from 0.61695 to 0.62871, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.2993 - accuracy: 0.6287\n",
      "Epoch 4/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.1761 - accuracy: 0.6439\n",
      "Epoch 4: accuracy improved from 0.62871 to 0.64342, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 1.1770 - accuracy: 0.6434\n",
      "Epoch 5/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.0653 - accuracy: 0.6630\n",
      "Epoch 5: accuracy improved from 0.64342 to 0.66284, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 1.0653 - accuracy: 0.6628\n",
      "Epoch 6/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.0097 - accuracy: 0.6744\n",
      "Epoch 6: accuracy improved from 0.66284 to 0.67417, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 16s 153ms/step - loss: 1.0101 - accuracy: 0.6742\n",
      "Epoch 7/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9609 - accuracy: 0.6899\n",
      "Epoch 7: accuracy improved from 0.67417 to 0.68947, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.9616 - accuracy: 0.6895\n",
      "Epoch 8/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9457 - accuracy: 0.6971\n",
      "Epoch 8: accuracy improved from 0.68947 to 0.69741, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.9450 - accuracy: 0.6974\n",
      "Epoch 9/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.7046\n",
      "Epoch 9: accuracy improved from 0.69741 to 0.70447, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 18s 165ms/step - loss: 0.9237 - accuracy: 0.7045\n",
      "Epoch 10/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.7000\n",
      "Epoch 10: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 0.9235 - accuracy: 0.6998\n",
      "Epoch 11/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9335 - accuracy: 0.7028\n",
      "Epoch 11: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.9331 - accuracy: 0.7027\n",
      "Epoch 12/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9567 - accuracy: 0.6921\n",
      "Epoch 12: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 148ms/step - loss: 0.9565 - accuracy: 0.6921\n",
      "Epoch 13/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9481 - accuracy: 0.6971\n",
      "Epoch 13: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.9490 - accuracy: 0.6970\n",
      "Epoch 14/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9751 - accuracy: 0.6865\n",
      "Epoch 14: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.9748 - accuracy: 0.6867\n"
     ]
    }
   ],
   "source": [
    "model.train(label_detection_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clip_000044442.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clip_0000adecb.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_0000d4322.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_0000fb6fe.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_0001d1559.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158533</th>\n",
       "      <td>clip_fffe49419.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158534</th>\n",
       "      <td>clip_ffff2fb36.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158535</th>\n",
       "      <td>clip_ffff90f56.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158536</th>\n",
       "      <td>clip_ffff98589.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158537</th>\n",
       "      <td>clip_ffffc7358.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fname    label\n",
       "0       clip_000044442.wav  unknown\n",
       "1       clip_0000adecb.wav  unknown\n",
       "2       clip_0000d4322.wav  unknown\n",
       "3       clip_0000fb6fe.wav  unknown\n",
       "4       clip_0001d1559.wav  unknown\n",
       "...                    ...      ...\n",
       "158533  clip_fffe49419.wav  unknown\n",
       "158534  clip_ffff2fb36.wav       no\n",
       "158535  clip_ffff90f56.wav       no\n",
       "158536  clip_ffff98589.wav       no\n",
       "158537  clip_ffffc7358.wav  unknown\n",
       "\n",
       "[158538 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import label_detection_training, label_detection_validation, silence_detection_training, silence_detection_validation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TensorflowDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "y_train = np.array([x[1] for x in train])\n",
    "labels = list(np.unique(y_train))\n",
    "train = TensorflowDataset('extracted_features\\\\features_training.pkl', labels=labels).dataset\n",
    "train = train.shuffle(len(train), reshuffle_each_iteration=True)\n",
    "val = TensorflowDataset('extracted_features\\\\features_validation.pkl', labels=labels).dataset\n",
    "val = val.shuffle(len(val), reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_units: 64, dropout_rate: 0.2, epoch: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "1811/1811 [==============================] - 129s 63ms/step - loss: 2.6652 - accuracy: 0.2528 - val_loss: 1.9334 - val_accuracy: 0.4479\n",
      "Epoch 2/10\n",
      "1811/1811 [==============================] - 109s 60ms/step - loss: 1.6147 - accuracy: 0.5191 - val_loss: 1.2439 - val_accuracy: 0.6325\n",
      "Epoch 3/10\n",
      "1811/1811 [==============================] - 115s 63ms/step - loss: 1.2631 - accuracy: 0.6300 - val_loss: 1.1999 - val_accuracy: 0.6531\n",
      "Epoch 4/10\n",
      "1811/1811 [==============================] - 120s 66ms/step - loss: 1.0962 - accuracy: 0.6830 - val_loss: 0.9321 - val_accuracy: 0.7339\n",
      "Epoch 5/10\n",
      "1811/1811 [==============================] - 136s 75ms/step - loss: 0.9915 - accuracy: 0.7112 - val_loss: 0.9044 - val_accuracy: 0.7445\n",
      "Epoch 6/10\n",
      "1811/1811 [==============================] - 155s 85ms/step - loss: 0.9212 - accuracy: 0.7350 - val_loss: 0.8960 - val_accuracy: 0.7417\n",
      "Epoch 7/10\n",
      "1811/1811 [==============================] - 155s 86ms/step - loss: 0.8621 - accuracy: 0.7524 - val_loss: 0.9002 - val_accuracy: 0.7554\n",
      "Epoch 8/10\n",
      "1811/1811 [==============================] - 161s 89ms/step - loss: 0.8176 - accuracy: 0.7643 - val_loss: 0.8435 - val_accuracy: 0.7648\n",
      "Epoch 9/10\n",
      "1811/1811 [==============================] - 161s 89ms/step - loss: 0.7814 - accuracy: 0.7758 - val_loss: 0.7811 - val_accuracy: 0.7799\n",
      "Epoch 10/10\n",
      "1811/1811 [==============================] - 163s 90ms/step - loss: 0.7469 - accuracy: 0.7852 - val_loss: 0.7652 - val_accuracy: 0.7899\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "906/906 [==============================] - 142s 117ms/step - loss: 2.7640 - accuracy: 0.2318 - val_loss: 1.6141 - val_accuracy: 0.5143\n",
      "Epoch 2/10\n",
      "906/906 [==============================] - 105s 116ms/step - loss: 1.5994 - accuracy: 0.5215 - val_loss: 1.2850 - val_accuracy: 0.6109\n",
      "Epoch 3/10\n",
      "906/906 [==============================] - 108s 119ms/step - loss: 1.2733 - accuracy: 0.6244 - val_loss: 1.1090 - val_accuracy: 0.6681\n",
      "Epoch 4/10\n",
      "906/906 [==============================] - 119s 131ms/step - loss: 1.0973 - accuracy: 0.6764 - val_loss: 0.9656 - val_accuracy: 0.7161\n",
      "Epoch 5/10\n",
      "906/906 [==============================] - 111s 123ms/step - loss: 0.9803 - accuracy: 0.7128 - val_loss: 0.9651 - val_accuracy: 0.7165\n",
      "Epoch 6/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.9258 - accuracy: 0.7300 - val_loss: 0.8353 - val_accuracy: 0.7573\n",
      "Epoch 7/10\n",
      "906/906 [==============================] - 126s 139ms/step - loss: 0.8567 - accuracy: 0.7512 - val_loss: 0.8387 - val_accuracy: 0.7543\n",
      "Epoch 8/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.8073 - accuracy: 0.7666 - val_loss: 0.8074 - val_accuracy: 0.7671\n",
      "Epoch 9/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.7510 - accuracy: 0.7821 - val_loss: 0.7063 - val_accuracy: 0.8010\n",
      "Epoch 10/10\n",
      "906/906 [==============================] - 119s 131ms/step - loss: 0.7229 - accuracy: 0.7914 - val_loss: 0.7477 - val_accuracy: 0.7895\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "1811/1811 [==============================] - 266s 127ms/step - loss: 2.6196 - accuracy: 0.2648 - val_loss: 1.7847 - val_accuracy: 0.4485\n",
      "Epoch 2/20\n",
      "1811/1811 [==============================] - 230s 127ms/step - loss: 1.5512 - accuracy: 0.5392 - val_loss: 1.3518 - val_accuracy: 0.5961\n",
      "Epoch 3/20\n",
      "1811/1811 [==============================] - 238s 131ms/step - loss: 1.2536 - accuracy: 0.6312 - val_loss: 1.0939 - val_accuracy: 0.6896\n",
      "Epoch 4/20\n",
      "1811/1811 [==============================] - 223s 123ms/step - loss: 1.0835 - accuracy: 0.6846 - val_loss: 1.0320 - val_accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "1811/1811 [==============================] - 237s 130ms/step - loss: 0.9828 - accuracy: 0.7159 - val_loss: 0.8607 - val_accuracy: 0.7638\n",
      "Epoch 6/20\n",
      "1811/1811 [==============================] - 237s 131ms/step - loss: 0.9098 - accuracy: 0.7380 - val_loss: 0.9389 - val_accuracy: 0.7408\n",
      "Epoch 7/20\n",
      "1811/1811 [==============================] - 240s 132ms/step - loss: 0.8544 - accuracy: 0.7545 - val_loss: 0.8234 - val_accuracy: 0.7780\n",
      "Epoch 8/20\n",
      "1811/1811 [==============================] - 247s 136ms/step - loss: 0.8074 - accuracy: 0.7666 - val_loss: 0.8261 - val_accuracy: 0.7732\n",
      "Epoch 9/20\n",
      "1811/1811 [==============================] - 253s 139ms/step - loss: 0.7867 - accuracy: 0.7745 - val_loss: 0.7653 - val_accuracy: 0.7942\n",
      "Epoch 10/20\n",
      "1811/1811 [==============================] - 259s 143ms/step - loss: 0.7455 - accuracy: 0.7859 - val_loss: 0.7680 - val_accuracy: 0.7871\n",
      "Epoch 11/20\n",
      "1811/1811 [==============================] - 266s 147ms/step - loss: 0.7142 - accuracy: 0.7979 - val_loss: 0.8229 - val_accuracy: 0.7864\n",
      "Epoch 12/20\n",
      "1811/1811 [==============================] - 268s 148ms/step - loss: 0.6908 - accuracy: 0.8039 - val_loss: 0.8267 - val_accuracy: 0.7771\n",
      "Epoch 13/20\n",
      "1811/1811 [==============================] - 273s 150ms/step - loss: 0.6709 - accuracy: 0.8098 - val_loss: 0.7216 - val_accuracy: 0.8089\n",
      "Epoch 14/20\n",
      "1811/1811 [==============================] - 276s 152ms/step - loss: 0.6488 - accuracy: 0.8150 - val_loss: 0.6778 - val_accuracy: 0.8077\n",
      "Epoch 15/20\n",
      "1811/1811 [==============================] - 307s 169ms/step - loss: 0.6429 - accuracy: 0.8168 - val_loss: 0.7247 - val_accuracy: 0.8080\n",
      "Epoch 16/20\n",
      "1811/1811 [==============================] - 309s 171ms/step - loss: 0.6171 - accuracy: 0.8236 - val_loss: 0.6800 - val_accuracy: 0.8189\n",
      "Epoch 17/20\n",
      "1811/1811 [==============================] - 316s 174ms/step - loss: 0.6033 - accuracy: 0.8276 - val_loss: 0.7309 - val_accuracy: 0.8041\n",
      "Epoch 18/20\n",
      "1811/1811 [==============================] - 314s 173ms/step - loss: 0.5969 - accuracy: 0.8284 - val_loss: 0.6816 - val_accuracy: 0.8179\n",
      "Epoch 19/20\n",
      "1811/1811 [==============================] - 313s 173ms/step - loss: 0.5858 - accuracy: 0.8328 - val_loss: 0.7035 - val_accuracy: 0.8113\n",
      "Epoch 20/20\n",
      "1811/1811 [==============================] - 292s 161ms/step - loss: 0.5642 - accuracy: 0.8384 - val_loss: 0.6486 - val_accuracy: 0.8301\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 20, batch_size: 64\n",
      "Epoch 1/20\n",
      "906/906 [==============================] - 239s 228ms/step - loss: 2.6978 - accuracy: 0.2500 - val_loss: 1.5733 - val_accuracy: 0.5484\n",
      "Epoch 2/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 1.6011 - accuracy: 0.5226 - val_loss: 1.1619 - val_accuracy: 0.6631\n",
      "Epoch 3/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 1.2770 - accuracy: 0.6214 - val_loss: 1.0547 - val_accuracy: 0.6914\n",
      "Epoch 4/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 1.0958 - accuracy: 0.6775 - val_loss: 1.0304 - val_accuracy: 0.6996\n",
      "Epoch 5/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 1.0298 - accuracy: 0.6976 - val_loss: 0.8362 - val_accuracy: 0.7592\n",
      "Epoch 6/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 0.9370 - accuracy: 0.7261 - val_loss: 0.8326 - val_accuracy: 0.7639\n",
      "Epoch 7/20\n",
      "906/906 [==============================] - 213s 235ms/step - loss: 0.8508 - accuracy: 0.7525 - val_loss: 0.7932 - val_accuracy: 0.7773\n",
      "Epoch 8/20\n",
      "906/906 [==============================] - 214s 236ms/step - loss: 0.8232 - accuracy: 0.7599 - val_loss: 0.7637 - val_accuracy: 0.7833\n",
      "Epoch 9/20\n",
      "906/906 [==============================] - 218s 240ms/step - loss: 0.8120 - accuracy: 0.7628 - val_loss: 0.7286 - val_accuracy: 0.7901\n",
      "Epoch 10/20\n",
      "906/906 [==============================] - 220s 242ms/step - loss: 0.7350 - accuracy: 0.7864 - val_loss: 0.7118 - val_accuracy: 0.7971\n",
      "Epoch 11/20\n",
      "906/906 [==============================] - 209s 231ms/step - loss: 0.7040 - accuracy: 0.7967 - val_loss: 0.7114 - val_accuracy: 0.7992\n",
      "Epoch 12/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 0.6772 - accuracy: 0.8043 - val_loss: 0.6814 - val_accuracy: 0.8104\n",
      "Epoch 13/20\n",
      "906/906 [==============================] - 201s 222ms/step - loss: 0.6565 - accuracy: 0.8107 - val_loss: 0.6717 - val_accuracy: 0.8060\n",
      "Epoch 14/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 0.6288 - accuracy: 0.8177 - val_loss: 0.6788 - val_accuracy: 0.8086\n",
      "Epoch 15/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 0.6370 - accuracy: 0.8161 - val_loss: 0.6551 - val_accuracy: 0.8123\n",
      "Epoch 16/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 0.6031 - accuracy: 0.8252 - val_loss: 0.6267 - val_accuracy: 0.8170\n",
      "Epoch 17/20\n",
      "906/906 [==============================] - 206s 227ms/step - loss: 0.5746 - accuracy: 0.8338 - val_loss: 0.6283 - val_accuracy: 0.8210\n",
      "Epoch 18/20\n",
      "906/906 [==============================] - 219s 241ms/step - loss: 0.5615 - accuracy: 0.8374 - val_loss: 0.6015 - val_accuracy: 0.8323\n",
      "Epoch 19/20\n",
      "906/906 [==============================] - 233s 257ms/step - loss: 0.5621 - accuracy: 0.8359 - val_loss: 0.5873 - val_accuracy: 0.8377\n",
      "Epoch 20/20\n",
      "906/906 [==============================] - 234s 258ms/step - loss: 0.5387 - accuracy: 0.8422 - val_loss: 0.6412 - val_accuracy: 0.8180\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "1811/1811 [==============================] - 515s 267ms/step - loss: 3.5523 - accuracy: 0.0521 - val_loss: 3.1456 - val_accuracy: 0.0805\n",
      "Epoch 2/10\n",
      "1811/1811 [==============================] - 461s 254ms/step - loss: 2.7619 - accuracy: 0.1851 - val_loss: 2.7847 - val_accuracy: 0.1031\n",
      "Epoch 3/10\n",
      "1811/1811 [==============================] - 484s 267ms/step - loss: 2.1291 - accuracy: 0.3575 - val_loss: 2.1608 - val_accuracy: 0.2746\n",
      "Epoch 4/10\n",
      "1811/1811 [==============================] - 514s 283ms/step - loss: 1.7338 - accuracy: 0.4813 - val_loss: 2.1876 - val_accuracy: 0.2951\n",
      "Epoch 5/10\n",
      "1811/1811 [==============================] - 527s 291ms/step - loss: 1.5662 - accuracy: 0.5394 - val_loss: 1.8745 - val_accuracy: 0.3725\n",
      "Epoch 6/10\n",
      "1811/1811 [==============================] - 508s 281ms/step - loss: 1.4173 - accuracy: 0.5914 - val_loss: 1.8350 - val_accuracy: 0.4320\n",
      "Epoch 7/10\n",
      "1811/1811 [==============================] - 472s 260ms/step - loss: 1.3224 - accuracy: 0.6251 - val_loss: 2.1019 - val_accuracy: 0.3751\n",
      "Epoch 8/10\n",
      "1811/1811 [==============================] - 480s 265ms/step - loss: 1.2329 - accuracy: 0.6530 - val_loss: 1.8472 - val_accuracy: 0.4138\n",
      "Epoch 9/10\n",
      "1811/1811 [==============================] - 489s 270ms/step - loss: 1.1695 - accuracy: 0.6729 - val_loss: 1.8965 - val_accuracy: 0.4179\n",
      "Epoch 10/10\n",
      "1811/1811 [==============================] - 492s 271ms/step - loss: 1.1219 - accuracy: 0.6879 - val_loss: 2.3462 - val_accuracy: 0.3152\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "906/906 [==============================] - 370s 379ms/step - loss: 3.6519 - accuracy: 0.0481 - val_loss: 3.3814 - val_accuracy: 0.0666\n",
      "Epoch 2/10\n",
      "906/906 [==============================] - 361s 398ms/step - loss: 3.0161 - accuracy: 0.1241 - val_loss: 3.0356 - val_accuracy: 0.0955\n",
      "Epoch 3/10\n",
      "906/906 [==============================] - 369s 407ms/step - loss: 2.6202 - accuracy: 0.2091 - val_loss: 2.6361 - val_accuracy: 0.2109\n",
      "Epoch 4/10\n",
      "906/906 [==============================] - 344s 379ms/step - loss: 2.2420 - accuracy: 0.3127 - val_loss: 2.5372 - val_accuracy: 0.2005\n",
      "Epoch 5/10\n",
      "906/906 [==============================] - 331s 365ms/step - loss: 1.8991 - accuracy: 0.4225 - val_loss: 2.4049 - val_accuracy: 0.2492\n",
      "Epoch 6/10\n",
      "906/906 [==============================] - 333s 368ms/step - loss: 1.6368 - accuracy: 0.5098 - val_loss: 2.1561 - val_accuracy: 0.3233\n",
      "Epoch 7/10\n",
      "906/906 [==============================] - 332s 366ms/step - loss: 1.4677 - accuracy: 0.5700 - val_loss: 2.1004 - val_accuracy: 0.3413\n",
      "Epoch 8/10\n",
      "906/906 [==============================] - 334s 369ms/step - loss: 1.3544 - accuracy: 0.6111 - val_loss: 2.0290 - val_accuracy: 0.3632\n",
      "Epoch 9/10\n",
      "906/906 [==============================] - 338s 373ms/step - loss: 1.2534 - accuracy: 0.6447 - val_loss: 2.3293 - val_accuracy: 0.2729\n",
      "Epoch 10/10\n",
      "906/906 [==============================] - 340s 375ms/step - loss: 1.1851 - accuracy: 0.6684 - val_loss: 2.2277 - val_accuracy: 0.3076\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "1811/1811 [==============================] - 535s 284ms/step - loss: 3.4949 - accuracy: 0.0639 - val_loss: 3.3306 - val_accuracy: 0.0558\n",
      "Epoch 2/20\n",
      "1811/1811 [==============================] - 528s 291ms/step - loss: 2.5940 - accuracy: 0.2265 - val_loss: 2.9047 - val_accuracy: 0.1165\n",
      "Epoch 3/20\n",
      "1811/1811 [==============================] - 538s 297ms/step - loss: 2.0765 - accuracy: 0.3648 - val_loss: 2.9113 - val_accuracy: 0.1443\n",
      "Epoch 4/20\n",
      "1811/1811 [==============================] - 548s 302ms/step - loss: 1.7885 - accuracy: 0.4575 - val_loss: 2.4928 - val_accuracy: 0.1842\n",
      "Epoch 5/20\n",
      "1811/1811 [==============================] - 591s 326ms/step - loss: 1.5852 - accuracy: 0.5346 - val_loss: 2.4043 - val_accuracy: 0.2342\n",
      "Epoch 6/20\n",
      "1811/1811 [==============================] - 616s 340ms/step - loss: 1.4329 - accuracy: 0.5890 - val_loss: 2.1779 - val_accuracy: 0.2623\n",
      "Epoch 7/20\n",
      "1811/1811 [==============================] - 624s 344ms/step - loss: 1.3117 - accuracy: 0.6281 - val_loss: 2.0153 - val_accuracy: 0.3517\n",
      "Epoch 8/20\n",
      "1811/1811 [==============================] - 675s 373ms/step - loss: 1.2338 - accuracy: 0.6570 - val_loss: 1.9247 - val_accuracy: 0.3519\n",
      "Epoch 9/20\n",
      "1811/1811 [==============================] - 676s 373ms/step - loss: 1.1695 - accuracy: 0.6764 - val_loss: 1.9387 - val_accuracy: 0.3592\n",
      "Epoch 10/20\n",
      "1811/1811 [==============================] - 681s 376ms/step - loss: 1.1301 - accuracy: 0.6889 - val_loss: 2.2268 - val_accuracy: 0.2564\n",
      "Epoch 11/20\n",
      "1811/1811 [==============================] - 664s 366ms/step - loss: 1.0817 - accuracy: 0.7024 - val_loss: 1.9388 - val_accuracy: 0.3379\n",
      "Epoch 12/20\n",
      "1811/1811 [==============================] - 665s 367ms/step - loss: 1.0434 - accuracy: 0.7155 - val_loss: 2.1829 - val_accuracy: 0.2554\n",
      "Epoch 13/20\n",
      "1811/1811 [==============================] - 669s 369ms/step - loss: 1.0158 - accuracy: 0.7243 - val_loss: 2.0610 - val_accuracy: 0.3099\n",
      "Epoch 14/20\n",
      "1811/1811 [==============================] - 686s 378ms/step - loss: 0.9954 - accuracy: 0.7283 - val_loss: 2.0930 - val_accuracy: 0.2842\n",
      "Epoch 15/20\n",
      "1811/1811 [==============================] - 689s 380ms/step - loss: 0.9680 - accuracy: 0.7364 - val_loss: 2.1951 - val_accuracy: 0.2835\n",
      "Epoch 16/20\n",
      "1179/1811 [==================>...........] - ETA: 3:33 - loss: 0.9402 - accuracy: 0.7461"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\models_gru.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Train the model with your training set and validate it with your validation set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mbatch(batch_size), epochs\u001b[39m=\u001b[39;49mepoch, validation_data\u001b[39m=\u001b[39;49mval\u001b[39m.\u001b[39;49mbatch(batch_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((results, pd\u001b[39m.\u001b[39mDataFrame([[gru_units, dropout_rate, epoch, batch_size, history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m]]], \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                                 columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgru_units\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set your model's hyperparameters\n",
    "results = pd.DataFrame(columns=['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "for gru_units in [64, 128]:\n",
    "    for dropout_rate in [0.2, 0.4]:\n",
    "            for epoch in [ 10, 20]:\n",
    "                  for batch_size in [32, 64]:\n",
    "                        print(f'gru_units: {gru_units}, dropout_rate: {dropout_rate}, epoch: {epoch}, batch_size: {batch_size}')\n",
    "                        num_classes = len(np.unique(labels))  # Number of unique classes in your dataset\n",
    "\n",
    "                        input_shape = (39, 44)\n",
    "\n",
    "\n",
    "                        model = Sequential([\n",
    "                            Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "                            BatchNormalization(),\n",
    "                            MaxPooling1D(pool_size=2),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GRU(gru_units, return_sequences=True),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GRU(gru_units, return_sequences=True),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(256, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(128, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(64, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GlobalMaxPooling1D(),\n",
    "                            Dense(512, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(256, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(128, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(num_classes, activation='softmax')\n",
    "                        ])\n",
    "\n",
    "\n",
    "                        # Compile the model\n",
    "                        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                        # Train the model with your training set and validate it with your validation set\n",
    "                        history = model.fit(train.batch(batch_size), epochs=epoch, validation_data=val.batch(batch_size))\n",
    "\n",
    "                        results = np.concatenate((results, pd.DataFrame([[gru_units, dropout_rate, epoch, batch_size, history.history['loss'][-1], history.history['loss'], history.history['accuracy'][-1], history.history['accuracy'], history.history['val_loss'][-1], history.history['val_loss'], history.history['val_accuracy'][-1], history.history['val_accuracy']]], \n",
    "                                                                        columns=['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f.to_pickle('results\\\\model_gru_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.746932</td>\n",
       "      <td>[2.665213108062744, 1.6146924495697021, 1.2631...</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>[0.2528011202812195, 0.5191202163696289, 0.629...</td>\n",
       "      <td>0.765207</td>\n",
       "      <td>[1.9333562850952148, 1.2438899278640747, 1.199...</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>[0.447925865650177, 0.6325389742851257, 0.6531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>[2.7639663219451904, 1.5994480848312378, 1.273...</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>[0.23180773854255676, 0.521537184715271, 0.624...</td>\n",
       "      <td>0.747728</td>\n",
       "      <td>[1.6140880584716797, 1.2849770784378052, 1.108...</td>\n",
       "      <td>0.789497</td>\n",
       "      <td>[0.5142688751220703, 0.610914945602417, 0.6681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.564152</td>\n",
       "      <td>[2.6195595264434814, 1.5511832237243652, 1.253...</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>[0.26476529240608215, 0.5392158627510071, 0.63...</td>\n",
       "      <td>0.648608</td>\n",
       "      <td>[1.784652590751648, 1.3517918586730957, 1.0938...</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>[0.44851428270339966, 0.596057653427124, 0.689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.538738</td>\n",
       "      <td>[2.697845935821533, 1.6010715961456299, 1.2770...</td>\n",
       "      <td>0.842222</td>\n",
       "      <td>[0.2499525249004364, 0.5225730538368225, 0.621...</td>\n",
       "      <td>0.641245</td>\n",
       "      <td>[1.5733102560043335, 1.1619383096694946, 1.054...</td>\n",
       "      <td>0.818035</td>\n",
       "      <td>[0.5483965873718262, 0.6631362438201904, 0.691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1.121856</td>\n",
       "      <td>[3.5523223876953125, 2.7618539333343506, 2.129...</td>\n",
       "      <td>0.687948</td>\n",
       "      <td>[0.052138183265924454, 0.1850905567407608, 0.3...</td>\n",
       "      <td>2.346172</td>\n",
       "      <td>[3.1456222534179688, 2.78471302986145, 2.16078...</td>\n",
       "      <td>0.31524</td>\n",
       "      <td>[0.08046483993530273, 0.10311856120824814, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1.185146</td>\n",
       "      <td>[3.6519148349761963, 3.016111135482788, 2.6201...</td>\n",
       "      <td>0.668422</td>\n",
       "      <td>[0.04808107390999794, 0.12409578263759613, 0.2...</td>\n",
       "      <td>2.227695</td>\n",
       "      <td>[3.3813705444335938, 3.035633087158203, 2.6361...</td>\n",
       "      <td>0.30759</td>\n",
       "      <td>[0.06663724780082703, 0.09546925872564316, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3         4   \\\n",
       "0  64  0.2  10  32  0.746932   \n",
       "1  64  0.2  10  64  0.722944   \n",
       "2  64  0.2  20  32  0.564152   \n",
       "3  64  0.2  20  64  0.538738   \n",
       "4  64  0.4  10  32  1.121856   \n",
       "5  64  0.4  10  64  1.185146   \n",
       "\n",
       "                                                  5         6   \\\n",
       "0  [2.665213108062744, 1.6146924495697021, 1.2631...  0.785215   \n",
       "1  [2.7639663219451904, 1.5994480848312378, 1.273...  0.791395   \n",
       "2  [2.6195595264434814, 1.5511832237243652, 1.253...  0.838389   \n",
       "3  [2.697845935821533, 1.6010715961456299, 1.2770...  0.842222   \n",
       "4  [3.5523223876953125, 2.7618539333343506, 2.129...  0.687948   \n",
       "5  [3.6519148349761963, 3.016111135482788, 2.6201...  0.668422   \n",
       "\n",
       "                                                  7         8   \\\n",
       "0  [0.2528011202812195, 0.5191202163696289, 0.629...  0.765207   \n",
       "1  [0.23180773854255676, 0.521537184715271, 0.624...  0.747728   \n",
       "2  [0.26476529240608215, 0.5392158627510071, 0.63...  0.648608   \n",
       "3  [0.2499525249004364, 0.5225730538368225, 0.621...  0.641245   \n",
       "4  [0.052138183265924454, 0.1850905567407608, 0.3...  2.346172   \n",
       "5  [0.04808107390999794, 0.12409578263759613, 0.2...  2.227695   \n",
       "\n",
       "                                                  9         10  \\\n",
       "0  [1.9333562850952148, 1.2438899278640747, 1.199...  0.789938   \n",
       "1  [1.6140880584716797, 1.2849770784378052, 1.108...  0.789497   \n",
       "2  [1.784652590751648, 1.3517918586730957, 1.0938...  0.830097   \n",
       "3  [1.5733102560043335, 1.1619383096694946, 1.054...  0.818035   \n",
       "4  [3.1456222534179688, 2.78471302986145, 2.16078...   0.31524   \n",
       "5  [3.3813705444335938, 3.035633087158203, 2.6361...   0.30759   \n",
       "\n",
       "                                                  11  \n",
       "0  [0.447925865650177, 0.6325389742851257, 0.6531...  \n",
       "1  [0.5142688751220703, 0.610914945602417, 0.6681...  \n",
       "2  [0.44851428270339966, 0.596057653427124, 0.689...  \n",
       "3  [0.5483965873718262, 0.6631362438201904, 0.691...  \n",
       "4  [0.08046483993530273, 0.10311856120824814, 0.2...  \n",
       "5  [0.06663724780082703, 0.09546925872564316, 0.2...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/214 [==============================] - 29s 69ms/step - loss: 6.9579 - accuracy: 0.0613 - val_loss: 3.6162 - val_accuracy: 0.0596\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 2.8994 - accuracy: 0.1918 - val_loss: 2.5313 - val_accuracy: 0.2439\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 2.4038 - accuracy: 0.2900 - val_loss: 2.0998 - val_accuracy: 0.3616\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 2.0500 - accuracy: 0.3930 - val_loss: 1.8843 - val_accuracy: 0.4451\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 1.8284 - accuracy: 0.4449 - val_loss: 1.6862 - val_accuracy: 0.4878\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.7079 - accuracy: 0.4755 - val_loss: 1.6471 - val_accuracy: 0.5157\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 1.6029 - accuracy: 0.5110 - val_loss: 1.7111 - val_accuracy: 0.4846\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.4772 - accuracy: 0.5429 - val_loss: 1.3925 - val_accuracy: 0.5940\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.4434 - accuracy: 0.5593 - val_loss: 1.3605 - val_accuracy: 0.5877\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.3525 - accuracy: 0.5852 - val_loss: 1.5537 - val_accuracy: 0.5327\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 32s 74ms/step - loss: 7.0433 - accuracy: 0.0601 - val_loss: 3.7041 - val_accuracy: 0.0825\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 13s 59ms/step - loss: 2.9475 - accuracy: 0.1612 - val_loss: 2.6197 - val_accuracy: 0.2352\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 2.4830 - accuracy: 0.2714 - val_loss: 2.4180 - val_accuracy: 0.2440\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 14s 63ms/step - loss: 2.2000 - accuracy: 0.3486 - val_loss: 2.0087 - val_accuracy: 0.3954\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 2.0040 - accuracy: 0.3947 - val_loss: 1.8612 - val_accuracy: 0.4391\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 14s 65ms/step - loss: 1.7949 - accuracy: 0.4582 - val_loss: 1.6778 - val_accuracy: 0.5038\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.6568 - accuracy: 0.4979 - val_loss: 1.6263 - val_accuracy: 0.5074\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 1.5562 - accuracy: 0.5257 - val_loss: 1.5525 - val_accuracy: 0.5432\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.4460 - accuracy: 0.5596 - val_loss: 1.4088 - val_accuracy: 0.5755\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.3738 - accuracy: 0.5808 - val_loss: 1.3234 - val_accuracy: 0.6034\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.3215 - accuracy: 0.5972 - val_loss: 1.3095 - val_accuracy: 0.6165\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 13s 59ms/step - loss: 1.2586 - accuracy: 0.6228 - val_loss: 1.3344 - val_accuracy: 0.6078\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.2080 - accuracy: 0.6317 - val_loss: 1.2495 - val_accuracy: 0.6239\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 1.1354 - accuracy: 0.6506 - val_loss: 1.2837 - val_accuracy: 0.6208\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.0749 - accuracy: 0.6679 - val_loss: 1.2364 - val_accuracy: 0.6302\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.0549 - accuracy: 0.6783 - val_loss: 1.2053 - val_accuracy: 0.6565\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.0081 - accuracy: 0.6963 - val_loss: 1.2228 - val_accuracy: 0.6484\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 10s 46ms/step - loss: 0.9734 - accuracy: 0.7046 - val_loss: 1.1869 - val_accuracy: 0.6556\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 0.9531 - accuracy: 0.7132 - val_loss: 1.1757 - val_accuracy: 0.6556\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 12s 58ms/step - loss: 0.8807 - accuracy: 0.7359 - val_loss: 1.1120 - val_accuracy: 0.6752\n",
      "Epoch 1/30\n",
      "214/214 [==============================] - 31s 72ms/step - loss: 6.9051 - accuracy: 0.0533 - val_loss: 3.5724 - val_accuracy: 0.0884\n",
      "Epoch 2/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 3.0638 - accuracy: 0.1432 - val_loss: 2.7562 - val_accuracy: 0.1831\n",
      "Epoch 3/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 2.6700 - accuracy: 0.2117 - val_loss: 2.4194 - val_accuracy: 0.2816\n",
      "Epoch 4/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 2.3523 - accuracy: 0.3017 - val_loss: 2.1408 - val_accuracy: 0.3460\n",
      "Epoch 5/30\n",
      "214/214 [==============================] - 16s 75ms/step - loss: 2.0523 - accuracy: 0.3814 - val_loss: 1.9107 - val_accuracy: 0.4219\n",
      "Epoch 6/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.8623 - accuracy: 0.4328 - val_loss: 1.6799 - val_accuracy: 0.4915\n",
      "Epoch 7/30\n",
      "214/214 [==============================] - 15s 69ms/step - loss: 1.7306 - accuracy: 0.4767 - val_loss: 1.8279 - val_accuracy: 0.4532\n",
      "Epoch 8/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.5679 - accuracy: 0.5194 - val_loss: 1.5297 - val_accuracy: 0.5487\n",
      "Epoch 9/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 1.4944 - accuracy: 0.5489 - val_loss: 1.8609 - val_accuracy: 0.4631\n",
      "Epoch 10/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 1.3754 - accuracy: 0.5811 - val_loss: 1.4511 - val_accuracy: 0.5612\n",
      "Epoch 11/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.3142 - accuracy: 0.5962 - val_loss: 1.6520 - val_accuracy: 0.5063\n",
      "Epoch 12/30\n",
      "214/214 [==============================] - 17s 81ms/step - loss: 1.2475 - accuracy: 0.6212 - val_loss: 1.4921 - val_accuracy: 0.5530\n",
      "Epoch 13/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.1977 - accuracy: 0.6356 - val_loss: 1.3856 - val_accuracy: 0.5883\n",
      "Epoch 14/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.1326 - accuracy: 0.6516 - val_loss: 1.4644 - val_accuracy: 0.5816\n",
      "Epoch 15/30\n",
      "214/214 [==============================] - 16s 75ms/step - loss: 1.1059 - accuracy: 0.6676 - val_loss: 1.4233 - val_accuracy: 0.5866\n",
      "Epoch 16/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.0603 - accuracy: 0.6844 - val_loss: 1.3383 - val_accuracy: 0.6167\n",
      "Epoch 17/30\n",
      "214/214 [==============================] - 16s 73ms/step - loss: 1.0167 - accuracy: 0.6873 - val_loss: 1.3003 - val_accuracy: 0.6299\n",
      "Epoch 18/30\n",
      "214/214 [==============================] - 14s 64ms/step - loss: 0.9844 - accuracy: 0.6989 - val_loss: 1.2893 - val_accuracy: 0.6399\n",
      "Epoch 19/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.9126 - accuracy: 0.7257 - val_loss: 1.3397 - val_accuracy: 0.6200\n",
      "Epoch 20/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.9105 - accuracy: 0.7293 - val_loss: 1.3537 - val_accuracy: 0.6242\n",
      "Epoch 21/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.9157 - accuracy: 0.7217 - val_loss: 1.1880 - val_accuracy: 0.6593\n",
      "Epoch 22/30\n",
      "214/214 [==============================] - 17s 77ms/step - loss: 0.8420 - accuracy: 0.7469 - val_loss: 1.2463 - val_accuracy: 0.6512\n",
      "Epoch 23/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.8305 - accuracy: 0.7448 - val_loss: 1.2248 - val_accuracy: 0.6527\n",
      "Epoch 24/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.8228 - accuracy: 0.7523 - val_loss: 1.2024 - val_accuracy: 0.6561\n",
      "Epoch 25/30\n",
      "214/214 [==============================] - 16s 76ms/step - loss: 0.7958 - accuracy: 0.7546 - val_loss: 1.2288 - val_accuracy: 0.6649\n",
      "Epoch 26/30\n",
      "214/214 [==============================] - 16s 76ms/step - loss: 0.7639 - accuracy: 0.7661 - val_loss: 1.1902 - val_accuracy: 0.6684\n",
      "Epoch 27/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.7293 - accuracy: 0.7802 - val_loss: 1.4078 - val_accuracy: 0.6174\n",
      "Epoch 28/30\n",
      "214/214 [==============================] - 14s 68ms/step - loss: 0.7016 - accuracy: 0.7819 - val_loss: 1.4334 - val_accuracy: 0.6258\n",
      "Epoch 29/30\n",
      "214/214 [==============================] - 14s 65ms/step - loss: 0.6995 - accuracy: 0.7908 - val_loss: 1.2373 - val_accuracy: 0.6698\n",
      "Epoch 30/30\n",
      "214/214 [==============================] - 14s 64ms/step - loss: 0.6693 - accuracy: 0.7928 - val_loss: 1.3256 - val_accuracy: 0.6462\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 59s 192ms/step - loss: 8.3429 - accuracy: 0.0461 - val_loss: 7.4951 - val_accuracy: 0.0743\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 4.4138 - accuracy: 0.1140 - val_loss: 3.2584 - val_accuracy: 0.0743\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 2.9274 - accuracy: 0.1764 - val_loss: 2.7914 - val_accuracy: 0.1723\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 2.5817 - accuracy: 0.2417 - val_loss: 2.3677 - val_accuracy: 0.2938\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 14s 131ms/step - loss: 2.2904 - accuracy: 0.3198 - val_loss: 2.1464 - val_accuracy: 0.3395\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 14s 127ms/step - loss: 2.0537 - accuracy: 0.3924 - val_loss: 1.9141 - val_accuracy: 0.4188\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 1.8429 - accuracy: 0.4500 - val_loss: 1.7669 - val_accuracy: 0.4576\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 14s 129ms/step - loss: 1.6742 - accuracy: 0.4931 - val_loss: 1.5686 - val_accuracy: 0.5199\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 14s 127ms/step - loss: 1.5556 - accuracy: 0.5312 - val_loss: 1.4759 - val_accuracy: 0.5547\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 1.4483 - accuracy: 0.5650 - val_loss: 1.5240 - val_accuracy: 0.5382\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 72s 201ms/step - loss: 8.3628 - accuracy: 0.0492 - val_loss: 7.5054 - val_accuracy: 0.0562\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 4.3262 - accuracy: 0.1366 - val_loss: 3.3106 - val_accuracy: 0.0893\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 15s 142ms/step - loss: 2.6514 - accuracy: 0.2497 - val_loss: 2.5653 - val_accuracy: 0.2482\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 14s 128ms/step - loss: 2.2688 - accuracy: 0.3255 - val_loss: 2.1358 - val_accuracy: 0.3650\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 2.0394 - accuracy: 0.3814 - val_loss: 2.0232 - val_accuracy: 0.3882\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 15s 136ms/step - loss: 1.8691 - accuracy: 0.4279 - val_loss: 1.7886 - val_accuracy: 0.4590\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 1.7005 - accuracy: 0.4775 - val_loss: 1.5619 - val_accuracy: 0.5265\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 15s 141ms/step - loss: 1.5951 - accuracy: 0.5112 - val_loss: 1.6718 - val_accuracy: 0.4918\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 15s 137ms/step - loss: 1.4862 - accuracy: 0.5311 - val_loss: 1.5485 - val_accuracy: 0.5334\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 1.4114 - accuracy: 0.5652 - val_loss: 1.4867 - val_accuracy: 0.5550\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 1.3064 - accuracy: 0.5927 - val_loss: 1.4195 - val_accuracy: 0.5793\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.2338 - accuracy: 0.6157 - val_loss: 1.4493 - val_accuracy: 0.5716\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 1.1957 - accuracy: 0.6263 - val_loss: 1.2520 - val_accuracy: 0.6296\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.1372 - accuracy: 0.6477 - val_loss: 1.3404 - val_accuracy: 0.6047\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 15s 137ms/step - loss: 1.0871 - accuracy: 0.6595 - val_loss: 1.2558 - val_accuracy: 0.6283\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 13s 121ms/step - loss: 1.0566 - accuracy: 0.6753 - val_loss: 1.3120 - val_accuracy: 0.6087\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 1.0077 - accuracy: 0.6869 - val_loss: 1.3003 - val_accuracy: 0.6168\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - 12s 117ms/step - loss: 0.9694 - accuracy: 0.7033 - val_loss: 1.2277 - val_accuracy: 0.6393\n",
      "Epoch 19/20\n",
      "107/107 [==============================] - 13s 122ms/step - loss: 0.9582 - accuracy: 0.7086 - val_loss: 1.1671 - val_accuracy: 0.6623\n",
      "Epoch 20/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 0.8884 - accuracy: 0.7321 - val_loss: 1.2016 - val_accuracy: 0.6525\n",
      "Epoch 1/30\n",
      "107/107 [==============================] - 54s 200ms/step - loss: 8.3468 - accuracy: 0.0525 - val_loss: 8.6673 - val_accuracy: 0.0407\n",
      "Epoch 2/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 4.3760 - accuracy: 0.1245 - val_loss: 3.4172 - val_accuracy: 0.1083\n",
      "Epoch 3/30\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 2.7519 - accuracy: 0.2224 - val_loss: 2.7985 - val_accuracy: 0.1893\n",
      "Epoch 4/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 2.3802 - accuracy: 0.3029 - val_loss: 2.1827 - val_accuracy: 0.3420\n",
      "Epoch 5/30\n",
      "107/107 [==============================] - 13s 119ms/step - loss: 2.1121 - accuracy: 0.3772 - val_loss: 2.0110 - val_accuracy: 0.3913\n",
      "Epoch 6/30\n",
      "107/107 [==============================] - 16s 147ms/step - loss: 1.9245 - accuracy: 0.4296 - val_loss: 1.9511 - val_accuracy: 0.3992\n",
      "Epoch 7/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.7435 - accuracy: 0.4682 - val_loss: 1.6947 - val_accuracy: 0.4819\n",
      "Epoch 8/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.6156 - accuracy: 0.5086 - val_loss: 1.5664 - val_accuracy: 0.5237\n",
      "Epoch 9/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.5066 - accuracy: 0.5435 - val_loss: 1.4485 - val_accuracy: 0.5703\n",
      "Epoch 10/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.3992 - accuracy: 0.5666 - val_loss: 1.3999 - val_accuracy: 0.5758\n",
      "Epoch 11/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 1.3325 - accuracy: 0.5971 - val_loss: 1.3934 - val_accuracy: 0.5818\n",
      "Epoch 12/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.2557 - accuracy: 0.6164 - val_loss: 1.3154 - val_accuracy: 0.6069\n",
      "Epoch 13/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.2010 - accuracy: 0.6294 - val_loss: 1.3169 - val_accuracy: 0.6099\n",
      "Epoch 14/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 1.1401 - accuracy: 0.6524 - val_loss: 1.4306 - val_accuracy: 0.5741\n",
      "Epoch 15/30\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 1.0986 - accuracy: 0.6609 - val_loss: 1.2652 - val_accuracy: 0.6330\n",
      "Epoch 16/30\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.0815 - accuracy: 0.6634 - val_loss: 1.1975 - val_accuracy: 0.6486\n",
      "Epoch 17/30\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 0.9899 - accuracy: 0.6945 - val_loss: 1.2283 - val_accuracy: 0.6386\n",
      "Epoch 18/30\n",
      "107/107 [==============================] - 16s 153ms/step - loss: 0.9463 - accuracy: 0.7087 - val_loss: 1.1994 - val_accuracy: 0.6534\n",
      "Epoch 19/30\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 0.9244 - accuracy: 0.7140 - val_loss: 1.1858 - val_accuracy: 0.6555\n",
      "Epoch 20/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.8808 - accuracy: 0.7298 - val_loss: 1.3737 - val_accuracy: 0.6021\n",
      "Epoch 21/30\n",
      "107/107 [==============================] - 17s 156ms/step - loss: 0.8691 - accuracy: 0.7298 - val_loss: 1.1265 - val_accuracy: 0.6784\n",
      "Epoch 22/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.8320 - accuracy: 0.7374 - val_loss: 1.0893 - val_accuracy: 0.6905\n",
      "Epoch 23/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 0.7950 - accuracy: 0.7564 - val_loss: 1.1373 - val_accuracy: 0.6742\n",
      "Epoch 24/30\n",
      "107/107 [==============================] - 17s 159ms/step - loss: 0.7885 - accuracy: 0.7605 - val_loss: 1.1152 - val_accuracy: 0.6736\n",
      "Epoch 25/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.7610 - accuracy: 0.7650 - val_loss: 1.0783 - val_accuracy: 0.6896\n",
      "Epoch 26/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.7199 - accuracy: 0.7776 - val_loss: 1.0572 - val_accuracy: 0.6964\n",
      "Epoch 27/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 0.7120 - accuracy: 0.7819 - val_loss: 1.1540 - val_accuracy: 0.6696\n",
      "Epoch 28/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.6881 - accuracy: 0.7899 - val_loss: 1.1242 - val_accuracy: 0.6789\n",
      "Epoch 29/30\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.6817 - accuracy: 0.7899 - val_loss: 1.0723 - val_accuracy: 0.6930\n",
      "Epoch 30/30\n",
      "107/107 [==============================] - 13s 120ms/step - loss: 0.6398 - accuracy: 0.8031 - val_loss: 1.1086 - val_accuracy: 0.6815\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 56s 441ms/step - loss: 8.6960 - accuracy: 0.0341 - val_loss: 8.6882 - val_accuracy: 0.0259\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 19s 351ms/step - loss: 7.2206 - accuracy: 0.0764 - val_loss: 7.6456 - val_accuracy: 0.0346\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 18s 337ms/step - loss: 4.2527 - accuracy: 0.1276 - val_loss: 5.6083 - val_accuracy: 0.0388\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 16s 305ms/step - loss: 3.1073 - accuracy: 0.1716 - val_loss: 3.9642 - val_accuracy: 0.0902\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 18s 343ms/step - loss: 2.6839 - accuracy: 0.2394 - val_loss: 2.9942 - val_accuracy: 0.1828\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 19s 355ms/step - loss: 2.3857 - accuracy: 0.2977 - val_loss: 2.3869 - val_accuracy: 0.3008\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 19s 347ms/step - loss: 2.1894 - accuracy: 0.3544 - val_loss: 2.3284 - val_accuracy: 0.3154\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 19s 348ms/step - loss: 2.0403 - accuracy: 0.3857 - val_loss: 2.0037 - val_accuracy: 0.4095\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 19s 350ms/step - loss: 1.9107 - accuracy: 0.4271 - val_loss: 1.8836 - val_accuracy: 0.4387\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 19s 359ms/step - loss: 1.7874 - accuracy: 0.4613 - val_loss: 1.8424 - val_accuracy: 0.4619\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 41s 412ms/step - loss: 8.6832 - accuracy: 0.0410 - val_loss: 8.4885 - val_accuracy: 0.0363\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 19s 361ms/step - loss: 6.9175 - accuracy: 0.0945 - val_loss: 5.1615 - val_accuracy: 0.0844\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 19s 346ms/step - loss: 3.8346 - accuracy: 0.1505 - val_loss: 3.1494 - val_accuracy: 0.0891\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 18s 327ms/step - loss: 2.8827 - accuracy: 0.2108 - val_loss: 2.8984 - val_accuracy: 0.1342\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 2.5728 - accuracy: 0.2628 - val_loss: 2.4879 - val_accuracy: 0.2607\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 19s 361ms/step - loss: 2.3419 - accuracy: 0.3115 - val_loss: 2.2945 - val_accuracy: 0.3214\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 2.1429 - accuracy: 0.3674 - val_loss: 1.9825 - val_accuracy: 0.4107\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.9820 - accuracy: 0.4116 - val_loss: 1.9123 - val_accuracy: 0.4116\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 19s 359ms/step - loss: 1.7590 - accuracy: 0.4676 - val_loss: 1.6838 - val_accuracy: 0.4959\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 19s 354ms/step - loss: 1.6221 - accuracy: 0.5146 - val_loss: 1.8302 - val_accuracy: 0.4737\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 19s 354ms/step - loss: 1.5208 - accuracy: 0.5266 - val_loss: 1.6006 - val_accuracy: 0.5265\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 19s 357ms/step - loss: 1.4533 - accuracy: 0.5484 - val_loss: 1.4694 - val_accuracy: 0.5625\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 19s 357ms/step - loss: 1.3628 - accuracy: 0.5854 - val_loss: 1.4248 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.2974 - accuracy: 0.5985 - val_loss: 1.3169 - val_accuracy: 0.6137\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 1.2285 - accuracy: 0.6250 - val_loss: 1.2788 - val_accuracy: 0.6144\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 1.1675 - accuracy: 0.6442 - val_loss: 1.2509 - val_accuracy: 0.6290\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.1061 - accuracy: 0.6613 - val_loss: 1.2098 - val_accuracy: 0.6406\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 19s 355ms/step - loss: 1.0713 - accuracy: 0.6685 - val_loss: 1.2237 - val_accuracy: 0.6322\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.0397 - accuracy: 0.6774 - val_loss: 1.2762 - val_accuracy: 0.6334\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 17s 311ms/step - loss: 1.0036 - accuracy: 0.6916 - val_loss: 1.2740 - val_accuracy: 0.6264\n",
      "Epoch 1/30\n",
      "54/54 [==============================] - 60s 468ms/step - loss: 8.6953 - accuracy: 0.0375 - val_loss: 8.4761 - val_accuracy: 0.0387\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 21s 385ms/step - loss: 7.1219 - accuracy: 0.0967 - val_loss: 7.2628 - val_accuracy: 0.0438\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 4.0549 - accuracy: 0.1494 - val_loss: 5.3226 - val_accuracy: 0.0480\n",
      "Epoch 4/30\n",
      "54/54 [==============================] - 20s 375ms/step - loss: 2.9604 - accuracy: 0.2164 - val_loss: 3.2636 - val_accuracy: 0.1571\n",
      "Epoch 5/30\n",
      "54/54 [==============================] - 20s 374ms/step - loss: 2.5425 - accuracy: 0.2761 - val_loss: 2.4530 - val_accuracy: 0.2891\n",
      "Epoch 6/30\n",
      "54/54 [==============================] - 21s 395ms/step - loss: 2.3195 - accuracy: 0.3219 - val_loss: 2.2652 - val_accuracy: 0.3283\n",
      "Epoch 7/30\n",
      "54/54 [==============================] - 22s 401ms/step - loss: 2.0795 - accuracy: 0.3820 - val_loss: 2.0398 - val_accuracy: 0.3869\n",
      "Epoch 8/30\n",
      "54/54 [==============================] - 22s 404ms/step - loss: 1.9134 - accuracy: 0.4291 - val_loss: 1.9169 - val_accuracy: 0.4363\n",
      "Epoch 9/30\n",
      "54/54 [==============================] - 19s 348ms/step - loss: 1.7872 - accuracy: 0.4677 - val_loss: 1.7210 - val_accuracy: 0.4940\n",
      "Epoch 10/30\n",
      "54/54 [==============================] - 17s 319ms/step - loss: 1.6563 - accuracy: 0.4974 - val_loss: 1.6902 - val_accuracy: 0.4976\n",
      "Epoch 11/30\n",
      "54/54 [==============================] - 20s 366ms/step - loss: 1.5374 - accuracy: 0.5266 - val_loss: 1.5907 - val_accuracy: 0.5138\n",
      "Epoch 12/30\n",
      "54/54 [==============================] - 20s 365ms/step - loss: 1.4538 - accuracy: 0.5546 - val_loss: 1.5060 - val_accuracy: 0.5447\n",
      "Epoch 13/30\n",
      "54/54 [==============================] - 21s 383ms/step - loss: 1.3753 - accuracy: 0.5767 - val_loss: 1.3702 - val_accuracy: 0.5972\n",
      "Epoch 14/30\n",
      "54/54 [==============================] - 22s 401ms/step - loss: 1.2922 - accuracy: 0.6061 - val_loss: 1.3609 - val_accuracy: 0.6049\n",
      "Epoch 15/30\n",
      "54/54 [==============================] - 21s 381ms/step - loss: 1.2325 - accuracy: 0.6195 - val_loss: 1.3045 - val_accuracy: 0.6115\n",
      "Epoch 16/30\n",
      "54/54 [==============================] - 19s 346ms/step - loss: 1.1996 - accuracy: 0.6329 - val_loss: 1.3964 - val_accuracy: 0.5968\n",
      "Epoch 17/30\n",
      "54/54 [==============================] - 21s 391ms/step - loss: 1.1199 - accuracy: 0.6541 - val_loss: 1.2429 - val_accuracy: 0.6411\n",
      "Epoch 18/30\n",
      "54/54 [==============================] - 22s 403ms/step - loss: 1.1052 - accuracy: 0.6604 - val_loss: 1.2267 - val_accuracy: 0.6421\n",
      "Epoch 19/30\n",
      "54/54 [==============================] - 21s 392ms/step - loss: 1.0736 - accuracy: 0.6683 - val_loss: 1.2646 - val_accuracy: 0.6289\n",
      "Epoch 20/30\n",
      "54/54 [==============================] - 21s 389ms/step - loss: 1.0045 - accuracy: 0.6884 - val_loss: 1.2691 - val_accuracy: 0.6259\n",
      "Epoch 21/30\n",
      "54/54 [==============================] - 21s 391ms/step - loss: 0.9610 - accuracy: 0.7039 - val_loss: 1.1567 - val_accuracy: 0.6624\n",
      "Epoch 22/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.9318 - accuracy: 0.7178 - val_loss: 1.1774 - val_accuracy: 0.6606\n",
      "Epoch 23/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 0.9039 - accuracy: 0.7229 - val_loss: 1.1701 - val_accuracy: 0.6565\n",
      "Epoch 24/30\n",
      "54/54 [==============================] - 21s 387ms/step - loss: 0.8910 - accuracy: 0.7233 - val_loss: 1.1604 - val_accuracy: 0.6724\n",
      "Epoch 25/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.8436 - accuracy: 0.7397 - val_loss: 1.1659 - val_accuracy: 0.6712\n",
      "Epoch 26/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 0.8314 - accuracy: 0.7384 - val_loss: 1.1607 - val_accuracy: 0.6723\n",
      "Epoch 27/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.7693 - accuracy: 0.7582 - val_loss: 1.2005 - val_accuracy: 0.6664\n",
      "Epoch 28/30\n",
      "54/54 [==============================] - 21s 381ms/step - loss: 0.7611 - accuracy: 0.7663 - val_loss: 1.1441 - val_accuracy: 0.6858\n",
      "Epoch 29/30\n",
      "54/54 [==============================] - 21s 390ms/step - loss: 0.7576 - accuracy: 0.7650 - val_loss: 1.1158 - val_accuracy: 0.6884\n",
      "Epoch 30/30\n",
      "54/54 [==============================] - 22s 416ms/step - loss: 0.7219 - accuracy: 0.7756 - val_loss: 1.1015 - val_accuracy: 0.6939\n",
      "Epoch 1/10\n",
      "214/214 [==============================] - 59s 138ms/step - loss: 7.6407 - accuracy: 0.0272 - val_loss: 6.3628 - val_accuracy: 0.0371\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 25s 115ms/step - loss: 3.6563 - accuracy: 0.0416 - val_loss: 3.5027 - val_accuracy: 0.0385\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 23s 108ms/step - loss: 3.3790 - accuracy: 0.0584 - val_loss: 3.3863 - val_accuracy: 0.0519\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 23s 107ms/step - loss: 3.2447 - accuracy: 0.0840 - val_loss: 3.3434 - val_accuracy: 0.0581\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 3.1106 - accuracy: 0.1090 - val_loss: 3.2034 - val_accuracy: 0.0768\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.9721 - accuracy: 0.1407 - val_loss: 3.2022 - val_accuracy: 0.0590\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.7840 - accuracy: 0.1851 - val_loss: 3.1261 - val_accuracy: 0.0962\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 2.6585 - accuracy: 0.2132 - val_loss: 3.2379 - val_accuracy: 0.0691\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.5643 - accuracy: 0.2345 - val_loss: 3.1529 - val_accuracy: 0.0709\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.5075 - accuracy: 0.2571 - val_loss: 3.0531 - val_accuracy: 0.0928\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 59s 156ms/step - loss: 7.6124 - accuracy: 0.0290 - val_loss: 5.0708 - val_accuracy: 0.0397\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 3.6629 - accuracy: 0.0410 - val_loss: 3.5386 - val_accuracy: 0.0515\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 28s 132ms/step - loss: 3.4113 - accuracy: 0.0522 - val_loss: 3.4701 - val_accuracy: 0.0513\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 3.2679 - accuracy: 0.0718 - val_loss: 3.5072 - val_accuracy: 0.0468\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 3.1596 - accuracy: 0.0853 - val_loss: 3.2331 - val_accuracy: 0.0650\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 29s 138ms/step - loss: 3.0465 - accuracy: 0.1089 - val_loss: 3.2744 - val_accuracy: 0.0799\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 2.8993 - accuracy: 0.1415 - val_loss: 2.9531 - val_accuracy: 0.1486\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.7607 - accuracy: 0.1827 - val_loss: 2.8858 - val_accuracy: 0.1306\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.6890 - accuracy: 0.1909 - val_loss: 2.8315 - val_accuracy: 0.1427\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.5833 - accuracy: 0.2206 - val_loss: 2.8545 - val_accuracy: 0.1296\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.5153 - accuracy: 0.2402 - val_loss: 2.7437 - val_accuracy: 0.1652\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.4434 - accuracy: 0.2486 - val_loss: 2.7354 - val_accuracy: 0.1745\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.3876 - accuracy: 0.2673 - val_loss: 2.7026 - val_accuracy: 0.1723\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.2967 - accuracy: 0.2913 - val_loss: 2.6316 - val_accuracy: 0.1609\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.2610 - accuracy: 0.3045 - val_loss: 2.7259 - val_accuracy: 0.1489\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.1848 - accuracy: 0.3289 - val_loss: 2.6762 - val_accuracy: 0.1523\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 26s 124ms/step - loss: 2.1461 - accuracy: 0.3457 - val_loss: 2.6048 - val_accuracy: 0.1608\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.0719 - accuracy: 0.3611 - val_loss: 2.4964 - val_accuracy: 0.2048\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.0546 - accuracy: 0.3747 - val_loss: 3.0179 - val_accuracy: 0.1102\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 1.9842 - accuracy: 0.3922 - val_loss: 2.7053 - val_accuracy: 0.1486\n",
      "Epoch 1/30\n",
      "214/214 [==============================] - 55s 150ms/step - loss: 7.6077 - accuracy: 0.0255 - val_loss: 6.3048 - val_accuracy: 0.0355\n",
      "Epoch 2/30\n",
      "214/214 [==============================] - 27s 128ms/step - loss: 3.6763 - accuracy: 0.0407 - val_loss: 3.6138 - val_accuracy: 0.0387\n",
      "Epoch 3/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 3.4164 - accuracy: 0.0502 - val_loss: 3.5350 - val_accuracy: 0.0374\n",
      "Epoch 4/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 3.3103 - accuracy: 0.0654 - val_loss: 3.4398 - val_accuracy: 0.0560\n",
      "Epoch 5/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 3.1998 - accuracy: 0.0783 - val_loss: 3.8442 - val_accuracy: 0.0487\n",
      "Epoch 6/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 3.0635 - accuracy: 0.1129 - val_loss: 3.6193 - val_accuracy: 0.0472\n",
      "Epoch 7/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.9482 - accuracy: 0.1403 - val_loss: 3.0812 - val_accuracy: 0.1028\n",
      "Epoch 8/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 2.8086 - accuracy: 0.1684 - val_loss: 3.1497 - val_accuracy: 0.0777\n",
      "Epoch 9/30\n",
      "214/214 [==============================] - 29s 135ms/step - loss: 2.6743 - accuracy: 0.1968 - val_loss: 2.9683 - val_accuracy: 0.1062\n",
      "Epoch 10/30\n",
      "214/214 [==============================] - 29s 135ms/step - loss: 2.5646 - accuracy: 0.2237 - val_loss: 2.8905 - val_accuracy: 0.1272\n",
      "Epoch 11/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 2.4562 - accuracy: 0.2480 - val_loss: 3.0217 - val_accuracy: 0.0968\n",
      "Epoch 12/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 2.3439 - accuracy: 0.2854 - val_loss: 2.8542 - val_accuracy: 0.1471\n",
      "Epoch 13/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.2958 - accuracy: 0.2964 - val_loss: 2.6205 - val_accuracy: 0.1862\n",
      "Epoch 14/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.2045 - accuracy: 0.3233 - val_loss: 3.0113 - val_accuracy: 0.1503\n",
      "Epoch 15/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 2.1456 - accuracy: 0.3485 - val_loss: 3.0305 - val_accuracy: 0.1061\n",
      "Epoch 16/30\n",
      "214/214 [==============================] - 30s 138ms/step - loss: 2.0589 - accuracy: 0.3683 - val_loss: 2.6883 - val_accuracy: 0.1655\n",
      "Epoch 17/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 2.0002 - accuracy: 0.3842 - val_loss: 2.7958 - val_accuracy: 0.1575\n",
      "Epoch 18/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.9336 - accuracy: 0.4063 - val_loss: 2.6311 - val_accuracy: 0.1924\n",
      "Epoch 19/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 1.9015 - accuracy: 0.4159 - val_loss: 2.4952 - val_accuracy: 0.2327\n",
      "Epoch 20/30\n",
      "214/214 [==============================] - 25s 117ms/step - loss: 1.8155 - accuracy: 0.4396 - val_loss: 2.5379 - val_accuracy: 0.2208\n",
      "Epoch 21/30\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 1.7616 - accuracy: 0.4581 - val_loss: 2.9012 - val_accuracy: 0.1289\n",
      "Epoch 22/30\n",
      "214/214 [==============================] - 30s 142ms/step - loss: 1.7308 - accuracy: 0.4701 - val_loss: 3.0173 - val_accuracy: 0.1495\n",
      "Epoch 23/30\n",
      "214/214 [==============================] - 30s 141ms/step - loss: 1.6903 - accuracy: 0.4882 - val_loss: 3.1102 - val_accuracy: 0.1530\n",
      "Epoch 24/30\n",
      "214/214 [==============================] - 31s 143ms/step - loss: 1.6401 - accuracy: 0.4948 - val_loss: 2.9288 - val_accuracy: 0.1246\n",
      "Epoch 25/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.6004 - accuracy: 0.5093 - val_loss: 2.6909 - val_accuracy: 0.2234\n",
      "Epoch 26/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.5720 - accuracy: 0.5230 - val_loss: 3.0861 - val_accuracy: 0.0849\n",
      "Epoch 27/30\n",
      "214/214 [==============================] - 31s 143ms/step - loss: 1.5460 - accuracy: 0.5342 - val_loss: 3.0862 - val_accuracy: 0.0940\n",
      "Epoch 28/30\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 1.5005 - accuracy: 0.5434 - val_loss: 3.1784 - val_accuracy: 0.0877\n",
      "Epoch 29/30\n",
      "214/214 [==============================] - 32s 148ms/step - loss: 1.4367 - accuracy: 0.5573 - val_loss: 3.2768 - val_accuracy: 0.1218\n",
      "Epoch 30/30\n",
      "214/214 [==============================] - 32s 149ms/step - loss: 1.4314 - accuracy: 0.5709 - val_loss: 3.4443 - val_accuracy: 0.0794\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 60s 260ms/step - loss: 8.5679 - accuracy: 0.0238 - val_loss: 7.9406 - val_accuracy: 0.0338\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 5.7353 - accuracy: 0.0360 - val_loss: 3.5704 - val_accuracy: 0.0277\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.6253 - accuracy: 0.0417 - val_loss: 3.4716 - val_accuracy: 0.0382\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.4319 - accuracy: 0.0481 - val_loss: 3.3987 - val_accuracy: 0.0488\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.3538 - accuracy: 0.0622 - val_loss: 3.3120 - val_accuracy: 0.0660\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.2870 - accuracy: 0.0746 - val_loss: 3.2615 - val_accuracy: 0.0743\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.2132 - accuracy: 0.0920 - val_loss: 3.1983 - val_accuracy: 0.0872\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.1077 - accuracy: 0.1124 - val_loss: 3.1568 - val_accuracy: 0.0933\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 24s 222ms/step - loss: 3.0130 - accuracy: 0.1320 - val_loss: 3.0376 - val_accuracy: 0.1149\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 24s 220ms/step - loss: 2.8656 - accuracy: 0.1642 - val_loss: 3.0000 - val_accuracy: 0.1111\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 58s 275ms/step - loss: 8.5766 - accuracy: 0.0290 - val_loss: 7.6757 - val_accuracy: 0.0363\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 25s 231ms/step - loss: 5.7774 - accuracy: 0.0367 - val_loss: 3.5707 - val_accuracy: 0.0256\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 23s 217ms/step - loss: 3.6383 - accuracy: 0.0401 - val_loss: 3.4301 - val_accuracy: 0.0381\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 3.4291 - accuracy: 0.0557 - val_loss: 3.4169 - val_accuracy: 0.0474\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 3.3058 - accuracy: 0.0753 - val_loss: 3.3118 - val_accuracy: 0.0759\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 3.1518 - accuracy: 0.1061 - val_loss: 3.2522 - val_accuracy: 0.0837\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 24s 221ms/step - loss: 3.0188 - accuracy: 0.1339 - val_loss: 3.0768 - val_accuracy: 0.1096\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 2.9296 - accuracy: 0.1551 - val_loss: 3.0910 - val_accuracy: 0.0937\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 2.8200 - accuracy: 0.1723 - val_loss: 2.9121 - val_accuracy: 0.1447\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.6948 - accuracy: 0.2099 - val_loss: 2.8344 - val_accuracy: 0.1753\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 19s 178ms/step - loss: 2.5828 - accuracy: 0.2300 - val_loss: 2.9087 - val_accuracy: 0.1128\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 22s 205ms/step - loss: 2.4771 - accuracy: 0.2503 - val_loss: 2.8410 - val_accuracy: 0.1336\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 2.3989 - accuracy: 0.2711 - val_loss: 2.7500 - val_accuracy: 0.1662\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.3299 - accuracy: 0.2914 - val_loss: 2.7609 - val_accuracy: 0.1527\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 23s 216ms/step - loss: 2.2765 - accuracy: 0.3074 - val_loss: 2.7898 - val_accuracy: 0.1308\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 23s 214ms/step - loss: 2.1938 - accuracy: 0.3293 - val_loss: 2.6956 - val_accuracy: 0.1389\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.1271 - accuracy: 0.3539 - val_loss: 2.7525 - val_accuracy: 0.1464\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.0833 - accuracy: 0.3593 - val_loss: 2.6285 - val_accuracy: 0.1904\n",
      "Epoch 19/20\n",
      "107/107 [==============================] - 23s 213ms/step - loss: 1.9867 - accuracy: 0.3845 - val_loss: 2.8375 - val_accuracy: 0.1555\n",
      "Epoch 20/20\n",
      "107/107 [==============================] - 23s 215ms/step - loss: 1.9679 - accuracy: 0.3887 - val_loss: 2.7543 - val_accuracy: 0.1755\n",
      "Epoch 1/30\n",
      "107/107 [==============================] - 54s 254ms/step - loss: 8.5978 - accuracy: 0.0241 - val_loss: 7.8583 - val_accuracy: 0.0257\n",
      "Epoch 2/30\n",
      "107/107 [==============================] - 24s 221ms/step - loss: 5.8947 - accuracy: 0.0411 - val_loss: 3.5241 - val_accuracy: 0.0330\n",
      "Epoch 3/30\n",
      "107/107 [==============================] - 22s 209ms/step - loss: 3.6506 - accuracy: 0.0446 - val_loss: 3.4169 - val_accuracy: 0.0540\n",
      "Epoch 4/30\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 3.4279 - accuracy: 0.0575 - val_loss: 3.3771 - val_accuracy: 0.0630\n",
      "Epoch 5/30\n",
      "107/107 [==============================] - 22s 203ms/step - loss: 3.3346 - accuracy: 0.0729 - val_loss: 3.3670 - val_accuracy: 0.0631\n",
      "Epoch 6/30\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 3.2260 - accuracy: 0.0928 - val_loss: 3.2759 - val_accuracy: 0.0688\n",
      "Epoch 7/30\n",
      "107/107 [==============================] - 22s 204ms/step - loss: 3.1317 - accuracy: 0.1043 - val_loss: 3.2186 - val_accuracy: 0.0761\n",
      "Epoch 8/30\n",
      "107/107 [==============================] - 22s 204ms/step - loss: 3.0668 - accuracy: 0.1151 - val_loss: 3.1435 - val_accuracy: 0.0828\n",
      "Epoch 9/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.9936 - accuracy: 0.1270 - val_loss: 3.1323 - val_accuracy: 0.0931\n",
      "Epoch 10/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.9417 - accuracy: 0.1364 - val_loss: 3.0135 - val_accuracy: 0.1121\n",
      "Epoch 11/30\n",
      "107/107 [==============================] - 22s 203ms/step - loss: 2.8699 - accuracy: 0.1548 - val_loss: 2.9865 - val_accuracy: 0.1306\n",
      "Epoch 12/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.7785 - accuracy: 0.1658 - val_loss: 2.9534 - val_accuracy: 0.1228\n",
      "Epoch 13/30\n",
      "107/107 [==============================] - 20s 186ms/step - loss: 2.6975 - accuracy: 0.1892 - val_loss: 2.8921 - val_accuracy: 0.1233\n",
      "Epoch 14/30\n",
      "107/107 [==============================] - 21s 192ms/step - loss: 2.6072 - accuracy: 0.2146 - val_loss: 2.8603 - val_accuracy: 0.1397\n",
      "Epoch 15/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.5243 - accuracy: 0.2449 - val_loss: 2.8537 - val_accuracy: 0.1358\n",
      "Epoch 16/30\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 2.4132 - accuracy: 0.2672 - val_loss: 2.8277 - val_accuracy: 0.1311\n",
      "Epoch 17/30\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 2.3374 - accuracy: 0.2898 - val_loss: 2.7675 - val_accuracy: 0.1380\n",
      "Epoch 18/30\n",
      "107/107 [==============================] - 21s 195ms/step - loss: 2.2559 - accuracy: 0.3159 - val_loss: 2.6296 - val_accuracy: 0.1733\n",
      "Epoch 19/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.1869 - accuracy: 0.3349 - val_loss: 2.5757 - val_accuracy: 0.1737\n",
      "Epoch 20/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.1039 - accuracy: 0.3577 - val_loss: 2.5970 - val_accuracy: 0.1661\n",
      "Epoch 21/30\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 2.0389 - accuracy: 0.3671 - val_loss: 2.5201 - val_accuracy: 0.2029\n",
      "Epoch 22/30\n",
      "107/107 [==============================] - 21s 195ms/step - loss: 1.9986 - accuracy: 0.3789 - val_loss: 2.4863 - val_accuracy: 0.2152\n",
      "Epoch 23/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 1.9380 - accuracy: 0.3966 - val_loss: 2.4691 - val_accuracy: 0.2036\n",
      "Epoch 24/30\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 1.8949 - accuracy: 0.4139 - val_loss: 2.3428 - val_accuracy: 0.2460\n",
      "Epoch 25/30\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 1.8391 - accuracy: 0.4272 - val_loss: 2.0932 - val_accuracy: 0.3358\n",
      "Epoch 26/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 1.8000 - accuracy: 0.4404 - val_loss: 2.1410 - val_accuracy: 0.3249\n",
      "Epoch 27/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.7532 - accuracy: 0.4556 - val_loss: 2.3527 - val_accuracy: 0.2479\n",
      "Epoch 28/30\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 1.6929 - accuracy: 0.4664 - val_loss: 2.0769 - val_accuracy: 0.3605\n",
      "Epoch 29/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.6613 - accuracy: 0.4800 - val_loss: 2.2904 - val_accuracy: 0.2745\n",
      "Epoch 30/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.6418 - accuracy: 0.4931 - val_loss: 2.0305 - val_accuracy: 0.3628\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 56s 679ms/step - loss: 8.7600 - accuracy: 0.0196 - val_loss: 8.7080 - val_accuracy: 0.0387\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 34s 624ms/step - loss: 8.2317 - accuracy: 0.0337 - val_loss: 7.9319 - val_accuracy: 0.0356\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 33s 620ms/step - loss: 6.2632 - accuracy: 0.0364 - val_loss: 5.1785 - val_accuracy: 0.0387\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 32s 604ms/step - loss: 4.1740 - accuracy: 0.0410 - val_loss: 3.6850 - val_accuracy: 0.0409\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 34s 629ms/step - loss: 3.6179 - accuracy: 0.0394 - val_loss: 3.4795 - val_accuracy: 0.0506\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 34s 638ms/step - loss: 3.4891 - accuracy: 0.0483 - val_loss: 3.3871 - val_accuracy: 0.0494\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 3.4088 - accuracy: 0.0576 - val_loss: 3.3228 - val_accuracy: 0.0655\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 32s 589ms/step - loss: 3.3307 - accuracy: 0.0705 - val_loss: 3.2166 - val_accuracy: 0.0994\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 34s 636ms/step - loss: 3.2260 - accuracy: 0.0890 - val_loss: 3.1571 - val_accuracy: 0.1006\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 3.1493 - accuracy: 0.1072 - val_loss: 3.1063 - val_accuracy: 0.1036\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 60s 675ms/step - loss: 8.7690 - accuracy: 0.0184 - val_loss: 8.5618 - val_accuracy: 0.0352\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 33s 617ms/step - loss: 8.2444 - accuracy: 0.0296 - val_loss: 7.3850 - val_accuracy: 0.0387\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 33s 619ms/step - loss: 6.3000 - accuracy: 0.0370 - val_loss: 5.2844 - val_accuracy: 0.0384\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 33s 621ms/step - loss: 4.1813 - accuracy: 0.0348 - val_loss: 3.6658 - val_accuracy: 0.0353\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 33s 615ms/step - loss: 3.6137 - accuracy: 0.0433 - val_loss: 3.4962 - val_accuracy: 0.0356\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 28s 526ms/step - loss: 3.4708 - accuracy: 0.0522 - val_loss: 3.5655 - val_accuracy: 0.0327\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 34s 630ms/step - loss: 3.3718 - accuracy: 0.0663 - val_loss: 3.5656 - val_accuracy: 0.0418\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 33s 617ms/step - loss: 3.2711 - accuracy: 0.0895 - val_loss: 3.6005 - val_accuracy: 0.0418\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 34s 642ms/step - loss: 3.1712 - accuracy: 0.0999 - val_loss: 3.8518 - val_accuracy: 0.0287\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 33s 613ms/step - loss: 3.0689 - accuracy: 0.1113 - val_loss: 3.6705 - val_accuracy: 0.0472\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 35s 643ms/step - loss: 2.9936 - accuracy: 0.1244 - val_loss: 3.6851 - val_accuracy: 0.0459\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 33s 611ms/step - loss: 2.9058 - accuracy: 0.1331 - val_loss: 3.4318 - val_accuracy: 0.0638\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 31s 569ms/step - loss: 2.8390 - accuracy: 0.1541 - val_loss: 3.3976 - val_accuracy: 0.0616\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 33s 607ms/step - loss: 2.7687 - accuracy: 0.1697 - val_loss: 3.1365 - val_accuracy: 0.0908\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 33s 622ms/step - loss: 2.7066 - accuracy: 0.1860 - val_loss: 3.2357 - val_accuracy: 0.0756\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 34s 623ms/step - loss: 2.6630 - accuracy: 0.1912 - val_loss: 3.1489 - val_accuracy: 0.0840\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 33s 618ms/step - loss: 2.5956 - accuracy: 0.2132 - val_loss: 3.0764 - val_accuracy: 0.0977\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 33s 611ms/step - loss: 2.5549 - accuracy: 0.2183 - val_loss: 3.1855 - val_accuracy: 0.0880\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 34s 640ms/step - loss: 2.4995 - accuracy: 0.2323 - val_loss: 3.0971 - val_accuracy: 0.0914\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 2.4646 - accuracy: 0.2473 - val_loss: 3.1707 - val_accuracy: 0.0969\n",
      "Epoch 1/30\n",
      "54/54 [==============================] - 60s 667ms/step - loss: 8.7653 - accuracy: 0.0183 - val_loss: 8.6092 - val_accuracy: 0.0278\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 8.2575 - accuracy: 0.0348 - val_loss: 7.3404 - val_accuracy: 0.0338\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 35s 650ms/step - loss: 6.3265 - accuracy: 0.0391 - val_loss: 5.5368 - val_accuracy: 0.0338\n",
      "Epoch 4/30\n",
      "54/54 [==============================] - 35s 647ms/step - loss: 4.2024 - accuracy: 0.0439 - val_loss: 3.5305 - val_accuracy: 0.0338\n",
      "Epoch 5/30\n",
      "54/54 [==============================] - 35s 649ms/step - loss: 3.6229 - accuracy: 0.0435 - val_loss: 3.4494 - val_accuracy: 0.0402\n",
      "Epoch 6/30\n",
      "54/54 [==============================] - 37s 679ms/step - loss: 3.4686 - accuracy: 0.0495 - val_loss: 3.3984 - val_accuracy: 0.0516\n",
      "Epoch 7/30\n",
      "54/54 [==============================] - 34s 635ms/step - loss: 3.3959 - accuracy: 0.0598 - val_loss: 3.3474 - val_accuracy: 0.0563\n",
      "Epoch 8/30\n",
      "54/54 [==============================] - 32s 587ms/step - loss: 3.3273 - accuracy: 0.0701 - val_loss: 3.3237 - val_accuracy: 0.0566\n",
      "Epoch 9/30\n",
      "54/54 [==============================] - 35s 644ms/step - loss: 3.2655 - accuracy: 0.0840 - val_loss: 3.2341 - val_accuracy: 0.0744\n",
      "Epoch 10/30\n",
      "54/54 [==============================] - 34s 640ms/step - loss: 3.2030 - accuracy: 0.0989 - val_loss: 3.1895 - val_accuracy: 0.0796\n",
      "Epoch 11/30\n",
      "54/54 [==============================] - 35s 643ms/step - loss: 3.1375 - accuracy: 0.1075 - val_loss: 3.0922 - val_accuracy: 0.0805\n",
      "Epoch 12/30\n",
      "54/54 [==============================] - 34s 633ms/step - loss: 3.0388 - accuracy: 0.1257 - val_loss: 3.1110 - val_accuracy: 0.0759\n",
      "Epoch 13/30\n",
      "54/54 [==============================] - 34s 631ms/step - loss: 2.9448 - accuracy: 0.1378 - val_loss: 3.0002 - val_accuracy: 0.1106\n",
      "Epoch 14/30\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 2.8690 - accuracy: 0.1586 - val_loss: 2.9983 - val_accuracy: 0.1037\n",
      "Epoch 15/30\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 2.8047 - accuracy: 0.1697 - val_loss: 2.9431 - val_accuracy: 0.1256\n",
      "Epoch 16/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 2.7499 - accuracy: 0.1848 - val_loss: 2.8707 - val_accuracy: 0.1450\n",
      "Epoch 17/30\n",
      "54/54 [==============================] - 34s 637ms/step - loss: 2.6594 - accuracy: 0.1996 - val_loss: 2.8426 - val_accuracy: 0.1531\n",
      "Epoch 18/30\n",
      "54/54 [==============================] - 28s 524ms/step - loss: 2.6205 - accuracy: 0.2155 - val_loss: 2.7507 - val_accuracy: 0.1615\n",
      "Epoch 19/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 2.5543 - accuracy: 0.2257 - val_loss: 2.8442 - val_accuracy: 0.1328\n",
      "Epoch 20/30\n",
      "54/54 [==============================] - 35s 648ms/step - loss: 2.4846 - accuracy: 0.2385 - val_loss: 2.8307 - val_accuracy: 0.1331\n",
      "Epoch 21/30\n",
      "54/54 [==============================] - 35s 653ms/step - loss: 2.4241 - accuracy: 0.2691 - val_loss: 2.8229 - val_accuracy: 0.1133\n",
      "Epoch 22/30\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 2.3801 - accuracy: 0.2755 - val_loss: 2.5698 - val_accuracy: 0.1793\n",
      "Epoch 23/30\n",
      "54/54 [==============================] - 33s 605ms/step - loss: 2.3037 - accuracy: 0.2955 - val_loss: 2.6911 - val_accuracy: 0.1671\n",
      "Epoch 24/30\n",
      "54/54 [==============================] - 35s 648ms/step - loss: 2.2371 - accuracy: 0.3122 - val_loss: 2.6304 - val_accuracy: 0.1808\n",
      "Epoch 25/30\n",
      "54/54 [==============================] - 35s 656ms/step - loss: 2.1728 - accuracy: 0.3282 - val_loss: 2.6417 - val_accuracy: 0.1883\n",
      "Epoch 26/30\n",
      "54/54 [==============================] - 35s 646ms/step - loss: 2.1090 - accuracy: 0.3538 - val_loss: 2.4893 - val_accuracy: 0.2286\n",
      "Epoch 27/30\n",
      "54/54 [==============================] - 35s 651ms/step - loss: 2.0695 - accuracy: 0.3715 - val_loss: 2.5734 - val_accuracy: 0.1898\n",
      "Epoch 28/30\n",
      "54/54 [==============================] - 33s 621ms/step - loss: 2.0286 - accuracy: 0.3823 - val_loss: 2.4504 - val_accuracy: 0.2439\n",
      "Epoch 29/30\n",
      "54/54 [==============================] - 36s 671ms/step - loss: 1.9488 - accuracy: 0.3937 - val_loss: 2.4381 - val_accuracy: 0.1945\n",
      "Epoch 30/30\n",
      "54/54 [==============================] - 36s 672ms/step - loss: 1.9157 - accuracy: 0.4180 - val_loss: 2.4646 - val_accuracy: 0.2257\n",
      "Epoch 1/10\n",
      "214/214 [==============================] - 76s 195ms/step - loss: 7.9708 - accuracy: 0.0263 - val_loss: 4.4891 - val_accuracy: 0.0452\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 42s 199ms/step - loss: 4.0390 - accuracy: 0.0350 - val_loss: 3.4266 - val_accuracy: 0.0350\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 43s 201ms/step - loss: 3.5220 - accuracy: 0.0394 - val_loss: 3.4100 - val_accuracy: 0.0366\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 40s 186ms/step - loss: 3.4797 - accuracy: 0.0358 - val_loss: 3.4283 - val_accuracy: 0.0332\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4468 - accuracy: 0.0364 - val_loss: 3.4066 - val_accuracy: 0.0374\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.4451 - accuracy: 0.0307 - val_loss: 3.4160 - val_accuracy: 0.0382\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 3.4344 - accuracy: 0.0360 - val_loss: 3.4314 - val_accuracy: 0.0382\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 43s 202ms/step - loss: 3.4365 - accuracy: 0.0364 - val_loss: 3.4169 - val_accuracy: 0.0371\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 3.4355 - accuracy: 0.0356 - val_loss: 3.4178 - val_accuracy: 0.0396\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4233 - accuracy: 0.0361 - val_loss: 3.4170 - val_accuracy: 0.0359\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 76s 228ms/step - loss: 7.9263 - accuracy: 0.0271 - val_loss: 5.1164 - val_accuracy: 0.0382\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 44s 207ms/step - loss: 4.0093 - accuracy: 0.0372 - val_loss: 3.4348 - val_accuracy: 0.0363\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 3.5248 - accuracy: 0.0361 - val_loss: 3.4568 - val_accuracy: 0.0343\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 39s 182ms/step - loss: 3.4642 - accuracy: 0.0357 - val_loss: 3.4502 - val_accuracy: 0.0418\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 39s 183ms/step - loss: 3.4487 - accuracy: 0.0414 - val_loss: 3.4344 - val_accuracy: 0.0382\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 40s 188ms/step - loss: 3.4396 - accuracy: 0.0420 - val_loss: 3.4585 - val_accuracy: 0.0349\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 3.4359 - accuracy: 0.0392 - val_loss: 3.4653 - val_accuracy: 0.0382\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 3.4370 - accuracy: 0.0332 - val_loss: 3.4354 - val_accuracy: 0.0382\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 3.4342 - accuracy: 0.0395 - val_loss: 3.4204 - val_accuracy: 0.0478\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 38s 178ms/step - loss: 3.4341 - accuracy: 0.0342 - val_loss: 3.4171 - val_accuracy: 0.0381\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 3.4279 - accuracy: 0.0385 - val_loss: 3.4241 - val_accuracy: 0.0382\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 40s 188ms/step - loss: 3.4282 - accuracy: 0.0364 - val_loss: 3.4352 - val_accuracy: 0.0338\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 39s 184ms/step - loss: 3.4234 - accuracy: 0.0350 - val_loss: 3.4437 - val_accuracy: 0.0338\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 39s 181ms/step - loss: 3.4169 - accuracy: 0.0392 - val_loss: 3.4327 - val_accuracy: 0.0382\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 39s 182ms/step - loss: 3.4260 - accuracy: 0.0375 - val_loss: 3.4605 - val_accuracy: 0.0382\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 40s 185ms/step - loss: 3.4185 - accuracy: 0.0399 - val_loss: 3.4297 - val_accuracy: 0.0359\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4229 - accuracy: 0.0358 - val_loss: 3.4102 - val_accuracy: 0.0382\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 40s 185ms/step - loss: 3.4082 - accuracy: 0.0380 - val_loss: 3.4431 - val_accuracy: 0.0382\n",
      "Epoch 19/20\n",
      "213/214 [============================>.] - ETA: 0s - loss: 3.4122 - accuracy: 0.0359"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "results = pd.DataFrame(columns=['lstm_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "for gru_units in [64, 128, 256]:\n",
    "    for dropout_rate in [0.2, 0.4, 0.6]:\n",
    "        for batch_size in [32, 64, 128]:\n",
    "            for epochs in [10, 20, 30]:\n",
    "                num_classes = len(y_train_encoded)  # Number of unique classes in your dataset\n",
    "                input_shape = (39, 44)\n",
    "\n",
    "                model = Sequential([\n",
    "                    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling1D(pool_size=2),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GRU(gru_units, return_sequences=True),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GRU(gru_units, return_sequences=True),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(256, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(128, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(64, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(num_classes, activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                # Train the model with your training set and validate it with your validation set\n",
    "                epochs = epochs\n",
    "                batch_size = batch_size\n",
    "                history = model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val_encoded))\n",
    "\n",
    "                results = np.concatenate((results, pd.DataFrame([[gru_units, dropout_rate, epochs, batch_size, history.history['loss'][-1], history.history['loss'], history.history['accuracy'][-1], history.history['accuracy'], history.history['val_loss'][-1], history.history['val_loss'], history.history['val_accuracy'][-1], history.history['val_accuracy']]], \n",
    "                                                                    columns=['lstm_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 37, 128)           17024     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 37, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 18, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 18, 128)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 18, 256)          198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 18, 256)          296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 18, 128)           148224    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 18, 128)           99072     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 18, 256)          33024     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 128)          32896     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 64)           8256      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 18, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 18, 64)            0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               33280     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,040,842\n",
      "Trainable params: 1,036,362\n",
      "Non-trainable params: 4,480\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results\\\\model_gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
