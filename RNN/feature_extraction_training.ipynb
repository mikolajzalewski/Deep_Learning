{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import perform_vad, resample_wav, padding, cut_wav_into_clips, extract_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"train\\\\audio\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full training list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\validation_list.txt'\n",
    "output_labels_path = 'train\\\\training_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_labels_path, \"w\") as f1:\n",
    "    for folder in os.listdir(wav_dir):\n",
    "        if folder == '_background_noise_' or folder == 'silence':\n",
    "            continue\n",
    "        else:\n",
    "            for file in os.listdir(os.path.join(wav_dir, folder)):\n",
    "                path = f\"{folder}/{file}\"\n",
    "                if path not in file_paths:\n",
    "                    f1.write(path + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(output_labels_path', \"w\") as f1:\n",
    "    for file in part1:\n",
    "        f1.write(\"silence/\" + file + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `unknown` detection / `label` classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\training_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths_train = [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57923/57923 [1:12:09<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "features_train = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_train), total=len(file_paths_train), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  \n",
    "    perform_vad(wav_file, wav_file2)\n",
    "    padding(wav_file2, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\features_training.pkl', 'wb') as f:\n",
    "    pickle.dump(features_train, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\validation_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths_val = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6798/6798 [08:02<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "features_val = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_val), total=len(file_paths_val), leave=True):\n",
    "\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  \n",
    "    perform_vad(wav_file, wav_file2)\n",
    "    padding(wav_file2, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_val.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\features_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(features_val, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `silence` detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silence clips list creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"train\\\\audio\\\\_background_noise_\"\n",
    "output_folder = \"train\\\\audio\\\\silence\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        cut_wav_into_clips(f\"{input_folder}\\\\{filename}\", output_folder, filename, clip_duration_ms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"train\\\\audio\\\\silence\"\n",
    "\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# Split the list into two parts\n",
    "split_index = int(len(file_list) * 0.8)\n",
    "part1 = file_list[:split_index]\n",
    "part2 = file_list[split_index:]\n",
    "\n",
    "# Define the output file paths\n",
    "file1_path = \"train\\\\silence_testing_list.txt\"\n",
    "file2_path = \"train\\\\silence_validation_list.txt\"\n",
    "\n",
    "# Write the paths to the files to the output text files\n",
    "with open(file1_path, \"w\") as f1:\n",
    "    for file in part1:\n",
    "        f1.write(\"silence/\" + file + \"\\n\")\n",
    "\n",
    "with open(file2_path, \"w\") as f2:\n",
    "    for file in part2:\n",
    "        f2.write(\"silence/\" + file + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training part of non-silence (but without VAD detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\training_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths_train = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57923/57923 [52:34<00:00, 18.36it/s]  \n"
     ]
    }
   ],
   "source": [
    "features_train = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_train), total=len(file_paths_train), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  \n",
    "    padding(wav_file, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\non_silence_training.pkl', 'wb') as f:\n",
    "    pickle.dump(features_train, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training part of silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train\\\\silence_training_list.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "file_paths_train_silence = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 321/321 [00:17<00:00, 18.44it/s]\n"
     ]
    }
   ],
   "source": [
    "features_train_silence = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_train_silence), total=len(file_paths_train_silence), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  (without VAD)\n",
    "    padding(wav_file, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train_silence.append([features, label])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augment silence to 5000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_num_clips = 5000\n",
    "k = 0\n",
    "n = len(features_train_silence)\n",
    "\n",
    "while k + n < target_num_clips:\n",
    "    \n",
    "    for i, file in enumerate(file_paths_train_silence):\n",
    "        \n",
    "        wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "        \n",
    "        if k + n >= target_num_clips:\n",
    "            break\n",
    "\n",
    "        wav_file2 = 'working_sample.wav'\n",
    "        label = file.split('/')[0]\n",
    "\n",
    "        # Preprocess the data  (without VAD)\n",
    "        padding(wav_file, wav_file2, 1000)\n",
    "        resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "        audio, sr = librosa.load(wav_file2, sr=16000)\n",
    "        noise = np.random.randn(len(audio))\n",
    "        noise_level = 0.1\n",
    "        audio_noise = audio + noise_level * noise\n",
    "        pitch_shift = np.random.uniform(-100, 100)\n",
    "        audio_pitch = librosa.effects.pitch_shift(audio_noise, sr=sr, n_steps=pitch_shift/100.0)\n",
    "        wavfile.write(wav_file2, sr, audio_pitch.astype(np.float32))\n",
    "        \n",
    "        features = extract_features(wav_file2)\n",
    "\n",
    "        # Add to the list\n",
    "        features_train_silence.append([features, label])\n",
    "   \n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train_silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\silence_augmented_training.pkl', 'wb') as f:\n",
    "    pickle.dump(features_train_silence, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57923"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pickle.load(open('extracted_features\\\\features_training.pkl', 'rb'))\n",
    "len(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pickle\n",
    "with open('extracted_features\\\\non_silence_training.pkl', 'rb') as f1, open('extracted_features\\\\silence_augmented_training.pkl', 'rb') as f2:\n",
    "    non_silence = pickle.load(f1)\n",
    "    silence = pickle.load(f2)\n",
    "with open('extracted_features\\\\silence_detection_training.pkl', 'wb') as f:\n",
    "    pickle.dump(non_silence + silence, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train\\\\silence_validation_list.txt', 'r') as file:\n",
    "    lines = file.readlines() + file.readlines()\n",
    "file_paths_val_silence = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:04<00:00, 18.88it/s]\n"
     ]
    }
   ],
   "source": [
    "features_val_silence = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_val_silence), total=len(file_paths_val_silence), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  (without VAD)\n",
    "    padding(wav_file, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_val_silence.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\silence_detection_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(features_val + features_val_silence, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158538\n"
     ]
    }
   ],
   "source": [
    "with open('extracted_features\\\\features_test_silence.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "    print(len(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 39, 44), found shape=(None, 44)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[39m=\u001b[39m load_model(\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mmodel_gru_final_version.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m39\u001b[39m, \u001b[39m44\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Print the predictions\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4lpb5px1.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 39, 44), found shape=(None, 44)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "# Load the model from the .h5 file\n",
    "model = load_model('models\\\\model_gru_final_version.h5')\n",
    "x = np.random.rand(39, 44)\n",
    "predictions = model.predict(x)\n",
    "\n",
    "# Print the predictions\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
