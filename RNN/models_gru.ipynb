{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gru\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import labels_only_detection_training, labels_only_detection_validation, labels_only_detection_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 2.0643 - accuracy: 0.3122\n",
      "Epoch 1: val_accuracy improved from -inf to 0.58673, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 178s 147ms/step - loss: 2.0643 - accuracy: 0.3122 - val_loss: 1.1387 - val_accuracy: 0.5867\n",
      "Epoch 2/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 1.1974 - accuracy: 0.5764\n",
      "Epoch 2: val_accuracy improved from 0.58673 to 0.67404, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 81s 122ms/step - loss: 1.1974 - accuracy: 0.5764 - val_loss: 0.9452 - val_accuracy: 0.6740\n",
      "Epoch 3/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.9642 - accuracy: 0.6688\n",
      "Epoch 3: val_accuracy improved from 0.67404 to 0.74078, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 102s 154ms/step - loss: 0.9642 - accuracy: 0.6688 - val_loss: 0.7850 - val_accuracy: 0.7408\n",
      "Epoch 4/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.8318 - accuracy: 0.7175\n",
      "Epoch 4: val_accuracy improved from 0.74078 to 0.74272, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 108s 162ms/step - loss: 0.8318 - accuracy: 0.7175 - val_loss: 0.7799 - val_accuracy: 0.7427\n",
      "Epoch 5/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.7551 - accuracy: 0.7454\n",
      "Epoch 5: val_accuracy improved from 0.74272 to 0.77066, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 100s 152ms/step - loss: 0.7551 - accuracy: 0.7454 - val_loss: 0.6732 - val_accuracy: 0.7707\n",
      "Epoch 6/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.6923 - accuracy: 0.7675\n",
      "Epoch 6: val_accuracy improved from 0.77066 to 0.78502, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 113s 170ms/step - loss: 0.6923 - accuracy: 0.7675 - val_loss: 0.6352 - val_accuracy: 0.7850\n",
      "Epoch 7/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.6482 - accuracy: 0.7837\n",
      "Epoch 7: val_accuracy improved from 0.78502 to 0.81645, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 111s 168ms/step - loss: 0.6482 - accuracy: 0.7837 - val_loss: 0.5533 - val_accuracy: 0.8165\n",
      "Epoch 8/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.6094 - accuracy: 0.7967\n",
      "Epoch 8: val_accuracy improved from 0.81645 to 0.82460, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 122s 184ms/step - loss: 0.6094 - accuracy: 0.7967 - val_loss: 0.5455 - val_accuracy: 0.8246\n",
      "Epoch 9/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.8090\n",
      "Epoch 9: val_accuracy improved from 0.82460 to 0.82615, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 114s 172ms/step - loss: 0.5739 - accuracy: 0.8090 - val_loss: 0.5202 - val_accuracy: 0.8262\n",
      "Epoch 10/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.8223\n",
      "Epoch 10: val_accuracy improved from 0.82615 to 0.83508, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 112s 169ms/step - loss: 0.5406 - accuracy: 0.8223 - val_loss: 0.5253 - val_accuracy: 0.8351\n",
      "Epoch 11/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.5316 - accuracy: 0.8261\n",
      "Epoch 11: val_accuracy improved from 0.83508 to 0.83896, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 112s 169ms/step - loss: 0.5316 - accuracy: 0.8261 - val_loss: 0.5132 - val_accuracy: 0.8390\n",
      "Epoch 12/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4954 - accuracy: 0.8371\n",
      "Epoch 12: val_accuracy improved from 0.83896 to 0.84517, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 111s 167ms/step - loss: 0.4954 - accuracy: 0.8371 - val_loss: 0.4869 - val_accuracy: 0.8452\n",
      "Epoch 13/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.8401\n",
      "Epoch 13: val_accuracy improved from 0.84517 to 0.84633, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 105s 159ms/step - loss: 0.4896 - accuracy: 0.8401 - val_loss: 0.4721 - val_accuracy: 0.8463\n",
      "Epoch 14/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.8499\n",
      "Epoch 14: val_accuracy improved from 0.84633 to 0.84789, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 87s 131ms/step - loss: 0.4566 - accuracy: 0.8499 - val_loss: 0.4648 - val_accuracy: 0.8479\n",
      "Epoch 15/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4421 - accuracy: 0.8575\n",
      "Epoch 15: val_accuracy did not improve from 0.84789\n",
      "660/660 [==============================] - 84s 127ms/step - loss: 0.4421 - accuracy: 0.8575 - val_loss: 0.5014 - val_accuracy: 0.8393\n",
      "Epoch 16/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4188 - accuracy: 0.8619\n",
      "Epoch 16: val_accuracy did not improve from 0.84789\n",
      "660/660 [==============================] - 81s 122ms/step - loss: 0.4188 - accuracy: 0.8619 - val_loss: 0.4985 - val_accuracy: 0.8428\n",
      "Epoch 17/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.4004 - accuracy: 0.8705\n",
      "Epoch 17: val_accuracy improved from 0.84789 to 0.85215, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 80s 120ms/step - loss: 0.4004 - accuracy: 0.8705 - val_loss: 0.4757 - val_accuracy: 0.8522\n",
      "Epoch 18/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8739\n",
      "Epoch 18: val_accuracy did not improve from 0.85215\n",
      "660/660 [==============================] - 82s 124ms/step - loss: 0.3968 - accuracy: 0.8739 - val_loss: 0.4787 - val_accuracy: 0.8522\n",
      "Epoch 19/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.3799 - accuracy: 0.8748\n",
      "Epoch 19: val_accuracy did not improve from 0.85215\n",
      "660/660 [==============================] - 81s 123ms/step - loss: 0.3799 - accuracy: 0.8748 - val_loss: 0.4912 - val_accuracy: 0.8425\n",
      "Epoch 20/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 0.3629 - accuracy: 0.8816\n",
      "Epoch 20: val_accuracy did not improve from 0.85215\n",
      "660/660 [==============================] - 80s 122ms/step - loss: 0.3629 - accuracy: 0.8816 - val_loss: 0.4726 - val_accuracy: 0.8444\n",
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 2.2544 - accuracy: 0.2539\n",
      "Epoch 1: val_accuracy improved from -inf to 0.45984, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 123s 213ms/step - loss: 2.2544 - accuracy: 0.2539 - val_loss: 1.5491 - val_accuracy: 0.4598\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.2986 - accuracy: 0.5447\n",
      "Epoch 2: val_accuracy improved from 0.45984 to 0.66550, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 57s 172ms/step - loss: 1.2986 - accuracy: 0.5447 - val_loss: 0.9512 - val_accuracy: 0.6655\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.9989 - accuracy: 0.6561\n",
      "Epoch 3: val_accuracy improved from 0.66550 to 0.70120, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 57s 172ms/step - loss: 0.9989 - accuracy: 0.6561 - val_loss: 0.8435 - val_accuracy: 0.7012\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.8561 - accuracy: 0.7097\n",
      "Epoch 4: val_accuracy improved from 0.70120 to 0.72022, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 56s 169ms/step - loss: 0.8561 - accuracy: 0.7097 - val_loss: 0.7903 - val_accuracy: 0.7202\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7647 - accuracy: 0.7400\n",
      "Epoch 5: val_accuracy improved from 0.72022 to 0.79007, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 57s 171ms/step - loss: 0.7647 - accuracy: 0.7400 - val_loss: 0.6502 - val_accuracy: 0.7901\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6921 - accuracy: 0.7661\n",
      "Epoch 6: val_accuracy improved from 0.79007 to 0.79356, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 62s 187ms/step - loss: 0.6921 - accuracy: 0.7661 - val_loss: 0.6198 - val_accuracy: 0.7936\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6435 - accuracy: 0.7821\n",
      "Epoch 7: val_accuracy improved from 0.79356 to 0.80365, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 61s 184ms/step - loss: 0.6435 - accuracy: 0.7821 - val_loss: 0.6027 - val_accuracy: 0.8036\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6022 - accuracy: 0.7960\n",
      "Epoch 8: val_accuracy did not improve from 0.80365\n",
      "330/330 [==============================] - 61s 184ms/step - loss: 0.6022 - accuracy: 0.7960 - val_loss: 0.6577 - val_accuracy: 0.7866\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.8065\n",
      "Epoch 9: val_accuracy improved from 0.80365 to 0.80753, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 63s 192ms/step - loss: 0.5692 - accuracy: 0.8065 - val_loss: 0.5974 - val_accuracy: 0.8075\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5387 - accuracy: 0.8209\n",
      "Epoch 10: val_accuracy improved from 0.80753 to 0.81684, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 58s 175ms/step - loss: 0.5387 - accuracy: 0.8209 - val_loss: 0.5416 - val_accuracy: 0.8168\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.8294\n",
      "Epoch 11: val_accuracy did not improve from 0.81684\n",
      "330/330 [==============================] - 54s 163ms/step - loss: 0.5098 - accuracy: 0.8294 - val_loss: 0.5631 - val_accuracy: 0.8149\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4885 - accuracy: 0.8352\n",
      "Epoch 12: val_accuracy did not improve from 0.81684\n",
      "330/330 [==============================] - 49s 148ms/step - loss: 0.4885 - accuracy: 0.8352 - val_loss: 0.6451 - val_accuracy: 0.7839\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.8425\n",
      "Epoch 13: val_accuracy improved from 0.81684 to 0.83236, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 62s 187ms/step - loss: 0.4742 - accuracy: 0.8425 - val_loss: 0.5274 - val_accuracy: 0.8324\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8506\n",
      "Epoch 14: val_accuracy did not improve from 0.83236\n",
      "330/330 [==============================] - 56s 170ms/step - loss: 0.4482 - accuracy: 0.8506 - val_loss: 0.5670 - val_accuracy: 0.8137\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4295 - accuracy: 0.8570\n",
      "Epoch 15: val_accuracy did not improve from 0.83236\n",
      "330/330 [==============================] - 62s 186ms/step - loss: 0.4295 - accuracy: 0.8570 - val_loss: 0.5275 - val_accuracy: 0.8296\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4127 - accuracy: 0.8653\n",
      "Epoch 16: val_accuracy did not improve from 0.83236\n",
      "330/330 [==============================] - 63s 189ms/step - loss: 0.4127 - accuracy: 0.8653 - val_loss: 0.5383 - val_accuracy: 0.8199\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3985 - accuracy: 0.8682\n",
      "Epoch 17: val_accuracy improved from 0.83236 to 0.83314, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 61s 185ms/step - loss: 0.3985 - accuracy: 0.8682 - val_loss: 0.5089 - val_accuracy: 0.8331\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8715\n",
      "Epoch 18: val_accuracy improved from 0.83314 to 0.83818, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 63s 189ms/step - loss: 0.3896 - accuracy: 0.8715 - val_loss: 0.5138 - val_accuracy: 0.8382\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8766\n",
      "Epoch 19: val_accuracy improved from 0.83818 to 0.84400, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 69s 210ms/step - loss: 0.3735 - accuracy: 0.8766 - val_loss: 0.4874 - val_accuracy: 0.8440\n",
      "Epoch 20/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8835\n",
      "Epoch 20: val_accuracy improved from 0.84400 to 0.84594, saving model to models\\gru.h5\n",
      "330/330 [==============================] - 67s 203ms/step - loss: 0.3565 - accuracy: 0.8835 - val_loss: 0.4819 - val_accuracy: 0.8459\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 2.4798 - accuracy: 0.1975\n",
      "Epoch 1: val_accuracy improved from -inf to 0.21731, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 325s 739ms/step - loss: 2.4798 - accuracy: 0.1975 - val_loss: 2.0807 - val_accuracy: 0.2173\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.5653 - accuracy: 0.4534\n",
      "Epoch 2: val_accuracy improved from 0.21731 to 0.61195, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 73s 443ms/step - loss: 1.5653 - accuracy: 0.4534 - val_loss: 1.1247 - val_accuracy: 0.6120\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.1246 - accuracy: 0.6034\n",
      "Epoch 3: val_accuracy improved from 0.61195 to 0.67947, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 79s 476ms/step - loss: 1.1246 - accuracy: 0.6034 - val_loss: 0.8850 - val_accuracy: 0.6795\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.9466 - accuracy: 0.6738\n",
      "Epoch 4: val_accuracy improved from 0.67947 to 0.73380, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 72s 434ms/step - loss: 0.9466 - accuracy: 0.6738 - val_loss: 0.7590 - val_accuracy: 0.7338\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.8264 - accuracy: 0.7159\n",
      "Epoch 5: val_accuracy improved from 0.73380 to 0.76329, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 66s 397ms/step - loss: 0.8264 - accuracy: 0.7159 - val_loss: 0.6917 - val_accuracy: 0.7633\n",
      "Epoch 6/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.7534 - accuracy: 0.7445\n",
      "Epoch 6: val_accuracy improved from 0.76329 to 0.77105, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 71s 433ms/step - loss: 0.7534 - accuracy: 0.7445 - val_loss: 0.6682 - val_accuracy: 0.7711\n",
      "Epoch 7/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6926 - accuracy: 0.7623\n",
      "Epoch 7: val_accuracy did not improve from 0.77105\n",
      "165/165 [==============================] - 70s 424ms/step - loss: 0.6926 - accuracy: 0.7623 - val_loss: 0.6638 - val_accuracy: 0.7621\n",
      "Epoch 8/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6426 - accuracy: 0.7791\n",
      "Epoch 8: val_accuracy improved from 0.77105 to 0.80132, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 82s 496ms/step - loss: 0.6426 - accuracy: 0.7791 - val_loss: 0.5900 - val_accuracy: 0.8013\n",
      "Epoch 9/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.7923\n",
      "Epoch 9: val_accuracy improved from 0.80132 to 0.80171, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 73s 441ms/step - loss: 0.6079 - accuracy: 0.7923 - val_loss: 0.6280 - val_accuracy: 0.8017\n",
      "Epoch 10/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8028\n",
      "Epoch 10: val_accuracy improved from 0.80171 to 0.81024, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 72s 438ms/step - loss: 0.5772 - accuracy: 0.8028 - val_loss: 0.5515 - val_accuracy: 0.8102\n",
      "Epoch 11/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.8127\n",
      "Epoch 11: val_accuracy improved from 0.81024 to 0.82305, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 71s 429ms/step - loss: 0.5488 - accuracy: 0.8127 - val_loss: 0.5281 - val_accuracy: 0.8231\n",
      "Epoch 12/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.8202\n",
      "Epoch 12: val_accuracy improved from 0.82305 to 0.83198, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 68s 410ms/step - loss: 0.5239 - accuracy: 0.8202 - val_loss: 0.5161 - val_accuracy: 0.8320\n",
      "Epoch 13/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.8278\n",
      "Epoch 13: val_accuracy did not improve from 0.83198\n",
      "165/165 [==============================] - 71s 432ms/step - loss: 0.5114 - accuracy: 0.8278 - val_loss: 0.5751 - val_accuracy: 0.8114\n",
      "Epoch 14/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4830 - accuracy: 0.8372\n",
      "Epoch 14: val_accuracy did not improve from 0.83198\n",
      "165/165 [==============================] - 72s 437ms/step - loss: 0.4830 - accuracy: 0.8372 - val_loss: 0.5334 - val_accuracy: 0.8184\n",
      "Epoch 15/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4624 - accuracy: 0.8437\n",
      "Epoch 15: val_accuracy did not improve from 0.83198\n",
      "165/165 [==============================] - 69s 418ms/step - loss: 0.4624 - accuracy: 0.8437 - val_loss: 0.5168 - val_accuracy: 0.8320\n",
      "Epoch 16/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4501 - accuracy: 0.8471\n",
      "Epoch 16: val_accuracy improved from 0.83198 to 0.84090, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 73s 442ms/step - loss: 0.4501 - accuracy: 0.8471 - val_loss: 0.4608 - val_accuracy: 0.8409\n",
      "Epoch 17/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4314 - accuracy: 0.8560\n",
      "Epoch 17: val_accuracy did not improve from 0.84090\n",
      "165/165 [==============================] - 69s 416ms/step - loss: 0.4314 - accuracy: 0.8560 - val_loss: 0.4988 - val_accuracy: 0.8343\n",
      "Epoch 18/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.4115 - accuracy: 0.8589\n",
      "Epoch 18: val_accuracy improved from 0.84090 to 0.84129, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 79s 478ms/step - loss: 0.4115 - accuracy: 0.8589 - val_loss: 0.4899 - val_accuracy: 0.8413\n",
      "Epoch 19/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8709\n",
      "Epoch 19: val_accuracy improved from 0.84129 to 0.84517, saving model to models\\gru.h5\n",
      "165/165 [==============================] - 75s 452ms/step - loss: 0.3863 - accuracy: 0.8709 - val_loss: 0.4755 - val_accuracy: 0.8452\n",
      "Epoch 20/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8675\n",
      "Epoch 20: val_accuracy did not improve from 0.84517\n",
      "165/165 [==============================] - 74s 448ms/step - loss: 0.3875 - accuracy: 0.8675 - val_loss: 0.4694 - val_accuracy: 0.8432\n",
      "Epoch 1/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 1.6900 - accuracy: 0.4037\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55452, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 285s 241ms/step - loss: 1.6900 - accuracy: 0.4037 - val_loss: 1.5017 - val_accuracy: 0.5545\n",
      "Epoch 2/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 1.2229 - accuracy: 0.5790\n",
      "Epoch 2: val_accuracy improved from 0.55452 to 0.59139, saving model to models\\gru.h5\n",
      "660/660 [==============================] - 113s 170ms/step - loss: 1.2229 - accuracy: 0.5790 - val_loss: 1.1772 - val_accuracy: 0.5914\n",
      "Epoch 3/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 1.2430 - accuracy: 0.5770\n",
      "Epoch 3: val_accuracy did not improve from 0.59139\n",
      "660/660 [==============================] - 107s 163ms/step - loss: 1.2430 - accuracy: 0.5770 - val_loss: 1.4145 - val_accuracy: 0.5289\n",
      "Epoch 4/20\n",
      "660/660 [==============================] - ETA: 0s - loss: 1.3409 - accuracy: 0.5446\n",
      "Epoch 4: val_accuracy did not improve from 0.59139\n",
      "660/660 [==============================] - 120s 181ms/step - loss: 1.3409 - accuracy: 0.5446 - val_loss: 1.5281 - val_accuracy: 0.4614\n",
      "Epoch 5/20\n",
      "384/660 [================>.............] - ETA: 53s - loss: 1.4809 - accuracy: 0.4857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m [\u001b[39m32\u001b[39m,\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m]:\n\u001b[0;32m      7\u001b[0m     model \u001b[39m=\u001b[39m Gru(gru_units\u001b[39m=\u001b[39mgru_unit, dropout_rate\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, learning_rate\u001b[39m=\u001b[39mlr, num_classes\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch, epoch\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m----> 8\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(labels_only_detection_training, labels_only_detection_validation)\n\u001b[0;32m      9\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((res, pd\u001b[39m.\u001b[39mDataFrame([[gru_unit,\u001b[39m0.2\u001b[39m, lr, epoch, batch, model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]], \n\u001b[0;32m     10\u001b[0m                                                             columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgru_units\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\models.py:145\u001b[0m, in \u001b[0;36mGru.train\u001b[1;34m(self, train_Dataset, val_Dataset)\u001b[0m\n\u001b[0;32m    143\u001b[0m early \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m    144\u001b[0m callbacks_list \u001b[39m=\u001b[39m [checkpoint, early]\n\u001b[1;32m--> 145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(train_Dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size), validation_data\u001b[39m=\u001b[39;49mval_Dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size), epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch, callbacks\u001b[39m=\u001b[39;49mcallbacks_list)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])\n",
    "# res.columns = ['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max']\n",
    "for gru_unit in [64,128]:\n",
    "    for epoch in [20, 30]:\n",
    "        for lr in [0.001, 0.01, 0.1]:\n",
    "            for batch in [64, 32, 128]:\n",
    "                model = Gru(gru_units=gru_unit, dropout_rate=0.1, learning_rate=lr, num_classes=10, batch_size=batch, epoch=epoch)\n",
    "                model.train(labels_only_detection_training, labels_only_detection_validation)\n",
    "                res = np.concatenate((res, pd.DataFrame([[gru_unit,0.1, lr, epoch, batch, model.history.history['loss'][-1], model.history.history['accuracy'][-1], model.history.history['val_loss'][-1], model.history.history['val_accuracy'][-1]]], \n",
    "                                                                        columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res,columns=['gru_units', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max']).to_pickle('results\\\\gru_wyniki_only_known.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_pickle('results\\\\again_search_for_hparams.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gru_units</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.017513</td>\n",
       "      <td>0.996447</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>0.999709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>64.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>30.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.016528</td>\n",
       "      <td>0.997584</td>\n",
       "      <td>0.007348</td>\n",
       "      <td>0.999709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>128.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.01021</td>\n",
       "      <td>0.99801</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.999709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gru_units dropout_rate epoch  batch  loss_max accuracy_max val_loss_max  \\\n",
       "2       64.0          0.2  20.0  128.0  0.017513     0.996447     0.002362   \n",
       "10      64.0          0.2  30.0   64.0  0.016528     0.997584     0.007348   \n",
       "19     128.0          0.2  20.0   64.0   0.01021      0.99801     0.002979   \n",
       "\n",
       "   val_accuracy_max  \n",
       "2          0.999709  \n",
       "10         0.999709  \n",
       "19         0.999709  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[res.val_accuracy_max == res.val_accuracy_max.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gru(128,0.2,20,64,0.01,(39,44), 30, 'models\\\\gru_mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.9617 - accuracy: 0.5392\n",
      "Epoch 1: accuracy improved from -inf to 0.53913, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 22s 155ms/step - loss: 1.9615 - accuracy: 0.5391\n",
      "Epoch 2/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.4505 - accuracy: 0.6169\n",
      "Epoch 2: accuracy improved from 0.53913 to 0.61695, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 15s 141ms/step - loss: 1.4497 - accuracy: 0.6169\n",
      "Epoch 3/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.2981 - accuracy: 0.6288\n",
      "Epoch 3: accuracy improved from 0.61695 to 0.62871, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.2993 - accuracy: 0.6287\n",
      "Epoch 4/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.1761 - accuracy: 0.6439\n",
      "Epoch 4: accuracy improved from 0.62871 to 0.64342, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 1.1770 - accuracy: 0.6434\n",
      "Epoch 5/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.0653 - accuracy: 0.6630\n",
      "Epoch 5: accuracy improved from 0.64342 to 0.66284, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 1.0653 - accuracy: 0.6628\n",
      "Epoch 6/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.0097 - accuracy: 0.6744\n",
      "Epoch 6: accuracy improved from 0.66284 to 0.67417, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 16s 153ms/step - loss: 1.0101 - accuracy: 0.6742\n",
      "Epoch 7/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9609 - accuracy: 0.6899\n",
      "Epoch 7: accuracy improved from 0.67417 to 0.68947, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.9616 - accuracy: 0.6895\n",
      "Epoch 8/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9457 - accuracy: 0.6971\n",
      "Epoch 8: accuracy improved from 0.68947 to 0.69741, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.9450 - accuracy: 0.6974\n",
      "Epoch 9/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9223 - accuracy: 0.7046\n",
      "Epoch 9: accuracy improved from 0.69741 to 0.70447, saving model to models\\gru_mod.h5\n",
      "107/107 [==============================] - 18s 165ms/step - loss: 0.9237 - accuracy: 0.7045\n",
      "Epoch 10/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9233 - accuracy: 0.7000\n",
      "Epoch 10: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 0.9235 - accuracy: 0.6998\n",
      "Epoch 11/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9335 - accuracy: 0.7028\n",
      "Epoch 11: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.9331 - accuracy: 0.7027\n",
      "Epoch 12/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9567 - accuracy: 0.6921\n",
      "Epoch 12: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 148ms/step - loss: 0.9565 - accuracy: 0.6921\n",
      "Epoch 13/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9481 - accuracy: 0.6971\n",
      "Epoch 13: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.9490 - accuracy: 0.6970\n",
      "Epoch 14/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9751 - accuracy: 0.6865\n",
      "Epoch 14: accuracy did not improve from 0.70447\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.9748 - accuracy: 0.6867\n"
     ]
    }
   ],
   "source": [
    "model.train(label_detection_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clip_000044442.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>clip_0000adecb.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clip_0000d4322.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clip_0000fb6fe.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clip_0001d1559.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158533</th>\n",
       "      <td>clip_fffe49419.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158534</th>\n",
       "      <td>clip_ffff2fb36.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158535</th>\n",
       "      <td>clip_ffff90f56.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158536</th>\n",
       "      <td>clip_ffff98589.wav</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158537</th>\n",
       "      <td>clip_ffffc7358.wav</td>\n",
       "      <td>unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     fname    label\n",
       "0       clip_000044442.wav  unknown\n",
       "1       clip_0000adecb.wav  unknown\n",
       "2       clip_0000d4322.wav  unknown\n",
       "3       clip_0000fb6fe.wav  unknown\n",
       "4       clip_0001d1559.wav  unknown\n",
       "...                    ...      ...\n",
       "158533  clip_fffe49419.wav  unknown\n",
       "158534  clip_ffff2fb36.wav       no\n",
       "158535  clip_ffff90f56.wav       no\n",
       "158536  clip_ffff98589.wav       no\n",
       "158537  clip_ffffc7358.wav  unknown\n",
       "\n",
       "[158538 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import label_detection_training, label_detection_validation, silence_detection_training, silence_detection_validation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TensorflowDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "y_train = np.array([x[1] for x in train])\n",
    "labels = list(np.unique(y_train))\n",
    "train = TensorflowDataset('extracted_features\\\\features_training.pkl', labels=labels).dataset\n",
    "train = train.shuffle(len(train), reshuffle_each_iteration=True)\n",
    "val = TensorflowDataset('extracted_features\\\\features_validation.pkl', labels=labels).dataset\n",
    "val = val.shuffle(len(val), reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_units: 64, dropout_rate: 0.2, epoch: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "1811/1811 [==============================] - 129s 63ms/step - loss: 2.6652 - accuracy: 0.2528 - val_loss: 1.9334 - val_accuracy: 0.4479\n",
      "Epoch 2/10\n",
      "1811/1811 [==============================] - 109s 60ms/step - loss: 1.6147 - accuracy: 0.5191 - val_loss: 1.2439 - val_accuracy: 0.6325\n",
      "Epoch 3/10\n",
      "1811/1811 [==============================] - 115s 63ms/step - loss: 1.2631 - accuracy: 0.6300 - val_loss: 1.1999 - val_accuracy: 0.6531\n",
      "Epoch 4/10\n",
      "1811/1811 [==============================] - 120s 66ms/step - loss: 1.0962 - accuracy: 0.6830 - val_loss: 0.9321 - val_accuracy: 0.7339\n",
      "Epoch 5/10\n",
      "1811/1811 [==============================] - 136s 75ms/step - loss: 0.9915 - accuracy: 0.7112 - val_loss: 0.9044 - val_accuracy: 0.7445\n",
      "Epoch 6/10\n",
      "1811/1811 [==============================] - 155s 85ms/step - loss: 0.9212 - accuracy: 0.7350 - val_loss: 0.8960 - val_accuracy: 0.7417\n",
      "Epoch 7/10\n",
      "1811/1811 [==============================] - 155s 86ms/step - loss: 0.8621 - accuracy: 0.7524 - val_loss: 0.9002 - val_accuracy: 0.7554\n",
      "Epoch 8/10\n",
      "1811/1811 [==============================] - 161s 89ms/step - loss: 0.8176 - accuracy: 0.7643 - val_loss: 0.8435 - val_accuracy: 0.7648\n",
      "Epoch 9/10\n",
      "1811/1811 [==============================] - 161s 89ms/step - loss: 0.7814 - accuracy: 0.7758 - val_loss: 0.7811 - val_accuracy: 0.7799\n",
      "Epoch 10/10\n",
      "1811/1811 [==============================] - 163s 90ms/step - loss: 0.7469 - accuracy: 0.7852 - val_loss: 0.7652 - val_accuracy: 0.7899\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "906/906 [==============================] - 142s 117ms/step - loss: 2.7640 - accuracy: 0.2318 - val_loss: 1.6141 - val_accuracy: 0.5143\n",
      "Epoch 2/10\n",
      "906/906 [==============================] - 105s 116ms/step - loss: 1.5994 - accuracy: 0.5215 - val_loss: 1.2850 - val_accuracy: 0.6109\n",
      "Epoch 3/10\n",
      "906/906 [==============================] - 108s 119ms/step - loss: 1.2733 - accuracy: 0.6244 - val_loss: 1.1090 - val_accuracy: 0.6681\n",
      "Epoch 4/10\n",
      "906/906 [==============================] - 119s 131ms/step - loss: 1.0973 - accuracy: 0.6764 - val_loss: 0.9656 - val_accuracy: 0.7161\n",
      "Epoch 5/10\n",
      "906/906 [==============================] - 111s 123ms/step - loss: 0.9803 - accuracy: 0.7128 - val_loss: 0.9651 - val_accuracy: 0.7165\n",
      "Epoch 6/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.9258 - accuracy: 0.7300 - val_loss: 0.8353 - val_accuracy: 0.7573\n",
      "Epoch 7/10\n",
      "906/906 [==============================] - 126s 139ms/step - loss: 0.8567 - accuracy: 0.7512 - val_loss: 0.8387 - val_accuracy: 0.7543\n",
      "Epoch 8/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.8073 - accuracy: 0.7666 - val_loss: 0.8074 - val_accuracy: 0.7671\n",
      "Epoch 9/10\n",
      "906/906 [==============================] - 120s 132ms/step - loss: 0.7510 - accuracy: 0.7821 - val_loss: 0.7063 - val_accuracy: 0.8010\n",
      "Epoch 10/10\n",
      "906/906 [==============================] - 119s 131ms/step - loss: 0.7229 - accuracy: 0.7914 - val_loss: 0.7477 - val_accuracy: 0.7895\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "1811/1811 [==============================] - 266s 127ms/step - loss: 2.6196 - accuracy: 0.2648 - val_loss: 1.7847 - val_accuracy: 0.4485\n",
      "Epoch 2/20\n",
      "1811/1811 [==============================] - 230s 127ms/step - loss: 1.5512 - accuracy: 0.5392 - val_loss: 1.3518 - val_accuracy: 0.5961\n",
      "Epoch 3/20\n",
      "1811/1811 [==============================] - 238s 131ms/step - loss: 1.2536 - accuracy: 0.6312 - val_loss: 1.0939 - val_accuracy: 0.6896\n",
      "Epoch 4/20\n",
      "1811/1811 [==============================] - 223s 123ms/step - loss: 1.0835 - accuracy: 0.6846 - val_loss: 1.0320 - val_accuracy: 0.7024\n",
      "Epoch 5/20\n",
      "1811/1811 [==============================] - 237s 130ms/step - loss: 0.9828 - accuracy: 0.7159 - val_loss: 0.8607 - val_accuracy: 0.7638\n",
      "Epoch 6/20\n",
      "1811/1811 [==============================] - 237s 131ms/step - loss: 0.9098 - accuracy: 0.7380 - val_loss: 0.9389 - val_accuracy: 0.7408\n",
      "Epoch 7/20\n",
      "1811/1811 [==============================] - 240s 132ms/step - loss: 0.8544 - accuracy: 0.7545 - val_loss: 0.8234 - val_accuracy: 0.7780\n",
      "Epoch 8/20\n",
      "1811/1811 [==============================] - 247s 136ms/step - loss: 0.8074 - accuracy: 0.7666 - val_loss: 0.8261 - val_accuracy: 0.7732\n",
      "Epoch 9/20\n",
      "1811/1811 [==============================] - 253s 139ms/step - loss: 0.7867 - accuracy: 0.7745 - val_loss: 0.7653 - val_accuracy: 0.7942\n",
      "Epoch 10/20\n",
      "1811/1811 [==============================] - 259s 143ms/step - loss: 0.7455 - accuracy: 0.7859 - val_loss: 0.7680 - val_accuracy: 0.7871\n",
      "Epoch 11/20\n",
      "1811/1811 [==============================] - 266s 147ms/step - loss: 0.7142 - accuracy: 0.7979 - val_loss: 0.8229 - val_accuracy: 0.7864\n",
      "Epoch 12/20\n",
      "1811/1811 [==============================] - 268s 148ms/step - loss: 0.6908 - accuracy: 0.8039 - val_loss: 0.8267 - val_accuracy: 0.7771\n",
      "Epoch 13/20\n",
      "1811/1811 [==============================] - 273s 150ms/step - loss: 0.6709 - accuracy: 0.8098 - val_loss: 0.7216 - val_accuracy: 0.8089\n",
      "Epoch 14/20\n",
      "1811/1811 [==============================] - 276s 152ms/step - loss: 0.6488 - accuracy: 0.8150 - val_loss: 0.6778 - val_accuracy: 0.8077\n",
      "Epoch 15/20\n",
      "1811/1811 [==============================] - 307s 169ms/step - loss: 0.6429 - accuracy: 0.8168 - val_loss: 0.7247 - val_accuracy: 0.8080\n",
      "Epoch 16/20\n",
      "1811/1811 [==============================] - 309s 171ms/step - loss: 0.6171 - accuracy: 0.8236 - val_loss: 0.6800 - val_accuracy: 0.8189\n",
      "Epoch 17/20\n",
      "1811/1811 [==============================] - 316s 174ms/step - loss: 0.6033 - accuracy: 0.8276 - val_loss: 0.7309 - val_accuracy: 0.8041\n",
      "Epoch 18/20\n",
      "1811/1811 [==============================] - 314s 173ms/step - loss: 0.5969 - accuracy: 0.8284 - val_loss: 0.6816 - val_accuracy: 0.8179\n",
      "Epoch 19/20\n",
      "1811/1811 [==============================] - 313s 173ms/step - loss: 0.5858 - accuracy: 0.8328 - val_loss: 0.7035 - val_accuracy: 0.8113\n",
      "Epoch 20/20\n",
      "1811/1811 [==============================] - 292s 161ms/step - loss: 0.5642 - accuracy: 0.8384 - val_loss: 0.6486 - val_accuracy: 0.8301\n",
      "gru_units: 64, dropout_rate: 0.2, epoch: 20, batch_size: 64\n",
      "Epoch 1/20\n",
      "906/906 [==============================] - 239s 228ms/step - loss: 2.6978 - accuracy: 0.2500 - val_loss: 1.5733 - val_accuracy: 0.5484\n",
      "Epoch 2/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 1.6011 - accuracy: 0.5226 - val_loss: 1.1619 - val_accuracy: 0.6631\n",
      "Epoch 3/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 1.2770 - accuracy: 0.6214 - val_loss: 1.0547 - val_accuracy: 0.6914\n",
      "Epoch 4/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 1.0958 - accuracy: 0.6775 - val_loss: 1.0304 - val_accuracy: 0.6996\n",
      "Epoch 5/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 1.0298 - accuracy: 0.6976 - val_loss: 0.8362 - val_accuracy: 0.7592\n",
      "Epoch 6/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 0.9370 - accuracy: 0.7261 - val_loss: 0.8326 - val_accuracy: 0.7639\n",
      "Epoch 7/20\n",
      "906/906 [==============================] - 213s 235ms/step - loss: 0.8508 - accuracy: 0.7525 - val_loss: 0.7932 - val_accuracy: 0.7773\n",
      "Epoch 8/20\n",
      "906/906 [==============================] - 214s 236ms/step - loss: 0.8232 - accuracy: 0.7599 - val_loss: 0.7637 - val_accuracy: 0.7833\n",
      "Epoch 9/20\n",
      "906/906 [==============================] - 218s 240ms/step - loss: 0.8120 - accuracy: 0.7628 - val_loss: 0.7286 - val_accuracy: 0.7901\n",
      "Epoch 10/20\n",
      "906/906 [==============================] - 220s 242ms/step - loss: 0.7350 - accuracy: 0.7864 - val_loss: 0.7118 - val_accuracy: 0.7971\n",
      "Epoch 11/20\n",
      "906/906 [==============================] - 209s 231ms/step - loss: 0.7040 - accuracy: 0.7967 - val_loss: 0.7114 - val_accuracy: 0.7992\n",
      "Epoch 12/20\n",
      "906/906 [==============================] - 204s 225ms/step - loss: 0.6772 - accuracy: 0.8043 - val_loss: 0.6814 - val_accuracy: 0.8104\n",
      "Epoch 13/20\n",
      "906/906 [==============================] - 201s 222ms/step - loss: 0.6565 - accuracy: 0.8107 - val_loss: 0.6717 - val_accuracy: 0.8060\n",
      "Epoch 14/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 0.6288 - accuracy: 0.8177 - val_loss: 0.6788 - val_accuracy: 0.8086\n",
      "Epoch 15/20\n",
      "906/906 [==============================] - 205s 226ms/step - loss: 0.6370 - accuracy: 0.8161 - val_loss: 0.6551 - val_accuracy: 0.8123\n",
      "Epoch 16/20\n",
      "906/906 [==============================] - 207s 228ms/step - loss: 0.6031 - accuracy: 0.8252 - val_loss: 0.6267 - val_accuracy: 0.8170\n",
      "Epoch 17/20\n",
      "906/906 [==============================] - 206s 227ms/step - loss: 0.5746 - accuracy: 0.8338 - val_loss: 0.6283 - val_accuracy: 0.8210\n",
      "Epoch 18/20\n",
      "906/906 [==============================] - 219s 241ms/step - loss: 0.5615 - accuracy: 0.8374 - val_loss: 0.6015 - val_accuracy: 0.8323\n",
      "Epoch 19/20\n",
      "906/906 [==============================] - 233s 257ms/step - loss: 0.5621 - accuracy: 0.8359 - val_loss: 0.5873 - val_accuracy: 0.8377\n",
      "Epoch 20/20\n",
      "906/906 [==============================] - 234s 258ms/step - loss: 0.5387 - accuracy: 0.8422 - val_loss: 0.6412 - val_accuracy: 0.8180\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 10, batch_size: 32\n",
      "Epoch 1/10\n",
      "1811/1811 [==============================] - 515s 267ms/step - loss: 3.5523 - accuracy: 0.0521 - val_loss: 3.1456 - val_accuracy: 0.0805\n",
      "Epoch 2/10\n",
      "1811/1811 [==============================] - 461s 254ms/step - loss: 2.7619 - accuracy: 0.1851 - val_loss: 2.7847 - val_accuracy: 0.1031\n",
      "Epoch 3/10\n",
      "1811/1811 [==============================] - 484s 267ms/step - loss: 2.1291 - accuracy: 0.3575 - val_loss: 2.1608 - val_accuracy: 0.2746\n",
      "Epoch 4/10\n",
      "1811/1811 [==============================] - 514s 283ms/step - loss: 1.7338 - accuracy: 0.4813 - val_loss: 2.1876 - val_accuracy: 0.2951\n",
      "Epoch 5/10\n",
      "1811/1811 [==============================] - 527s 291ms/step - loss: 1.5662 - accuracy: 0.5394 - val_loss: 1.8745 - val_accuracy: 0.3725\n",
      "Epoch 6/10\n",
      "1811/1811 [==============================] - 508s 281ms/step - loss: 1.4173 - accuracy: 0.5914 - val_loss: 1.8350 - val_accuracy: 0.4320\n",
      "Epoch 7/10\n",
      "1811/1811 [==============================] - 472s 260ms/step - loss: 1.3224 - accuracy: 0.6251 - val_loss: 2.1019 - val_accuracy: 0.3751\n",
      "Epoch 8/10\n",
      "1811/1811 [==============================] - 480s 265ms/step - loss: 1.2329 - accuracy: 0.6530 - val_loss: 1.8472 - val_accuracy: 0.4138\n",
      "Epoch 9/10\n",
      "1811/1811 [==============================] - 489s 270ms/step - loss: 1.1695 - accuracy: 0.6729 - val_loss: 1.8965 - val_accuracy: 0.4179\n",
      "Epoch 10/10\n",
      "1811/1811 [==============================] - 492s 271ms/step - loss: 1.1219 - accuracy: 0.6879 - val_loss: 2.3462 - val_accuracy: 0.3152\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 10, batch_size: 64\n",
      "Epoch 1/10\n",
      "906/906 [==============================] - 370s 379ms/step - loss: 3.6519 - accuracy: 0.0481 - val_loss: 3.3814 - val_accuracy: 0.0666\n",
      "Epoch 2/10\n",
      "906/906 [==============================] - 361s 398ms/step - loss: 3.0161 - accuracy: 0.1241 - val_loss: 3.0356 - val_accuracy: 0.0955\n",
      "Epoch 3/10\n",
      "906/906 [==============================] - 369s 407ms/step - loss: 2.6202 - accuracy: 0.2091 - val_loss: 2.6361 - val_accuracy: 0.2109\n",
      "Epoch 4/10\n",
      "906/906 [==============================] - 344s 379ms/step - loss: 2.2420 - accuracy: 0.3127 - val_loss: 2.5372 - val_accuracy: 0.2005\n",
      "Epoch 5/10\n",
      "906/906 [==============================] - 331s 365ms/step - loss: 1.8991 - accuracy: 0.4225 - val_loss: 2.4049 - val_accuracy: 0.2492\n",
      "Epoch 6/10\n",
      "906/906 [==============================] - 333s 368ms/step - loss: 1.6368 - accuracy: 0.5098 - val_loss: 2.1561 - val_accuracy: 0.3233\n",
      "Epoch 7/10\n",
      "906/906 [==============================] - 332s 366ms/step - loss: 1.4677 - accuracy: 0.5700 - val_loss: 2.1004 - val_accuracy: 0.3413\n",
      "Epoch 8/10\n",
      "906/906 [==============================] - 334s 369ms/step - loss: 1.3544 - accuracy: 0.6111 - val_loss: 2.0290 - val_accuracy: 0.3632\n",
      "Epoch 9/10\n",
      "906/906 [==============================] - 338s 373ms/step - loss: 1.2534 - accuracy: 0.6447 - val_loss: 2.3293 - val_accuracy: 0.2729\n",
      "Epoch 10/10\n",
      "906/906 [==============================] - 340s 375ms/step - loss: 1.1851 - accuracy: 0.6684 - val_loss: 2.2277 - val_accuracy: 0.3076\n",
      "gru_units: 64, dropout_rate: 0.4, epoch: 20, batch_size: 32\n",
      "Epoch 1/20\n",
      "1811/1811 [==============================] - 535s 284ms/step - loss: 3.4949 - accuracy: 0.0639 - val_loss: 3.3306 - val_accuracy: 0.0558\n",
      "Epoch 2/20\n",
      "1811/1811 [==============================] - 528s 291ms/step - loss: 2.5940 - accuracy: 0.2265 - val_loss: 2.9047 - val_accuracy: 0.1165\n",
      "Epoch 3/20\n",
      "1811/1811 [==============================] - 538s 297ms/step - loss: 2.0765 - accuracy: 0.3648 - val_loss: 2.9113 - val_accuracy: 0.1443\n",
      "Epoch 4/20\n",
      "1811/1811 [==============================] - 548s 302ms/step - loss: 1.7885 - accuracy: 0.4575 - val_loss: 2.4928 - val_accuracy: 0.1842\n",
      "Epoch 5/20\n",
      "1811/1811 [==============================] - 591s 326ms/step - loss: 1.5852 - accuracy: 0.5346 - val_loss: 2.4043 - val_accuracy: 0.2342\n",
      "Epoch 6/20\n",
      "1811/1811 [==============================] - 616s 340ms/step - loss: 1.4329 - accuracy: 0.5890 - val_loss: 2.1779 - val_accuracy: 0.2623\n",
      "Epoch 7/20\n",
      "1811/1811 [==============================] - 624s 344ms/step - loss: 1.3117 - accuracy: 0.6281 - val_loss: 2.0153 - val_accuracy: 0.3517\n",
      "Epoch 8/20\n",
      "1811/1811 [==============================] - 675s 373ms/step - loss: 1.2338 - accuracy: 0.6570 - val_loss: 1.9247 - val_accuracy: 0.3519\n",
      "Epoch 9/20\n",
      "1811/1811 [==============================] - 676s 373ms/step - loss: 1.1695 - accuracy: 0.6764 - val_loss: 1.9387 - val_accuracy: 0.3592\n",
      "Epoch 10/20\n",
      "1811/1811 [==============================] - 681s 376ms/step - loss: 1.1301 - accuracy: 0.6889 - val_loss: 2.2268 - val_accuracy: 0.2564\n",
      "Epoch 11/20\n",
      "1811/1811 [==============================] - 664s 366ms/step - loss: 1.0817 - accuracy: 0.7024 - val_loss: 1.9388 - val_accuracy: 0.3379\n",
      "Epoch 12/20\n",
      "1811/1811 [==============================] - 665s 367ms/step - loss: 1.0434 - accuracy: 0.7155 - val_loss: 2.1829 - val_accuracy: 0.2554\n",
      "Epoch 13/20\n",
      "1811/1811 [==============================] - 669s 369ms/step - loss: 1.0158 - accuracy: 0.7243 - val_loss: 2.0610 - val_accuracy: 0.3099\n",
      "Epoch 14/20\n",
      "1811/1811 [==============================] - 686s 378ms/step - loss: 0.9954 - accuracy: 0.7283 - val_loss: 2.0930 - val_accuracy: 0.2842\n",
      "Epoch 15/20\n",
      "1811/1811 [==============================] - 689s 380ms/step - loss: 0.9680 - accuracy: 0.7364 - val_loss: 2.1951 - val_accuracy: 0.2835\n",
      "Epoch 16/20\n",
      "1179/1811 [==================>...........] - ETA: 3:33 - loss: 0.9402 - accuracy: 0.7461"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\models_gru.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# Train the model with your training set and validate it with your validation set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mbatch(batch_size), epochs\u001b[39m=\u001b[39;49mepoch, validation_data\u001b[39m=\u001b[39;49mval\u001b[39m.\u001b[39;49mbatch(batch_size))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m results \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((results, pd\u001b[39m.\u001b[39mDataFrame([[gru_units, dropout_rate, epoch, batch_size, history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m]]], \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_gru.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                                                 columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mgru_units\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set your model's hyperparameters\n",
    "results = pd.DataFrame(columns=['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "from tensorflow.keras.layers import GRU, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "for gru_units in [64, 128]:\n",
    "    for dropout_rate in [0.2, 0.4]:\n",
    "            for epoch in [ 10, 20]:\n",
    "                  for batch_size in [32, 64]:\n",
    "                        print(f'gru_units: {gru_units}, dropout_rate: {dropout_rate}, epoch: {epoch}, batch_size: {batch_size}')\n",
    "                        num_classes = len(np.unique(labels))  # Number of unique classes in your dataset\n",
    "\n",
    "                        input_shape = (39, 44)\n",
    "\n",
    "\n",
    "                        model = Sequential([\n",
    "                            Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "                            BatchNormalization(),\n",
    "                            MaxPooling1D(pool_size=2),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GRU(gru_units, return_sequences=True),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GRU(gru_units, return_sequences=True),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(256, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(128, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            TimeDistributed(Dense(64, activation='relu')),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            GlobalMaxPooling1D(),\n",
    "                            Dense(512, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(256, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(128, activation='relu'),\n",
    "                            BatchNormalization(),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(num_classes, activation='softmax')\n",
    "                        ])\n",
    "\n",
    "\n",
    "                        # Compile the model\n",
    "                        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                        # Train the model with your training set and validate it with your validation set\n",
    "                        history = model.fit(train.batch(batch_size), epochs=epoch, validation_data=val.batch(batch_size))\n",
    "\n",
    "                        results = np.concatenate((results, pd.DataFrame([[gru_units, dropout_rate, epoch, batch_size, history.history['loss'][-1], history.history['loss'], history.history['accuracy'][-1], history.history['accuracy'], history.history['val_loss'][-1], history.history['val_loss'], history.history['val_accuracy'][-1], history.history['val_accuracy']]], \n",
    "                                                                        columns=['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_f.to_pickle('results\\\\model_gru_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.746932</td>\n",
       "      <td>[2.665213108062744, 1.6146924495697021, 1.2631...</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>[0.2528011202812195, 0.5191202163696289, 0.629...</td>\n",
       "      <td>0.765207</td>\n",
       "      <td>[1.9333562850952148, 1.2438899278640747, 1.199...</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>[0.447925865650177, 0.6325389742851257, 0.6531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>0.722944</td>\n",
       "      <td>[2.7639663219451904, 1.5994480848312378, 1.273...</td>\n",
       "      <td>0.791395</td>\n",
       "      <td>[0.23180773854255676, 0.521537184715271, 0.624...</td>\n",
       "      <td>0.747728</td>\n",
       "      <td>[1.6140880584716797, 1.2849770784378052, 1.108...</td>\n",
       "      <td>0.789497</td>\n",
       "      <td>[0.5142688751220703, 0.610914945602417, 0.6681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.564152</td>\n",
       "      <td>[2.6195595264434814, 1.5511832237243652, 1.253...</td>\n",
       "      <td>0.838389</td>\n",
       "      <td>[0.26476529240608215, 0.5392158627510071, 0.63...</td>\n",
       "      <td>0.648608</td>\n",
       "      <td>[1.784652590751648, 1.3517918586730957, 1.0938...</td>\n",
       "      <td>0.830097</td>\n",
       "      <td>[0.44851428270339966, 0.596057653427124, 0.689...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0.538738</td>\n",
       "      <td>[2.697845935821533, 1.6010715961456299, 1.2770...</td>\n",
       "      <td>0.842222</td>\n",
       "      <td>[0.2499525249004364, 0.5225730538368225, 0.621...</td>\n",
       "      <td>0.641245</td>\n",
       "      <td>[1.5733102560043335, 1.1619383096694946, 1.054...</td>\n",
       "      <td>0.818035</td>\n",
       "      <td>[0.5483965873718262, 0.6631362438201904, 0.691...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1.121856</td>\n",
       "      <td>[3.5523223876953125, 2.7618539333343506, 2.129...</td>\n",
       "      <td>0.687948</td>\n",
       "      <td>[0.052138183265924454, 0.1850905567407608, 0.3...</td>\n",
       "      <td>2.346172</td>\n",
       "      <td>[3.1456222534179688, 2.78471302986145, 2.16078...</td>\n",
       "      <td>0.31524</td>\n",
       "      <td>[0.08046483993530273, 0.10311856120824814, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>64</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>64</td>\n",
       "      <td>1.185146</td>\n",
       "      <td>[3.6519148349761963, 3.016111135482788, 2.6201...</td>\n",
       "      <td>0.668422</td>\n",
       "      <td>[0.04808107390999794, 0.12409578263759613, 0.2...</td>\n",
       "      <td>2.227695</td>\n",
       "      <td>[3.3813705444335938, 3.035633087158203, 2.6361...</td>\n",
       "      <td>0.30759</td>\n",
       "      <td>[0.06663724780082703, 0.09546925872564316, 0.2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3         4   \\\n",
       "0  64  0.2  10  32  0.746932   \n",
       "1  64  0.2  10  64  0.722944   \n",
       "2  64  0.2  20  32  0.564152   \n",
       "3  64  0.2  20  64  0.538738   \n",
       "4  64  0.4  10  32  1.121856   \n",
       "5  64  0.4  10  64  1.185146   \n",
       "\n",
       "                                                  5         6   \\\n",
       "0  [2.665213108062744, 1.6146924495697021, 1.2631...  0.785215   \n",
       "1  [2.7639663219451904, 1.5994480848312378, 1.273...  0.791395   \n",
       "2  [2.6195595264434814, 1.5511832237243652, 1.253...  0.838389   \n",
       "3  [2.697845935821533, 1.6010715961456299, 1.2770...  0.842222   \n",
       "4  [3.5523223876953125, 2.7618539333343506, 2.129...  0.687948   \n",
       "5  [3.6519148349761963, 3.016111135482788, 2.6201...  0.668422   \n",
       "\n",
       "                                                  7         8   \\\n",
       "0  [0.2528011202812195, 0.5191202163696289, 0.629...  0.765207   \n",
       "1  [0.23180773854255676, 0.521537184715271, 0.624...  0.747728   \n",
       "2  [0.26476529240608215, 0.5392158627510071, 0.63...  0.648608   \n",
       "3  [0.2499525249004364, 0.5225730538368225, 0.621...  0.641245   \n",
       "4  [0.052138183265924454, 0.1850905567407608, 0.3...  2.346172   \n",
       "5  [0.04808107390999794, 0.12409578263759613, 0.2...  2.227695   \n",
       "\n",
       "                                                  9         10  \\\n",
       "0  [1.9333562850952148, 1.2438899278640747, 1.199...  0.789938   \n",
       "1  [1.6140880584716797, 1.2849770784378052, 1.108...  0.789497   \n",
       "2  [1.784652590751648, 1.3517918586730957, 1.0938...  0.830097   \n",
       "3  [1.5733102560043335, 1.1619383096694946, 1.054...  0.818035   \n",
       "4  [3.1456222534179688, 2.78471302986145, 2.16078...   0.31524   \n",
       "5  [3.3813705444335938, 3.035633087158203, 2.6361...   0.30759   \n",
       "\n",
       "                                                  11  \n",
       "0  [0.447925865650177, 0.6325389742851257, 0.6531...  \n",
       "1  [0.5142688751220703, 0.610914945602417, 0.6681...  \n",
       "2  [0.44851428270339966, 0.596057653427124, 0.689...  \n",
       "3  [0.5483965873718262, 0.6631362438201904, 0.691...  \n",
       "4  [0.08046483993530273, 0.10311856120824814, 0.2...  \n",
       "5  [0.06663724780082703, 0.09546925872564316, 0.2...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "214/214 [==============================] - 29s 69ms/step - loss: 6.9579 - accuracy: 0.0613 - val_loss: 3.6162 - val_accuracy: 0.0596\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 2.8994 - accuracy: 0.1918 - val_loss: 2.5313 - val_accuracy: 0.2439\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 2.4038 - accuracy: 0.2900 - val_loss: 2.0998 - val_accuracy: 0.3616\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 2.0500 - accuracy: 0.3930 - val_loss: 1.8843 - val_accuracy: 0.4451\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 1.8284 - accuracy: 0.4449 - val_loss: 1.6862 - val_accuracy: 0.4878\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.7079 - accuracy: 0.4755 - val_loss: 1.6471 - val_accuracy: 0.5157\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 1.6029 - accuracy: 0.5110 - val_loss: 1.7111 - val_accuracy: 0.4846\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.4772 - accuracy: 0.5429 - val_loss: 1.3925 - val_accuracy: 0.5940\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.4434 - accuracy: 0.5593 - val_loss: 1.3605 - val_accuracy: 0.5877\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.3525 - accuracy: 0.5852 - val_loss: 1.5537 - val_accuracy: 0.5327\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 32s 74ms/step - loss: 7.0433 - accuracy: 0.0601 - val_loss: 3.7041 - val_accuracy: 0.0825\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 13s 59ms/step - loss: 2.9475 - accuracy: 0.1612 - val_loss: 2.6197 - val_accuracy: 0.2352\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 2.4830 - accuracy: 0.2714 - val_loss: 2.4180 - val_accuracy: 0.2440\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 14s 63ms/step - loss: 2.2000 - accuracy: 0.3486 - val_loss: 2.0087 - val_accuracy: 0.3954\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 2.0040 - accuracy: 0.3947 - val_loss: 1.8612 - val_accuracy: 0.4391\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 14s 65ms/step - loss: 1.7949 - accuracy: 0.4582 - val_loss: 1.6778 - val_accuracy: 0.5038\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.6568 - accuracy: 0.4979 - val_loss: 1.6263 - val_accuracy: 0.5074\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 1.5562 - accuracy: 0.5257 - val_loss: 1.5525 - val_accuracy: 0.5432\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 12s 57ms/step - loss: 1.4460 - accuracy: 0.5596 - val_loss: 1.4088 - val_accuracy: 0.5755\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.3738 - accuracy: 0.5808 - val_loss: 1.3234 - val_accuracy: 0.6034\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.3215 - accuracy: 0.5972 - val_loss: 1.3095 - val_accuracy: 0.6165\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 13s 59ms/step - loss: 1.2586 - accuracy: 0.6228 - val_loss: 1.3344 - val_accuracy: 0.6078\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.2080 - accuracy: 0.6317 - val_loss: 1.2495 - val_accuracy: 0.6239\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 13s 61ms/step - loss: 1.1354 - accuracy: 0.6506 - val_loss: 1.2837 - val_accuracy: 0.6208\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.0749 - accuracy: 0.6679 - val_loss: 1.2364 - val_accuracy: 0.6302\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 13s 60ms/step - loss: 1.0549 - accuracy: 0.6783 - val_loss: 1.2053 - val_accuracy: 0.6565\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 12s 56ms/step - loss: 1.0081 - accuracy: 0.6963 - val_loss: 1.2228 - val_accuracy: 0.6484\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 10s 46ms/step - loss: 0.9734 - accuracy: 0.7046 - val_loss: 1.1869 - val_accuracy: 0.6556\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 13s 62ms/step - loss: 0.9531 - accuracy: 0.7132 - val_loss: 1.1757 - val_accuracy: 0.6556\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 12s 58ms/step - loss: 0.8807 - accuracy: 0.7359 - val_loss: 1.1120 - val_accuracy: 0.6752\n",
      "Epoch 1/30\n",
      "214/214 [==============================] - 31s 72ms/step - loss: 6.9051 - accuracy: 0.0533 - val_loss: 3.5724 - val_accuracy: 0.0884\n",
      "Epoch 2/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 3.0638 - accuracy: 0.1432 - val_loss: 2.7562 - val_accuracy: 0.1831\n",
      "Epoch 3/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 2.6700 - accuracy: 0.2117 - val_loss: 2.4194 - val_accuracy: 0.2816\n",
      "Epoch 4/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 2.3523 - accuracy: 0.3017 - val_loss: 2.1408 - val_accuracy: 0.3460\n",
      "Epoch 5/30\n",
      "214/214 [==============================] - 16s 75ms/step - loss: 2.0523 - accuracy: 0.3814 - val_loss: 1.9107 - val_accuracy: 0.4219\n",
      "Epoch 6/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.8623 - accuracy: 0.4328 - val_loss: 1.6799 - val_accuracy: 0.4915\n",
      "Epoch 7/30\n",
      "214/214 [==============================] - 15s 69ms/step - loss: 1.7306 - accuracy: 0.4767 - val_loss: 1.8279 - val_accuracy: 0.4532\n",
      "Epoch 8/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.5679 - accuracy: 0.5194 - val_loss: 1.5297 - val_accuracy: 0.5487\n",
      "Epoch 9/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 1.4944 - accuracy: 0.5489 - val_loss: 1.8609 - val_accuracy: 0.4631\n",
      "Epoch 10/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 1.3754 - accuracy: 0.5811 - val_loss: 1.4511 - val_accuracy: 0.5612\n",
      "Epoch 11/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.3142 - accuracy: 0.5962 - val_loss: 1.6520 - val_accuracy: 0.5063\n",
      "Epoch 12/30\n",
      "214/214 [==============================] - 17s 81ms/step - loss: 1.2475 - accuracy: 0.6212 - val_loss: 1.4921 - val_accuracy: 0.5530\n",
      "Epoch 13/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 1.1977 - accuracy: 0.6356 - val_loss: 1.3856 - val_accuracy: 0.5883\n",
      "Epoch 14/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.1326 - accuracy: 0.6516 - val_loss: 1.4644 - val_accuracy: 0.5816\n",
      "Epoch 15/30\n",
      "214/214 [==============================] - 16s 75ms/step - loss: 1.1059 - accuracy: 0.6676 - val_loss: 1.4233 - val_accuracy: 0.5866\n",
      "Epoch 16/30\n",
      "214/214 [==============================] - 16s 74ms/step - loss: 1.0603 - accuracy: 0.6844 - val_loss: 1.3383 - val_accuracy: 0.6167\n",
      "Epoch 17/30\n",
      "214/214 [==============================] - 16s 73ms/step - loss: 1.0167 - accuracy: 0.6873 - val_loss: 1.3003 - val_accuracy: 0.6299\n",
      "Epoch 18/30\n",
      "214/214 [==============================] - 14s 64ms/step - loss: 0.9844 - accuracy: 0.6989 - val_loss: 1.2893 - val_accuracy: 0.6399\n",
      "Epoch 19/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.9126 - accuracy: 0.7257 - val_loss: 1.3397 - val_accuracy: 0.6200\n",
      "Epoch 20/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.9105 - accuracy: 0.7293 - val_loss: 1.3537 - val_accuracy: 0.6242\n",
      "Epoch 21/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.9157 - accuracy: 0.7217 - val_loss: 1.1880 - val_accuracy: 0.6593\n",
      "Epoch 22/30\n",
      "214/214 [==============================] - 17s 77ms/step - loss: 0.8420 - accuracy: 0.7469 - val_loss: 1.2463 - val_accuracy: 0.6512\n",
      "Epoch 23/30\n",
      "214/214 [==============================] - 16s 77ms/step - loss: 0.8305 - accuracy: 0.7448 - val_loss: 1.2248 - val_accuracy: 0.6527\n",
      "Epoch 24/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.8228 - accuracy: 0.7523 - val_loss: 1.2024 - val_accuracy: 0.6561\n",
      "Epoch 25/30\n",
      "214/214 [==============================] - 16s 76ms/step - loss: 0.7958 - accuracy: 0.7546 - val_loss: 1.2288 - val_accuracy: 0.6649\n",
      "Epoch 26/30\n",
      "214/214 [==============================] - 16s 76ms/step - loss: 0.7639 - accuracy: 0.7661 - val_loss: 1.1902 - val_accuracy: 0.6684\n",
      "Epoch 27/30\n",
      "214/214 [==============================] - 17s 78ms/step - loss: 0.7293 - accuracy: 0.7802 - val_loss: 1.4078 - val_accuracy: 0.6174\n",
      "Epoch 28/30\n",
      "214/214 [==============================] - 14s 68ms/step - loss: 0.7016 - accuracy: 0.7819 - val_loss: 1.4334 - val_accuracy: 0.6258\n",
      "Epoch 29/30\n",
      "214/214 [==============================] - 14s 65ms/step - loss: 0.6995 - accuracy: 0.7908 - val_loss: 1.2373 - val_accuracy: 0.6698\n",
      "Epoch 30/30\n",
      "214/214 [==============================] - 14s 64ms/step - loss: 0.6693 - accuracy: 0.7928 - val_loss: 1.3256 - val_accuracy: 0.6462\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 59s 192ms/step - loss: 8.3429 - accuracy: 0.0461 - val_loss: 7.4951 - val_accuracy: 0.0743\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 4.4138 - accuracy: 0.1140 - val_loss: 3.2584 - val_accuracy: 0.0743\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 2.9274 - accuracy: 0.1764 - val_loss: 2.7914 - val_accuracy: 0.1723\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 2.5817 - accuracy: 0.2417 - val_loss: 2.3677 - val_accuracy: 0.2938\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 14s 131ms/step - loss: 2.2904 - accuracy: 0.3198 - val_loss: 2.1464 - val_accuracy: 0.3395\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 14s 127ms/step - loss: 2.0537 - accuracy: 0.3924 - val_loss: 1.9141 - val_accuracy: 0.4188\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 11s 102ms/step - loss: 1.8429 - accuracy: 0.4500 - val_loss: 1.7669 - val_accuracy: 0.4576\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 14s 129ms/step - loss: 1.6742 - accuracy: 0.4931 - val_loss: 1.5686 - val_accuracy: 0.5199\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 14s 127ms/step - loss: 1.5556 - accuracy: 0.5312 - val_loss: 1.4759 - val_accuracy: 0.5547\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 1.4483 - accuracy: 0.5650 - val_loss: 1.5240 - val_accuracy: 0.5382\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 72s 201ms/step - loss: 8.3628 - accuracy: 0.0492 - val_loss: 7.5054 - val_accuracy: 0.0562\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 4.3262 - accuracy: 0.1366 - val_loss: 3.3106 - val_accuracy: 0.0893\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 15s 142ms/step - loss: 2.6514 - accuracy: 0.2497 - val_loss: 2.5653 - val_accuracy: 0.2482\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 14s 128ms/step - loss: 2.2688 - accuracy: 0.3255 - val_loss: 2.1358 - val_accuracy: 0.3650\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 12s 115ms/step - loss: 2.0394 - accuracy: 0.3814 - val_loss: 2.0232 - val_accuracy: 0.3882\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 15s 136ms/step - loss: 1.8691 - accuracy: 0.4279 - val_loss: 1.7886 - val_accuracy: 0.4590\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 1.7005 - accuracy: 0.4775 - val_loss: 1.5619 - val_accuracy: 0.5265\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 15s 141ms/step - loss: 1.5951 - accuracy: 0.5112 - val_loss: 1.6718 - val_accuracy: 0.4918\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 15s 137ms/step - loss: 1.4862 - accuracy: 0.5311 - val_loss: 1.5485 - val_accuracy: 0.5334\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 1.4114 - accuracy: 0.5652 - val_loss: 1.4867 - val_accuracy: 0.5550\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 15s 138ms/step - loss: 1.3064 - accuracy: 0.5927 - val_loss: 1.4195 - val_accuracy: 0.5793\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.2338 - accuracy: 0.6157 - val_loss: 1.4493 - val_accuracy: 0.5716\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 1.1957 - accuracy: 0.6263 - val_loss: 1.2520 - val_accuracy: 0.6296\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.1372 - accuracy: 0.6477 - val_loss: 1.3404 - val_accuracy: 0.6047\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 15s 137ms/step - loss: 1.0871 - accuracy: 0.6595 - val_loss: 1.2558 - val_accuracy: 0.6283\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 13s 121ms/step - loss: 1.0566 - accuracy: 0.6753 - val_loss: 1.3120 - val_accuracy: 0.6087\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - 12s 116ms/step - loss: 1.0077 - accuracy: 0.6869 - val_loss: 1.3003 - val_accuracy: 0.6168\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - 12s 117ms/step - loss: 0.9694 - accuracy: 0.7033 - val_loss: 1.2277 - val_accuracy: 0.6393\n",
      "Epoch 19/20\n",
      "107/107 [==============================] - 13s 122ms/step - loss: 0.9582 - accuracy: 0.7086 - val_loss: 1.1671 - val_accuracy: 0.6623\n",
      "Epoch 20/20\n",
      "107/107 [==============================] - 15s 140ms/step - loss: 0.8884 - accuracy: 0.7321 - val_loss: 1.2016 - val_accuracy: 0.6525\n",
      "Epoch 1/30\n",
      "107/107 [==============================] - 54s 200ms/step - loss: 8.3468 - accuracy: 0.0525 - val_loss: 8.6673 - val_accuracy: 0.0407\n",
      "Epoch 2/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 4.3760 - accuracy: 0.1245 - val_loss: 3.4172 - val_accuracy: 0.1083\n",
      "Epoch 3/30\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 2.7519 - accuracy: 0.2224 - val_loss: 2.7985 - val_accuracy: 0.1893\n",
      "Epoch 4/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 2.3802 - accuracy: 0.3029 - val_loss: 2.1827 - val_accuracy: 0.3420\n",
      "Epoch 5/30\n",
      "107/107 [==============================] - 13s 119ms/step - loss: 2.1121 - accuracy: 0.3772 - val_loss: 2.0110 - val_accuracy: 0.3913\n",
      "Epoch 6/30\n",
      "107/107 [==============================] - 16s 147ms/step - loss: 1.9245 - accuracy: 0.4296 - val_loss: 1.9511 - val_accuracy: 0.3992\n",
      "Epoch 7/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.7435 - accuracy: 0.4682 - val_loss: 1.6947 - val_accuracy: 0.4819\n",
      "Epoch 8/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.6156 - accuracy: 0.5086 - val_loss: 1.5664 - val_accuracy: 0.5237\n",
      "Epoch 9/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.5066 - accuracy: 0.5435 - val_loss: 1.4485 - val_accuracy: 0.5703\n",
      "Epoch 10/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.3992 - accuracy: 0.5666 - val_loss: 1.3999 - val_accuracy: 0.5758\n",
      "Epoch 11/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 1.3325 - accuracy: 0.5971 - val_loss: 1.3934 - val_accuracy: 0.5818\n",
      "Epoch 12/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 1.2557 - accuracy: 0.6164 - val_loss: 1.3154 - val_accuracy: 0.6069\n",
      "Epoch 13/30\n",
      "107/107 [==============================] - 16s 150ms/step - loss: 1.2010 - accuracy: 0.6294 - val_loss: 1.3169 - val_accuracy: 0.6099\n",
      "Epoch 14/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 1.1401 - accuracy: 0.6524 - val_loss: 1.4306 - val_accuracy: 0.5741\n",
      "Epoch 15/30\n",
      "107/107 [==============================] - 14s 132ms/step - loss: 1.0986 - accuracy: 0.6609 - val_loss: 1.2652 - val_accuracy: 0.6330\n",
      "Epoch 16/30\n",
      "107/107 [==============================] - 15s 139ms/step - loss: 1.0815 - accuracy: 0.6634 - val_loss: 1.1975 - val_accuracy: 0.6486\n",
      "Epoch 17/30\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 0.9899 - accuracy: 0.6945 - val_loss: 1.2283 - val_accuracy: 0.6386\n",
      "Epoch 18/30\n",
      "107/107 [==============================] - 16s 153ms/step - loss: 0.9463 - accuracy: 0.7087 - val_loss: 1.1994 - val_accuracy: 0.6534\n",
      "Epoch 19/30\n",
      "107/107 [==============================] - 17s 158ms/step - loss: 0.9244 - accuracy: 0.7140 - val_loss: 1.1858 - val_accuracy: 0.6555\n",
      "Epoch 20/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.8808 - accuracy: 0.7298 - val_loss: 1.3737 - val_accuracy: 0.6021\n",
      "Epoch 21/30\n",
      "107/107 [==============================] - 17s 156ms/step - loss: 0.8691 - accuracy: 0.7298 - val_loss: 1.1265 - val_accuracy: 0.6784\n",
      "Epoch 22/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.8320 - accuracy: 0.7374 - val_loss: 1.0893 - val_accuracy: 0.6905\n",
      "Epoch 23/30\n",
      "107/107 [==============================] - 16s 152ms/step - loss: 0.7950 - accuracy: 0.7564 - val_loss: 1.1373 - val_accuracy: 0.6742\n",
      "Epoch 24/30\n",
      "107/107 [==============================] - 17s 159ms/step - loss: 0.7885 - accuracy: 0.7605 - val_loss: 1.1152 - val_accuracy: 0.6736\n",
      "Epoch 25/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.7610 - accuracy: 0.7650 - val_loss: 1.0783 - val_accuracy: 0.6896\n",
      "Epoch 26/30\n",
      "107/107 [==============================] - 17s 157ms/step - loss: 0.7199 - accuracy: 0.7776 - val_loss: 1.0572 - val_accuracy: 0.6964\n",
      "Epoch 27/30\n",
      "107/107 [==============================] - 17s 155ms/step - loss: 0.7120 - accuracy: 0.7819 - val_loss: 1.1540 - val_accuracy: 0.6696\n",
      "Epoch 28/30\n",
      "107/107 [==============================] - 16s 154ms/step - loss: 0.6881 - accuracy: 0.7899 - val_loss: 1.1242 - val_accuracy: 0.6789\n",
      "Epoch 29/30\n",
      "107/107 [==============================] - 16s 149ms/step - loss: 0.6817 - accuracy: 0.7899 - val_loss: 1.0723 - val_accuracy: 0.6930\n",
      "Epoch 30/30\n",
      "107/107 [==============================] - 13s 120ms/step - loss: 0.6398 - accuracy: 0.8031 - val_loss: 1.1086 - val_accuracy: 0.6815\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 56s 441ms/step - loss: 8.6960 - accuracy: 0.0341 - val_loss: 8.6882 - val_accuracy: 0.0259\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 19s 351ms/step - loss: 7.2206 - accuracy: 0.0764 - val_loss: 7.6456 - val_accuracy: 0.0346\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 18s 337ms/step - loss: 4.2527 - accuracy: 0.1276 - val_loss: 5.6083 - val_accuracy: 0.0388\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 16s 305ms/step - loss: 3.1073 - accuracy: 0.1716 - val_loss: 3.9642 - val_accuracy: 0.0902\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 18s 343ms/step - loss: 2.6839 - accuracy: 0.2394 - val_loss: 2.9942 - val_accuracy: 0.1828\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 19s 355ms/step - loss: 2.3857 - accuracy: 0.2977 - val_loss: 2.3869 - val_accuracy: 0.3008\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 19s 347ms/step - loss: 2.1894 - accuracy: 0.3544 - val_loss: 2.3284 - val_accuracy: 0.3154\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 19s 348ms/step - loss: 2.0403 - accuracy: 0.3857 - val_loss: 2.0037 - val_accuracy: 0.4095\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 19s 350ms/step - loss: 1.9107 - accuracy: 0.4271 - val_loss: 1.8836 - val_accuracy: 0.4387\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 19s 359ms/step - loss: 1.7874 - accuracy: 0.4613 - val_loss: 1.8424 - val_accuracy: 0.4619\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 41s 412ms/step - loss: 8.6832 - accuracy: 0.0410 - val_loss: 8.4885 - val_accuracy: 0.0363\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 19s 361ms/step - loss: 6.9175 - accuracy: 0.0945 - val_loss: 5.1615 - val_accuracy: 0.0844\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 19s 346ms/step - loss: 3.8346 - accuracy: 0.1505 - val_loss: 3.1494 - val_accuracy: 0.0891\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 18s 327ms/step - loss: 2.8827 - accuracy: 0.2108 - val_loss: 2.8984 - val_accuracy: 0.1342\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 2.5728 - accuracy: 0.2628 - val_loss: 2.4879 - val_accuracy: 0.2607\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 19s 361ms/step - loss: 2.3419 - accuracy: 0.3115 - val_loss: 2.2945 - val_accuracy: 0.3214\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 2.1429 - accuracy: 0.3674 - val_loss: 1.9825 - val_accuracy: 0.4107\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.9820 - accuracy: 0.4116 - val_loss: 1.9123 - val_accuracy: 0.4116\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 19s 359ms/step - loss: 1.7590 - accuracy: 0.4676 - val_loss: 1.6838 - val_accuracy: 0.4959\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 19s 354ms/step - loss: 1.6221 - accuracy: 0.5146 - val_loss: 1.8302 - val_accuracy: 0.4737\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 19s 354ms/step - loss: 1.5208 - accuracy: 0.5266 - val_loss: 1.6006 - val_accuracy: 0.5265\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 19s 357ms/step - loss: 1.4533 - accuracy: 0.5484 - val_loss: 1.4694 - val_accuracy: 0.5625\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 19s 357ms/step - loss: 1.3628 - accuracy: 0.5854 - val_loss: 1.4248 - val_accuracy: 0.5734\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.2974 - accuracy: 0.5985 - val_loss: 1.3169 - val_accuracy: 0.6137\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 1.2285 - accuracy: 0.6250 - val_loss: 1.2788 - val_accuracy: 0.6144\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 19s 358ms/step - loss: 1.1675 - accuracy: 0.6442 - val_loss: 1.2509 - val_accuracy: 0.6290\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.1061 - accuracy: 0.6613 - val_loss: 1.2098 - val_accuracy: 0.6406\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 19s 355ms/step - loss: 1.0713 - accuracy: 0.6685 - val_loss: 1.2237 - val_accuracy: 0.6322\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 19s 356ms/step - loss: 1.0397 - accuracy: 0.6774 - val_loss: 1.2762 - val_accuracy: 0.6334\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 17s 311ms/step - loss: 1.0036 - accuracy: 0.6916 - val_loss: 1.2740 - val_accuracy: 0.6264\n",
      "Epoch 1/30\n",
      "54/54 [==============================] - 60s 468ms/step - loss: 8.6953 - accuracy: 0.0375 - val_loss: 8.4761 - val_accuracy: 0.0387\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 21s 385ms/step - loss: 7.1219 - accuracy: 0.0967 - val_loss: 7.2628 - val_accuracy: 0.0438\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 4.0549 - accuracy: 0.1494 - val_loss: 5.3226 - val_accuracy: 0.0480\n",
      "Epoch 4/30\n",
      "54/54 [==============================] - 20s 375ms/step - loss: 2.9604 - accuracy: 0.2164 - val_loss: 3.2636 - val_accuracy: 0.1571\n",
      "Epoch 5/30\n",
      "54/54 [==============================] - 20s 374ms/step - loss: 2.5425 - accuracy: 0.2761 - val_loss: 2.4530 - val_accuracy: 0.2891\n",
      "Epoch 6/30\n",
      "54/54 [==============================] - 21s 395ms/step - loss: 2.3195 - accuracy: 0.3219 - val_loss: 2.2652 - val_accuracy: 0.3283\n",
      "Epoch 7/30\n",
      "54/54 [==============================] - 22s 401ms/step - loss: 2.0795 - accuracy: 0.3820 - val_loss: 2.0398 - val_accuracy: 0.3869\n",
      "Epoch 8/30\n",
      "54/54 [==============================] - 22s 404ms/step - loss: 1.9134 - accuracy: 0.4291 - val_loss: 1.9169 - val_accuracy: 0.4363\n",
      "Epoch 9/30\n",
      "54/54 [==============================] - 19s 348ms/step - loss: 1.7872 - accuracy: 0.4677 - val_loss: 1.7210 - val_accuracy: 0.4940\n",
      "Epoch 10/30\n",
      "54/54 [==============================] - 17s 319ms/step - loss: 1.6563 - accuracy: 0.4974 - val_loss: 1.6902 - val_accuracy: 0.4976\n",
      "Epoch 11/30\n",
      "54/54 [==============================] - 20s 366ms/step - loss: 1.5374 - accuracy: 0.5266 - val_loss: 1.5907 - val_accuracy: 0.5138\n",
      "Epoch 12/30\n",
      "54/54 [==============================] - 20s 365ms/step - loss: 1.4538 - accuracy: 0.5546 - val_loss: 1.5060 - val_accuracy: 0.5447\n",
      "Epoch 13/30\n",
      "54/54 [==============================] - 21s 383ms/step - loss: 1.3753 - accuracy: 0.5767 - val_loss: 1.3702 - val_accuracy: 0.5972\n",
      "Epoch 14/30\n",
      "54/54 [==============================] - 22s 401ms/step - loss: 1.2922 - accuracy: 0.6061 - val_loss: 1.3609 - val_accuracy: 0.6049\n",
      "Epoch 15/30\n",
      "54/54 [==============================] - 21s 381ms/step - loss: 1.2325 - accuracy: 0.6195 - val_loss: 1.3045 - val_accuracy: 0.6115\n",
      "Epoch 16/30\n",
      "54/54 [==============================] - 19s 346ms/step - loss: 1.1996 - accuracy: 0.6329 - val_loss: 1.3964 - val_accuracy: 0.5968\n",
      "Epoch 17/30\n",
      "54/54 [==============================] - 21s 391ms/step - loss: 1.1199 - accuracy: 0.6541 - val_loss: 1.2429 - val_accuracy: 0.6411\n",
      "Epoch 18/30\n",
      "54/54 [==============================] - 22s 403ms/step - loss: 1.1052 - accuracy: 0.6604 - val_loss: 1.2267 - val_accuracy: 0.6421\n",
      "Epoch 19/30\n",
      "54/54 [==============================] - 21s 392ms/step - loss: 1.0736 - accuracy: 0.6683 - val_loss: 1.2646 - val_accuracy: 0.6289\n",
      "Epoch 20/30\n",
      "54/54 [==============================] - 21s 389ms/step - loss: 1.0045 - accuracy: 0.6884 - val_loss: 1.2691 - val_accuracy: 0.6259\n",
      "Epoch 21/30\n",
      "54/54 [==============================] - 21s 391ms/step - loss: 0.9610 - accuracy: 0.7039 - val_loss: 1.1567 - val_accuracy: 0.6624\n",
      "Epoch 22/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.9318 - accuracy: 0.7178 - val_loss: 1.1774 - val_accuracy: 0.6606\n",
      "Epoch 23/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 0.9039 - accuracy: 0.7229 - val_loss: 1.1701 - val_accuracy: 0.6565\n",
      "Epoch 24/30\n",
      "54/54 [==============================] - 21s 387ms/step - loss: 0.8910 - accuracy: 0.7233 - val_loss: 1.1604 - val_accuracy: 0.6724\n",
      "Epoch 25/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.8436 - accuracy: 0.7397 - val_loss: 1.1659 - val_accuracy: 0.6712\n",
      "Epoch 26/30\n",
      "54/54 [==============================] - 21s 386ms/step - loss: 0.8314 - accuracy: 0.7384 - val_loss: 1.1607 - val_accuracy: 0.6723\n",
      "Epoch 27/30\n",
      "54/54 [==============================] - 21s 384ms/step - loss: 0.7693 - accuracy: 0.7582 - val_loss: 1.2005 - val_accuracy: 0.6664\n",
      "Epoch 28/30\n",
      "54/54 [==============================] - 21s 381ms/step - loss: 0.7611 - accuracy: 0.7663 - val_loss: 1.1441 - val_accuracy: 0.6858\n",
      "Epoch 29/30\n",
      "54/54 [==============================] - 21s 390ms/step - loss: 0.7576 - accuracy: 0.7650 - val_loss: 1.1158 - val_accuracy: 0.6884\n",
      "Epoch 30/30\n",
      "54/54 [==============================] - 22s 416ms/step - loss: 0.7219 - accuracy: 0.7756 - val_loss: 1.1015 - val_accuracy: 0.6939\n",
      "Epoch 1/10\n",
      "214/214 [==============================] - 59s 138ms/step - loss: 7.6407 - accuracy: 0.0272 - val_loss: 6.3628 - val_accuracy: 0.0371\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 25s 115ms/step - loss: 3.6563 - accuracy: 0.0416 - val_loss: 3.5027 - val_accuracy: 0.0385\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 23s 108ms/step - loss: 3.3790 - accuracy: 0.0584 - val_loss: 3.3863 - val_accuracy: 0.0519\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 23s 107ms/step - loss: 3.2447 - accuracy: 0.0840 - val_loss: 3.3434 - val_accuracy: 0.0581\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 3.1106 - accuracy: 0.1090 - val_loss: 3.2034 - val_accuracy: 0.0768\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.9721 - accuracy: 0.1407 - val_loss: 3.2022 - val_accuracy: 0.0590\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.7840 - accuracy: 0.1851 - val_loss: 3.1261 - val_accuracy: 0.0962\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 24s 111ms/step - loss: 2.6585 - accuracy: 0.2132 - val_loss: 3.2379 - val_accuracy: 0.0691\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.5643 - accuracy: 0.2345 - val_loss: 3.1529 - val_accuracy: 0.0709\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 24s 112ms/step - loss: 2.5075 - accuracy: 0.2571 - val_loss: 3.0531 - val_accuracy: 0.0928\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 59s 156ms/step - loss: 7.6124 - accuracy: 0.0290 - val_loss: 5.0708 - val_accuracy: 0.0397\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 3.6629 - accuracy: 0.0410 - val_loss: 3.5386 - val_accuracy: 0.0515\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 28s 132ms/step - loss: 3.4113 - accuracy: 0.0522 - val_loss: 3.4701 - val_accuracy: 0.0513\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 3.2679 - accuracy: 0.0718 - val_loss: 3.5072 - val_accuracy: 0.0468\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 30s 139ms/step - loss: 3.1596 - accuracy: 0.0853 - val_loss: 3.2331 - val_accuracy: 0.0650\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 29s 138ms/step - loss: 3.0465 - accuracy: 0.1089 - val_loss: 3.2744 - val_accuracy: 0.0799\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 2.8993 - accuracy: 0.1415 - val_loss: 2.9531 - val_accuracy: 0.1486\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.7607 - accuracy: 0.1827 - val_loss: 2.8858 - val_accuracy: 0.1306\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.6890 - accuracy: 0.1909 - val_loss: 2.8315 - val_accuracy: 0.1427\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.5833 - accuracy: 0.2206 - val_loss: 2.8545 - val_accuracy: 0.1296\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 28s 130ms/step - loss: 2.5153 - accuracy: 0.2402 - val_loss: 2.7437 - val_accuracy: 0.1652\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.4434 - accuracy: 0.2486 - val_loss: 2.7354 - val_accuracy: 0.1745\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.3876 - accuracy: 0.2673 - val_loss: 2.7026 - val_accuracy: 0.1723\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.2967 - accuracy: 0.2913 - val_loss: 2.6316 - val_accuracy: 0.1609\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.2610 - accuracy: 0.3045 - val_loss: 2.7259 - val_accuracy: 0.1489\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.1848 - accuracy: 0.3289 - val_loss: 2.6762 - val_accuracy: 0.1523\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 26s 124ms/step - loss: 2.1461 - accuracy: 0.3457 - val_loss: 2.6048 - val_accuracy: 0.1608\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 2.0719 - accuracy: 0.3611 - val_loss: 2.4964 - val_accuracy: 0.2048\n",
      "Epoch 19/20\n",
      "214/214 [==============================] - 26s 122ms/step - loss: 2.0546 - accuracy: 0.3747 - val_loss: 3.0179 - val_accuracy: 0.1102\n",
      "Epoch 20/20\n",
      "214/214 [==============================] - 26s 123ms/step - loss: 1.9842 - accuracy: 0.3922 - val_loss: 2.7053 - val_accuracy: 0.1486\n",
      "Epoch 1/30\n",
      "214/214 [==============================] - 55s 150ms/step - loss: 7.6077 - accuracy: 0.0255 - val_loss: 6.3048 - val_accuracy: 0.0355\n",
      "Epoch 2/30\n",
      "214/214 [==============================] - 27s 128ms/step - loss: 3.6763 - accuracy: 0.0407 - val_loss: 3.6138 - val_accuracy: 0.0387\n",
      "Epoch 3/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 3.4164 - accuracy: 0.0502 - val_loss: 3.5350 - val_accuracy: 0.0374\n",
      "Epoch 4/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 3.3103 - accuracy: 0.0654 - val_loss: 3.4398 - val_accuracy: 0.0560\n",
      "Epoch 5/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 3.1998 - accuracy: 0.0783 - val_loss: 3.8442 - val_accuracy: 0.0487\n",
      "Epoch 6/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 3.0635 - accuracy: 0.1129 - val_loss: 3.6193 - val_accuracy: 0.0472\n",
      "Epoch 7/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.9482 - accuracy: 0.1403 - val_loss: 3.0812 - val_accuracy: 0.1028\n",
      "Epoch 8/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 2.8086 - accuracy: 0.1684 - val_loss: 3.1497 - val_accuracy: 0.0777\n",
      "Epoch 9/30\n",
      "214/214 [==============================] - 29s 135ms/step - loss: 2.6743 - accuracy: 0.1968 - val_loss: 2.9683 - val_accuracy: 0.1062\n",
      "Epoch 10/30\n",
      "214/214 [==============================] - 29s 135ms/step - loss: 2.5646 - accuracy: 0.2237 - val_loss: 2.8905 - val_accuracy: 0.1272\n",
      "Epoch 11/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 2.4562 - accuracy: 0.2480 - val_loss: 3.0217 - val_accuracy: 0.0968\n",
      "Epoch 12/30\n",
      "214/214 [==============================] - 29s 134ms/step - loss: 2.3439 - accuracy: 0.2854 - val_loss: 2.8542 - val_accuracy: 0.1471\n",
      "Epoch 13/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.2958 - accuracy: 0.2964 - val_loss: 2.6205 - val_accuracy: 0.1862\n",
      "Epoch 14/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 2.2045 - accuracy: 0.3233 - val_loss: 3.0113 - val_accuracy: 0.1503\n",
      "Epoch 15/30\n",
      "214/214 [==============================] - 29s 136ms/step - loss: 2.1456 - accuracy: 0.3485 - val_loss: 3.0305 - val_accuracy: 0.1061\n",
      "Epoch 16/30\n",
      "214/214 [==============================] - 30s 138ms/step - loss: 2.0589 - accuracy: 0.3683 - val_loss: 2.6883 - val_accuracy: 0.1655\n",
      "Epoch 17/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 2.0002 - accuracy: 0.3842 - val_loss: 2.7958 - val_accuracy: 0.1575\n",
      "Epoch 18/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.9336 - accuracy: 0.4063 - val_loss: 2.6311 - val_accuracy: 0.1924\n",
      "Epoch 19/30\n",
      "214/214 [==============================] - 29s 137ms/step - loss: 1.9015 - accuracy: 0.4159 - val_loss: 2.4952 - val_accuracy: 0.2327\n",
      "Epoch 20/30\n",
      "214/214 [==============================] - 25s 117ms/step - loss: 1.8155 - accuracy: 0.4396 - val_loss: 2.5379 - val_accuracy: 0.2208\n",
      "Epoch 21/30\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 1.7616 - accuracy: 0.4581 - val_loss: 2.9012 - val_accuracy: 0.1289\n",
      "Epoch 22/30\n",
      "214/214 [==============================] - 30s 142ms/step - loss: 1.7308 - accuracy: 0.4701 - val_loss: 3.0173 - val_accuracy: 0.1495\n",
      "Epoch 23/30\n",
      "214/214 [==============================] - 30s 141ms/step - loss: 1.6903 - accuracy: 0.4882 - val_loss: 3.1102 - val_accuracy: 0.1530\n",
      "Epoch 24/30\n",
      "214/214 [==============================] - 31s 143ms/step - loss: 1.6401 - accuracy: 0.4948 - val_loss: 2.9288 - val_accuracy: 0.1246\n",
      "Epoch 25/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.6004 - accuracy: 0.5093 - val_loss: 2.6909 - val_accuracy: 0.2234\n",
      "Epoch 26/30\n",
      "214/214 [==============================] - 31s 144ms/step - loss: 1.5720 - accuracy: 0.5230 - val_loss: 3.0861 - val_accuracy: 0.0849\n",
      "Epoch 27/30\n",
      "214/214 [==============================] - 31s 143ms/step - loss: 1.5460 - accuracy: 0.5342 - val_loss: 3.0862 - val_accuracy: 0.0940\n",
      "Epoch 28/30\n",
      "214/214 [==============================] - 30s 140ms/step - loss: 1.5005 - accuracy: 0.5434 - val_loss: 3.1784 - val_accuracy: 0.0877\n",
      "Epoch 29/30\n",
      "214/214 [==============================] - 32s 148ms/step - loss: 1.4367 - accuracy: 0.5573 - val_loss: 3.2768 - val_accuracy: 0.1218\n",
      "Epoch 30/30\n",
      "214/214 [==============================] - 32s 149ms/step - loss: 1.4314 - accuracy: 0.5709 - val_loss: 3.4443 - val_accuracy: 0.0794\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 60s 260ms/step - loss: 8.5679 - accuracy: 0.0238 - val_loss: 7.9406 - val_accuracy: 0.0338\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 5.7353 - accuracy: 0.0360 - val_loss: 3.5704 - val_accuracy: 0.0277\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.6253 - accuracy: 0.0417 - val_loss: 3.4716 - val_accuracy: 0.0382\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.4319 - accuracy: 0.0481 - val_loss: 3.3987 - val_accuracy: 0.0488\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.3538 - accuracy: 0.0622 - val_loss: 3.3120 - val_accuracy: 0.0660\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.2870 - accuracy: 0.0746 - val_loss: 3.2615 - val_accuracy: 0.0743\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 24s 224ms/step - loss: 3.2132 - accuracy: 0.0920 - val_loss: 3.1983 - val_accuracy: 0.0872\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 24s 223ms/step - loss: 3.1077 - accuracy: 0.1124 - val_loss: 3.1568 - val_accuracy: 0.0933\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 24s 222ms/step - loss: 3.0130 - accuracy: 0.1320 - val_loss: 3.0376 - val_accuracy: 0.1149\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 24s 220ms/step - loss: 2.8656 - accuracy: 0.1642 - val_loss: 3.0000 - val_accuracy: 0.1111\n",
      "Epoch 1/20\n",
      "107/107 [==============================] - 58s 275ms/step - loss: 8.5766 - accuracy: 0.0290 - val_loss: 7.6757 - val_accuracy: 0.0363\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - 25s 231ms/step - loss: 5.7774 - accuracy: 0.0367 - val_loss: 3.5707 - val_accuracy: 0.0256\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - 23s 217ms/step - loss: 3.6383 - accuracy: 0.0401 - val_loss: 3.4301 - val_accuracy: 0.0381\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 3.4291 - accuracy: 0.0557 - val_loss: 3.4169 - val_accuracy: 0.0474\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 3.3058 - accuracy: 0.0753 - val_loss: 3.3118 - val_accuracy: 0.0759\n",
      "Epoch 6/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 3.1518 - accuracy: 0.1061 - val_loss: 3.2522 - val_accuracy: 0.0837\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - 24s 221ms/step - loss: 3.0188 - accuracy: 0.1339 - val_loss: 3.0768 - val_accuracy: 0.1096\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - 23s 218ms/step - loss: 2.9296 - accuracy: 0.1551 - val_loss: 3.0910 - val_accuracy: 0.0937\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - 23s 219ms/step - loss: 2.8200 - accuracy: 0.1723 - val_loss: 2.9121 - val_accuracy: 0.1447\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.6948 - accuracy: 0.2099 - val_loss: 2.8344 - val_accuracy: 0.1753\n",
      "Epoch 11/20\n",
      "107/107 [==============================] - 19s 178ms/step - loss: 2.5828 - accuracy: 0.2300 - val_loss: 2.9087 - val_accuracy: 0.1128\n",
      "Epoch 12/20\n",
      "107/107 [==============================] - 22s 205ms/step - loss: 2.4771 - accuracy: 0.2503 - val_loss: 2.8410 - val_accuracy: 0.1336\n",
      "Epoch 13/20\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 2.3989 - accuracy: 0.2711 - val_loss: 2.7500 - val_accuracy: 0.1662\n",
      "Epoch 14/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.3299 - accuracy: 0.2914 - val_loss: 2.7609 - val_accuracy: 0.1527\n",
      "Epoch 15/20\n",
      "107/107 [==============================] - 23s 216ms/step - loss: 2.2765 - accuracy: 0.3074 - val_loss: 2.7898 - val_accuracy: 0.1308\n",
      "Epoch 16/20\n",
      "107/107 [==============================] - 23s 214ms/step - loss: 2.1938 - accuracy: 0.3293 - val_loss: 2.6956 - val_accuracy: 0.1389\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.1271 - accuracy: 0.3539 - val_loss: 2.7525 - val_accuracy: 0.1464\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - 23s 212ms/step - loss: 2.0833 - accuracy: 0.3593 - val_loss: 2.6285 - val_accuracy: 0.1904\n",
      "Epoch 19/20\n",
      "107/107 [==============================] - 23s 213ms/step - loss: 1.9867 - accuracy: 0.3845 - val_loss: 2.8375 - val_accuracy: 0.1555\n",
      "Epoch 20/20\n",
      "107/107 [==============================] - 23s 215ms/step - loss: 1.9679 - accuracy: 0.3887 - val_loss: 2.7543 - val_accuracy: 0.1755\n",
      "Epoch 1/30\n",
      "107/107 [==============================] - 54s 254ms/step - loss: 8.5978 - accuracy: 0.0241 - val_loss: 7.8583 - val_accuracy: 0.0257\n",
      "Epoch 2/30\n",
      "107/107 [==============================] - 24s 221ms/step - loss: 5.8947 - accuracy: 0.0411 - val_loss: 3.5241 - val_accuracy: 0.0330\n",
      "Epoch 3/30\n",
      "107/107 [==============================] - 22s 209ms/step - loss: 3.6506 - accuracy: 0.0446 - val_loss: 3.4169 - val_accuracy: 0.0540\n",
      "Epoch 4/30\n",
      "107/107 [==============================] - 21s 199ms/step - loss: 3.4279 - accuracy: 0.0575 - val_loss: 3.3771 - val_accuracy: 0.0630\n",
      "Epoch 5/30\n",
      "107/107 [==============================] - 22s 203ms/step - loss: 3.3346 - accuracy: 0.0729 - val_loss: 3.3670 - val_accuracy: 0.0631\n",
      "Epoch 6/30\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 3.2260 - accuracy: 0.0928 - val_loss: 3.2759 - val_accuracy: 0.0688\n",
      "Epoch 7/30\n",
      "107/107 [==============================] - 22s 204ms/step - loss: 3.1317 - accuracy: 0.1043 - val_loss: 3.2186 - val_accuracy: 0.0761\n",
      "Epoch 8/30\n",
      "107/107 [==============================] - 22s 204ms/step - loss: 3.0668 - accuracy: 0.1151 - val_loss: 3.1435 - val_accuracy: 0.0828\n",
      "Epoch 9/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.9936 - accuracy: 0.1270 - val_loss: 3.1323 - val_accuracy: 0.0931\n",
      "Epoch 10/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.9417 - accuracy: 0.1364 - val_loss: 3.0135 - val_accuracy: 0.1121\n",
      "Epoch 11/30\n",
      "107/107 [==============================] - 22s 203ms/step - loss: 2.8699 - accuracy: 0.1548 - val_loss: 2.9865 - val_accuracy: 0.1306\n",
      "Epoch 12/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 2.7785 - accuracy: 0.1658 - val_loss: 2.9534 - val_accuracy: 0.1228\n",
      "Epoch 13/30\n",
      "107/107 [==============================] - 20s 186ms/step - loss: 2.6975 - accuracy: 0.1892 - val_loss: 2.8921 - val_accuracy: 0.1233\n",
      "Epoch 14/30\n",
      "107/107 [==============================] - 21s 192ms/step - loss: 2.6072 - accuracy: 0.2146 - val_loss: 2.8603 - val_accuracy: 0.1397\n",
      "Epoch 15/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.5243 - accuracy: 0.2449 - val_loss: 2.8537 - val_accuracy: 0.1358\n",
      "Epoch 16/30\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 2.4132 - accuracy: 0.2672 - val_loss: 2.8277 - val_accuracy: 0.1311\n",
      "Epoch 17/30\n",
      "107/107 [==============================] - 21s 194ms/step - loss: 2.3374 - accuracy: 0.2898 - val_loss: 2.7675 - val_accuracy: 0.1380\n",
      "Epoch 18/30\n",
      "107/107 [==============================] - 21s 195ms/step - loss: 2.2559 - accuracy: 0.3159 - val_loss: 2.6296 - val_accuracy: 0.1733\n",
      "Epoch 19/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.1869 - accuracy: 0.3349 - val_loss: 2.5757 - val_accuracy: 0.1737\n",
      "Epoch 20/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 2.1039 - accuracy: 0.3577 - val_loss: 2.5970 - val_accuracy: 0.1661\n",
      "Epoch 21/30\n",
      "107/107 [==============================] - 21s 196ms/step - loss: 2.0389 - accuracy: 0.3671 - val_loss: 2.5201 - val_accuracy: 0.2029\n",
      "Epoch 22/30\n",
      "107/107 [==============================] - 21s 195ms/step - loss: 1.9986 - accuracy: 0.3789 - val_loss: 2.4863 - val_accuracy: 0.2152\n",
      "Epoch 23/30\n",
      "107/107 [==============================] - 21s 197ms/step - loss: 1.9380 - accuracy: 0.3966 - val_loss: 2.4691 - val_accuracy: 0.2036\n",
      "Epoch 24/30\n",
      "107/107 [==============================] - 21s 200ms/step - loss: 1.8949 - accuracy: 0.4139 - val_loss: 2.3428 - val_accuracy: 0.2460\n",
      "Epoch 25/30\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 1.8391 - accuracy: 0.4272 - val_loss: 2.0932 - val_accuracy: 0.3358\n",
      "Epoch 26/30\n",
      "107/107 [==============================] - 22s 202ms/step - loss: 1.8000 - accuracy: 0.4404 - val_loss: 2.1410 - val_accuracy: 0.3249\n",
      "Epoch 27/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.7532 - accuracy: 0.4556 - val_loss: 2.3527 - val_accuracy: 0.2479\n",
      "Epoch 28/30\n",
      "107/107 [==============================] - 21s 198ms/step - loss: 1.6929 - accuracy: 0.4664 - val_loss: 2.0769 - val_accuracy: 0.3605\n",
      "Epoch 29/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.6613 - accuracy: 0.4800 - val_loss: 2.2904 - val_accuracy: 0.2745\n",
      "Epoch 30/30\n",
      "107/107 [==============================] - 21s 201ms/step - loss: 1.6418 - accuracy: 0.4931 - val_loss: 2.0305 - val_accuracy: 0.3628\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 56s 679ms/step - loss: 8.7600 - accuracy: 0.0196 - val_loss: 8.7080 - val_accuracy: 0.0387\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 34s 624ms/step - loss: 8.2317 - accuracy: 0.0337 - val_loss: 7.9319 - val_accuracy: 0.0356\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 33s 620ms/step - loss: 6.2632 - accuracy: 0.0364 - val_loss: 5.1785 - val_accuracy: 0.0387\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 32s 604ms/step - loss: 4.1740 - accuracy: 0.0410 - val_loss: 3.6850 - val_accuracy: 0.0409\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 34s 629ms/step - loss: 3.6179 - accuracy: 0.0394 - val_loss: 3.4795 - val_accuracy: 0.0506\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 34s 638ms/step - loss: 3.4891 - accuracy: 0.0483 - val_loss: 3.3871 - val_accuracy: 0.0494\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 3.4088 - accuracy: 0.0576 - val_loss: 3.3228 - val_accuracy: 0.0655\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 32s 589ms/step - loss: 3.3307 - accuracy: 0.0705 - val_loss: 3.2166 - val_accuracy: 0.0994\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 34s 636ms/step - loss: 3.2260 - accuracy: 0.0890 - val_loss: 3.1571 - val_accuracy: 0.1006\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 3.1493 - accuracy: 0.1072 - val_loss: 3.1063 - val_accuracy: 0.1036\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 60s 675ms/step - loss: 8.7690 - accuracy: 0.0184 - val_loss: 8.5618 - val_accuracy: 0.0352\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 33s 617ms/step - loss: 8.2444 - accuracy: 0.0296 - val_loss: 7.3850 - val_accuracy: 0.0387\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 33s 619ms/step - loss: 6.3000 - accuracy: 0.0370 - val_loss: 5.2844 - val_accuracy: 0.0384\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 33s 621ms/step - loss: 4.1813 - accuracy: 0.0348 - val_loss: 3.6658 - val_accuracy: 0.0353\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 33s 615ms/step - loss: 3.6137 - accuracy: 0.0433 - val_loss: 3.4962 - val_accuracy: 0.0356\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 28s 526ms/step - loss: 3.4708 - accuracy: 0.0522 - val_loss: 3.5655 - val_accuracy: 0.0327\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 34s 630ms/step - loss: 3.3718 - accuracy: 0.0663 - val_loss: 3.5656 - val_accuracy: 0.0418\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 33s 617ms/step - loss: 3.2711 - accuracy: 0.0895 - val_loss: 3.6005 - val_accuracy: 0.0418\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 34s 642ms/step - loss: 3.1712 - accuracy: 0.0999 - val_loss: 3.8518 - val_accuracy: 0.0287\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 33s 613ms/step - loss: 3.0689 - accuracy: 0.1113 - val_loss: 3.6705 - val_accuracy: 0.0472\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 35s 643ms/step - loss: 2.9936 - accuracy: 0.1244 - val_loss: 3.6851 - val_accuracy: 0.0459\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 33s 611ms/step - loss: 2.9058 - accuracy: 0.1331 - val_loss: 3.4318 - val_accuracy: 0.0638\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 31s 569ms/step - loss: 2.8390 - accuracy: 0.1541 - val_loss: 3.3976 - val_accuracy: 0.0616\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 33s 607ms/step - loss: 2.7687 - accuracy: 0.1697 - val_loss: 3.1365 - val_accuracy: 0.0908\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 33s 622ms/step - loss: 2.7066 - accuracy: 0.1860 - val_loss: 3.2357 - val_accuracy: 0.0756\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 34s 623ms/step - loss: 2.6630 - accuracy: 0.1912 - val_loss: 3.1489 - val_accuracy: 0.0840\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 33s 618ms/step - loss: 2.5956 - accuracy: 0.2132 - val_loss: 3.0764 - val_accuracy: 0.0977\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 33s 611ms/step - loss: 2.5549 - accuracy: 0.2183 - val_loss: 3.1855 - val_accuracy: 0.0880\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 34s 640ms/step - loss: 2.4995 - accuracy: 0.2323 - val_loss: 3.0971 - val_accuracy: 0.0914\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 2.4646 - accuracy: 0.2473 - val_loss: 3.1707 - val_accuracy: 0.0969\n",
      "Epoch 1/30\n",
      "54/54 [==============================] - 60s 667ms/step - loss: 8.7653 - accuracy: 0.0183 - val_loss: 8.6092 - val_accuracy: 0.0278\n",
      "Epoch 2/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 8.2575 - accuracy: 0.0348 - val_loss: 7.3404 - val_accuracy: 0.0338\n",
      "Epoch 3/30\n",
      "54/54 [==============================] - 35s 650ms/step - loss: 6.3265 - accuracy: 0.0391 - val_loss: 5.5368 - val_accuracy: 0.0338\n",
      "Epoch 4/30\n",
      "54/54 [==============================] - 35s 647ms/step - loss: 4.2024 - accuracy: 0.0439 - val_loss: 3.5305 - val_accuracy: 0.0338\n",
      "Epoch 5/30\n",
      "54/54 [==============================] - 35s 649ms/step - loss: 3.6229 - accuracy: 0.0435 - val_loss: 3.4494 - val_accuracy: 0.0402\n",
      "Epoch 6/30\n",
      "54/54 [==============================] - 37s 679ms/step - loss: 3.4686 - accuracy: 0.0495 - val_loss: 3.3984 - val_accuracy: 0.0516\n",
      "Epoch 7/30\n",
      "54/54 [==============================] - 34s 635ms/step - loss: 3.3959 - accuracy: 0.0598 - val_loss: 3.3474 - val_accuracy: 0.0563\n",
      "Epoch 8/30\n",
      "54/54 [==============================] - 32s 587ms/step - loss: 3.3273 - accuracy: 0.0701 - val_loss: 3.3237 - val_accuracy: 0.0566\n",
      "Epoch 9/30\n",
      "54/54 [==============================] - 35s 644ms/step - loss: 3.2655 - accuracy: 0.0840 - val_loss: 3.2341 - val_accuracy: 0.0744\n",
      "Epoch 10/30\n",
      "54/54 [==============================] - 34s 640ms/step - loss: 3.2030 - accuracy: 0.0989 - val_loss: 3.1895 - val_accuracy: 0.0796\n",
      "Epoch 11/30\n",
      "54/54 [==============================] - 35s 643ms/step - loss: 3.1375 - accuracy: 0.1075 - val_loss: 3.0922 - val_accuracy: 0.0805\n",
      "Epoch 12/30\n",
      "54/54 [==============================] - 34s 633ms/step - loss: 3.0388 - accuracy: 0.1257 - val_loss: 3.1110 - val_accuracy: 0.0759\n",
      "Epoch 13/30\n",
      "54/54 [==============================] - 34s 631ms/step - loss: 2.9448 - accuracy: 0.1378 - val_loss: 3.0002 - val_accuracy: 0.1106\n",
      "Epoch 14/30\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 2.8690 - accuracy: 0.1586 - val_loss: 2.9983 - val_accuracy: 0.1037\n",
      "Epoch 15/30\n",
      "54/54 [==============================] - 35s 641ms/step - loss: 2.8047 - accuracy: 0.1697 - val_loss: 2.9431 - val_accuracy: 0.1256\n",
      "Epoch 16/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 2.7499 - accuracy: 0.1848 - val_loss: 2.8707 - val_accuracy: 0.1450\n",
      "Epoch 17/30\n",
      "54/54 [==============================] - 34s 637ms/step - loss: 2.6594 - accuracy: 0.1996 - val_loss: 2.8426 - val_accuracy: 0.1531\n",
      "Epoch 18/30\n",
      "54/54 [==============================] - 28s 524ms/step - loss: 2.6205 - accuracy: 0.2155 - val_loss: 2.7507 - val_accuracy: 0.1615\n",
      "Epoch 19/30\n",
      "54/54 [==============================] - 34s 641ms/step - loss: 2.5543 - accuracy: 0.2257 - val_loss: 2.8442 - val_accuracy: 0.1328\n",
      "Epoch 20/30\n",
      "54/54 [==============================] - 35s 648ms/step - loss: 2.4846 - accuracy: 0.2385 - val_loss: 2.8307 - val_accuracy: 0.1331\n",
      "Epoch 21/30\n",
      "54/54 [==============================] - 35s 653ms/step - loss: 2.4241 - accuracy: 0.2691 - val_loss: 2.8229 - val_accuracy: 0.1133\n",
      "Epoch 22/30\n",
      "54/54 [==============================] - 35s 642ms/step - loss: 2.3801 - accuracy: 0.2755 - val_loss: 2.5698 - val_accuracy: 0.1793\n",
      "Epoch 23/30\n",
      "54/54 [==============================] - 33s 605ms/step - loss: 2.3037 - accuracy: 0.2955 - val_loss: 2.6911 - val_accuracy: 0.1671\n",
      "Epoch 24/30\n",
      "54/54 [==============================] - 35s 648ms/step - loss: 2.2371 - accuracy: 0.3122 - val_loss: 2.6304 - val_accuracy: 0.1808\n",
      "Epoch 25/30\n",
      "54/54 [==============================] - 35s 656ms/step - loss: 2.1728 - accuracy: 0.3282 - val_loss: 2.6417 - val_accuracy: 0.1883\n",
      "Epoch 26/30\n",
      "54/54 [==============================] - 35s 646ms/step - loss: 2.1090 - accuracy: 0.3538 - val_loss: 2.4893 - val_accuracy: 0.2286\n",
      "Epoch 27/30\n",
      "54/54 [==============================] - 35s 651ms/step - loss: 2.0695 - accuracy: 0.3715 - val_loss: 2.5734 - val_accuracy: 0.1898\n",
      "Epoch 28/30\n",
      "54/54 [==============================] - 33s 621ms/step - loss: 2.0286 - accuracy: 0.3823 - val_loss: 2.4504 - val_accuracy: 0.2439\n",
      "Epoch 29/30\n",
      "54/54 [==============================] - 36s 671ms/step - loss: 1.9488 - accuracy: 0.3937 - val_loss: 2.4381 - val_accuracy: 0.1945\n",
      "Epoch 30/30\n",
      "54/54 [==============================] - 36s 672ms/step - loss: 1.9157 - accuracy: 0.4180 - val_loss: 2.4646 - val_accuracy: 0.2257\n",
      "Epoch 1/10\n",
      "214/214 [==============================] - 76s 195ms/step - loss: 7.9708 - accuracy: 0.0263 - val_loss: 4.4891 - val_accuracy: 0.0452\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 42s 199ms/step - loss: 4.0390 - accuracy: 0.0350 - val_loss: 3.4266 - val_accuracy: 0.0350\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 43s 201ms/step - loss: 3.5220 - accuracy: 0.0394 - val_loss: 3.4100 - val_accuracy: 0.0366\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 40s 186ms/step - loss: 3.4797 - accuracy: 0.0358 - val_loss: 3.4283 - val_accuracy: 0.0332\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4468 - accuracy: 0.0364 - val_loss: 3.4066 - val_accuracy: 0.0374\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 41s 192ms/step - loss: 3.4451 - accuracy: 0.0307 - val_loss: 3.4160 - val_accuracy: 0.0382\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 3.4344 - accuracy: 0.0360 - val_loss: 3.4314 - val_accuracy: 0.0382\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 43s 202ms/step - loss: 3.4365 - accuracy: 0.0364 - val_loss: 3.4169 - val_accuracy: 0.0371\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 42s 194ms/step - loss: 3.4355 - accuracy: 0.0356 - val_loss: 3.4178 - val_accuracy: 0.0396\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4233 - accuracy: 0.0361 - val_loss: 3.4170 - val_accuracy: 0.0359\n",
      "Epoch 1/20\n",
      "214/214 [==============================] - 76s 228ms/step - loss: 7.9263 - accuracy: 0.0271 - val_loss: 5.1164 - val_accuracy: 0.0382\n",
      "Epoch 2/20\n",
      "214/214 [==============================] - 44s 207ms/step - loss: 4.0093 - accuracy: 0.0372 - val_loss: 3.4348 - val_accuracy: 0.0363\n",
      "Epoch 3/20\n",
      "214/214 [==============================] - 42s 197ms/step - loss: 3.5248 - accuracy: 0.0361 - val_loss: 3.4568 - val_accuracy: 0.0343\n",
      "Epoch 4/20\n",
      "214/214 [==============================] - 39s 182ms/step - loss: 3.4642 - accuracy: 0.0357 - val_loss: 3.4502 - val_accuracy: 0.0418\n",
      "Epoch 5/20\n",
      "214/214 [==============================] - 39s 183ms/step - loss: 3.4487 - accuracy: 0.0414 - val_loss: 3.4344 - val_accuracy: 0.0382\n",
      "Epoch 6/20\n",
      "214/214 [==============================] - 40s 188ms/step - loss: 3.4396 - accuracy: 0.0420 - val_loss: 3.4585 - val_accuracy: 0.0349\n",
      "Epoch 7/20\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 3.4359 - accuracy: 0.0392 - val_loss: 3.4653 - val_accuracy: 0.0382\n",
      "Epoch 8/20\n",
      "214/214 [==============================] - 42s 196ms/step - loss: 3.4370 - accuracy: 0.0332 - val_loss: 3.4354 - val_accuracy: 0.0382\n",
      "Epoch 9/20\n",
      "214/214 [==============================] - 41s 191ms/step - loss: 3.4342 - accuracy: 0.0395 - val_loss: 3.4204 - val_accuracy: 0.0478\n",
      "Epoch 10/20\n",
      "214/214 [==============================] - 38s 178ms/step - loss: 3.4341 - accuracy: 0.0342 - val_loss: 3.4171 - val_accuracy: 0.0381\n",
      "Epoch 11/20\n",
      "214/214 [==============================] - 41s 190ms/step - loss: 3.4279 - accuracy: 0.0385 - val_loss: 3.4241 - val_accuracy: 0.0382\n",
      "Epoch 12/20\n",
      "214/214 [==============================] - 40s 188ms/step - loss: 3.4282 - accuracy: 0.0364 - val_loss: 3.4352 - val_accuracy: 0.0338\n",
      "Epoch 13/20\n",
      "214/214 [==============================] - 39s 184ms/step - loss: 3.4234 - accuracy: 0.0350 - val_loss: 3.4437 - val_accuracy: 0.0338\n",
      "Epoch 14/20\n",
      "214/214 [==============================] - 39s 181ms/step - loss: 3.4169 - accuracy: 0.0392 - val_loss: 3.4327 - val_accuracy: 0.0382\n",
      "Epoch 15/20\n",
      "214/214 [==============================] - 39s 182ms/step - loss: 3.4260 - accuracy: 0.0375 - val_loss: 3.4605 - val_accuracy: 0.0382\n",
      "Epoch 16/20\n",
      "214/214 [==============================] - 40s 185ms/step - loss: 3.4185 - accuracy: 0.0399 - val_loss: 3.4297 - val_accuracy: 0.0359\n",
      "Epoch 17/20\n",
      "214/214 [==============================] - 41s 193ms/step - loss: 3.4229 - accuracy: 0.0358 - val_loss: 3.4102 - val_accuracy: 0.0382\n",
      "Epoch 18/20\n",
      "214/214 [==============================] - 40s 185ms/step - loss: 3.4082 - accuracy: 0.0380 - val_loss: 3.4431 - val_accuracy: 0.0382\n",
      "Epoch 19/20\n",
      "213/214 [============================>.] - ETA: 0s - loss: 3.4122 - accuracy: 0.0359"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D\n",
    "\n",
    "results = pd.DataFrame(columns=['lstm_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "for gru_units in [64, 128, 256]:\n",
    "    for dropout_rate in [0.2, 0.4, 0.6]:\n",
    "        for batch_size in [32, 64, 128]:\n",
    "            for epochs in [10, 20, 30]:\n",
    "                num_classes = len(y_train_encoded)  # Number of unique classes in your dataset\n",
    "                input_shape = (39, 44)\n",
    "\n",
    "                model = Sequential([\n",
    "                    Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling1D(pool_size=2),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Bidirectional(GRU(gru_units, return_sequences=True)),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GRU(gru_units, return_sequences=True),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GRU(gru_units, return_sequences=True),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(256, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(128, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    TimeDistributed(Dense(64, activation='relu')),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    GlobalMaxPooling1D(),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(256, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(128, activation='relu'),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(dropout_rate),\n",
    "                    Dense(num_classes, activation='softmax')\n",
    "                ])\n",
    "\n",
    "                # Compile the model\n",
    "                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                # Train the model with your training set and validate it with your validation set\n",
    "                epochs = epochs\n",
    "                batch_size = batch_size\n",
    "                history = model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val_encoded))\n",
    "\n",
    "                results = np.concatenate((results, pd.DataFrame([[gru_units, dropout_rate, epochs, batch_size, history.history['loss'][-1], history.history['loss'], history.history['accuracy'][-1], history.history['accuracy'], history.history['val_loss'][-1], history.history['val_loss'], history.history['val_accuracy'][-1], history.history['val_accuracy']]], \n",
    "                                                                    columns=['lstm_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 37, 128)           17024     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 37, 128)          512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 18, 128)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 18, 128)           0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 18, 256)          198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 18, 256)          296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 18, 128)           148224    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 18, 128)           99072     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 18, 256)          33024     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 18, 256)          1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 18, 256)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 18, 128)          32896     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 18, 128)          512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 18, 128)           0         \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 18, 64)           8256      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 18, 64)           256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 18, 64)            0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               33280     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 512)              2048      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,040,842\n",
      "Trainable params: 1,036,362\n",
      "Non-trainable params: 4,480\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "opt = Adam(lr=0.0001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results\\\\model_gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
