{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import pickle\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pydub import AudioSegment\n",
    "from tqdm import tqdm\n",
    "\n",
    "from preprocessing import perform_vad, resample_wav, padding, cut_wav_into_clips, extract_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_dir = \"train\\\\audio\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get full training list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\validation_list.txt'\n",
    "output_labels_path = 'train\\\\training_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_labels_path, \"w\") as f1:\n",
    "    for folder in os.listdir(wav_dir):\n",
    "        if folder == '_background_noise_' or folder == 'silence':\n",
    "            continue\n",
    "        else:\n",
    "            for file in os.listdir(os.path.join(wav_dir, folder)):\n",
    "                path = f\"{folder}/{file}\"\n",
    "                if path not in file_paths:\n",
    "                    f1.write(path + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(output_labels_path', \"w\") as f1:\n",
    "    for file in part1:\n",
    "        f1.write(\"silence/\" + file + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `unknown` detection / `label` classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\training_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths_train = [line.strip() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57923/57923 [1:12:09<00:00, 13.38it/s]\n"
     ]
    }
   ],
   "source": [
    "features_train = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_train), total=len(file_paths_train), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  \n",
    "    perform_vad(wav_file, wav_file2)\n",
    "    padding(wav_file2, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\features_training.pkl', 'wb') as f:\n",
    "    pickle.dump(features_train, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'train\\\\validation_list.txt'\n",
    "with open(labels_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "file_paths_val = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 69/6798 [00:04<06:36, 16.96it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Preprocess the data  \u001b[39;00m\n\u001b[0;32m      9\u001b[0m perform_vad(wav_file, wav_file2)\n\u001b[1;32m---> 10\u001b[0m padding(wav_file2, wav_file2, \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m resample_wav(wav_file2, wav_file2, \u001b[39m8000\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Extract features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\preprocessing.py:180\u001b[0m, in \u001b[0;36mpadding\u001b[1;34m(wav_file_path, new_path, pad_ms)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpadding\u001b[39m(wav_file_path, new_path, pad_ms\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m    Pads a wav file with silence.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m    pad_ms: clip duration in milliseconds after padding\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     audio \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_wav(wav_file_path)\n\u001b[0;32m    181\u001b[0m     silence \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39msilent(duration\u001b[39m=\u001b[39mpad_ms\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(audio)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    184\u001b[0m     padded \u001b[39m=\u001b[39m audio \u001b[39m+\u001b[39m silence\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\audio_segment.py:808\u001b[0m, in \u001b[0;36mAudioSegment.from_wav\u001b[1;34m(cls, file, parameters)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_wav\u001b[39m(\u001b[39mcls\u001b[39m, file, parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 808\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_file(file, \u001b[39m'\u001b[39;49m\u001b[39mwav\u001b[39;49m\u001b[39m'\u001b[39;49m, parameters\u001b[39m=\u001b[39;49mparameters)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_val = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_val), total=len(file_paths_val), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1],file)\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  \n",
    "    perform_vad(wav_file, wav_file2)\n",
    "    padding(wav_file2, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\features_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(features_val, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `silence` detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silence clips list creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"train\\\\audio\\\\_background_noise_\"\n",
    "output_folder = \"train\\\\audio\\\\silence\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        cut_wav_into_clips(f\"{input_folder}\\\\{filename}\", output_folder, filename, clip_duration_ms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"train\\\\audio\\\\silence\"\n",
    "\n",
    "file_list = os.listdir(dir_path)\n",
    "\n",
    "random.shuffle(file_list)\n",
    "\n",
    "# Split the list into two parts\n",
    "split_index = int(len(file_list) * 0.8)\n",
    "part1 = file_list[:split_index]\n",
    "part2 = file_list[split_index:]\n",
    "\n",
    "# Define the output file paths\n",
    "file1_path = \"train\\\\silence_testing_list.txt\"\n",
    "file2_path = \"train\\\\silence_validation_list.txt\"\n",
    "\n",
    "# Write the paths to the files to the output text files\n",
    "with open(file1_path, \"w\") as f1:\n",
    "    for file in part1:\n",
    "        f1.write(\"silence/\" + file + \"\\n\")\n",
    "\n",
    "with open(file2_path, \"w\") as f2:\n",
    "    for file in part2:\n",
    "        f2.write(\"silence/\" + file + \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample silence to 3000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train\\\\silence_training_list.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "file_paths_train_silence = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths_train_silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m audio_pitch \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39meffects\u001b[39m.\u001b[39mpitch_shift(audio_noise, sr\u001b[39m=\u001b[39msr, n_steps\u001b[39m=\u001b[39mpitch_shift\u001b[39m/\u001b[39m\u001b[39m100.0\u001b[39m)\n\u001b[0;32m     24\u001b[0m new_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maugmented\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m.wav\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 25\u001b[0m wavfile\u001b[39m.\u001b[39;49mwrite(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, new_filename), sr, audio_pitch\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32))\n\u001b[0;32m     27\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(text_file_dir, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     28\u001b[0m     f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39msilence/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m new_filename \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\scipy\\io\\wavfile.py:822\u001b[0m, in \u001b[0;36mwrite\u001b[1;34m(filename, rate, data)\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbyteorder \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m (data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbyteorder \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    820\u001b[0m                                    sys\u001b[39m.\u001b[39mbyteorder \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbig\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    821\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mbyteswap()\n\u001b[1;32m--> 822\u001b[0m _array_tofile(fid, data)\n\u001b[0;32m    824\u001b[0m \u001b[39m# Determine file size and place it in correct\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \u001b[39m#  position at start of the file.\u001b[39;00m\n\u001b[0;32m    826\u001b[0m size \u001b[39m=\u001b[39m fid\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\scipy\\io\\wavfile.py:839\u001b[0m, in \u001b[0;36m_array_tofile\u001b[1;34m(fid, data)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_array_tofile\u001b[39m(fid, data):\n\u001b[0;32m    838\u001b[0m     \u001b[39m# ravel gives a c-contiguous buffer\u001b[39;00m\n\u001b[1;32m--> 839\u001b[0m     fid\u001b[39m.\u001b[39;49mwrite(data\u001b[39m.\u001b[39;49mravel()\u001b[39m.\u001b[39;49mview(\u001b[39m'\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mdata)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "input_dir = \"train\\\\audio\\\\silence\"\n",
    "output_dir = \"train\\\\audio\\\\silence\"\n",
    "text_file_dir = \"train\\\\silence_training_list.txt\"\n",
    "\n",
    "target_num_clips = 3000\n",
    "k = 0\n",
    "n = len(file_paths_train_silence)\n",
    "\n",
    "while k + n < target_num_clips:\n",
    "    \n",
    "    for i, file in enumerate(file_paths_train_silence):\n",
    "        file_path = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "        \n",
    "        if k + n >= target_num_clips:\n",
    "            break\n",
    "\n",
    "\n",
    "        audio, sr = librosa.load(file_path, sr=16000)\n",
    "        noise = np.random.randn(len(audio))\n",
    "        noise_level = 0.1\n",
    "        audio_noise = audio + noise_level * noise\n",
    "        pitch_shift = np.random.uniform(-100, 100)\n",
    "        audio_pitch = librosa.effects.pitch_shift(audio_noise, sr=sr, n_steps=pitch_shift/100.0)\n",
    "        new_filename = f\"augmented{k}.wav\"\n",
    "        wavfile.write(os.path.join(output_dir, new_filename), sr, audio_pitch.astype(np.float32))\n",
    "\n",
    "        with open(text_file_dir, \"a\") as f:\n",
    "            f.write(\"silence/\" + new_filename + \"\\n\")\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1945"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(input_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#open pickle\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mextracted_features\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39msilence_detection_validation.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m     \u001b[39mlen\u001b[39m(x)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "#open pickle\n",
    "with open('extracted_features\\\\silence_detection_validation.pkl', 'rb') as f:\n",
    "    x = pickle.load(f)\n",
    "    len(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train\\\\silence_training_list.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "file_paths_train_silence = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1863"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths_train_silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 319/1863 [00:15<00:57, 27.00it/s]c:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      " 17%|█▋        | 321/1863 [00:15<01:14, 20.72it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Nie można odnaleźć określonego pliku",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m label \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39m# Preprocess the data  (without VAD)\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m padding(wav_file, wav_file2, \u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     10\u001b[0m resample_wav(wav_file2, wav_file2, \u001b[39m8000\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Extract features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\preprocessing.py:180\u001b[0m, in \u001b[0;36mpadding\u001b[1;34m(wav_file_path, new_path, pad_ms)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpadding\u001b[39m(wav_file_path, new_path, pad_ms\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m    Pads a wav file with silence.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m    Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39m    pad_ms: clip duration in milliseconds after padding\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     audio \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39;49mfrom_wav(wav_file_path)\n\u001b[0;32m    181\u001b[0m     silence \u001b[39m=\u001b[39m AudioSegment\u001b[39m.\u001b[39msilent(duration\u001b[39m=\u001b[39mpad_ms\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(audio)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    184\u001b[0m     padded \u001b[39m=\u001b[39m audio \u001b[39m+\u001b[39m silence\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\audio_segment.py:808\u001b[0m, in \u001b[0;36mAudioSegment.from_wav\u001b[1;34m(cls, file, parameters)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_wav\u001b[39m(\u001b[39mcls\u001b[39m, file, parameters\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 808\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrom_file(file, \u001b[39m'\u001b[39;49m\u001b[39mwav\u001b[39;49m\u001b[39m'\u001b[39;49m, parameters\u001b[39m=\u001b[39;49mparameters)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m     info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 728\u001b[0m     info \u001b[39m=\u001b[39m mediainfo_json(orig_file, read_ahead_limit\u001b[39m=\u001b[39;49mread_ahead_limit)\n\u001b[0;32m    729\u001b[0m \u001b[39mif\u001b[39;00m info:\n\u001b[0;32m    730\u001b[0m     audio_streams \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m info[\u001b[39m'\u001b[39m\u001b[39mstreams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    731\u001b[0m                      \u001b[39mif\u001b[39;00m x[\u001b[39m'\u001b[39m\u001b[39mcodec_type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\pydub\\utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[1;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[0;32m    271\u001b[0m         file\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    273\u001b[0m command \u001b[39m=\u001b[39m [prober, \u001b[39m'\u001b[39m\u001b[39m-of\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mjson\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m command_args\n\u001b[1;32m--> 274\u001b[0m res \u001b[39m=\u001b[39m Popen(command, stdin\u001b[39m=\u001b[39;49mstdin_parameter, stdout\u001b[39m=\u001b[39;49mPIPE, stderr\u001b[39m=\u001b[39;49mPIPE)\n\u001b[0;32m    275\u001b[0m output, stderr \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m\u001b[39m=\u001b[39mstdin_data)\n\u001b[0;32m    276\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:969\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    965\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[0;32m    966\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[0;32m    967\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m--> 969\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0;32m    970\u001b[0m                         pass_fds, cwd, env,\n\u001b[0;32m    971\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[0;32m    972\u001b[0m                         p2cread, p2cwrite,\n\u001b[0;32m    973\u001b[0m                         c2pread, c2pwrite,\n\u001b[0;32m    974\u001b[0m                         errread, errwrite,\n\u001b[0;32m    975\u001b[0m                         restore_signals,\n\u001b[0;32m    976\u001b[0m                         gid, gids, uid, umask,\n\u001b[0;32m    977\u001b[0m                         start_new_session)\n\u001b[0;32m    978\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    980\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\subprocess.py:1438\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1436\u001b[0m \u001b[39m# Start the process\u001b[39;00m\n\u001b[0;32m   1437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1438\u001b[0m     hp, ht, pid, tid \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mCreateProcess(executable, args,\n\u001b[0;32m   1439\u001b[0m                              \u001b[39m# no special security\u001b[39;49;00m\n\u001b[0;32m   1440\u001b[0m                              \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   1441\u001b[0m                              \u001b[39mint\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m close_fds),\n\u001b[0;32m   1442\u001b[0m                              creationflags,\n\u001b[0;32m   1443\u001b[0m                              env,\n\u001b[0;32m   1444\u001b[0m                              cwd,\n\u001b[0;32m   1445\u001b[0m                              startupinfo)\n\u001b[0;32m   1446\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1447\u001b[0m     \u001b[39m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m     \u001b[39m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1451\u001b[0m     \u001b[39m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1452\u001b[0m     \u001b[39m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1453\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1454\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1455\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Nie można odnaleźć określonego pliku"
     ]
    }
   ],
   "source": [
    "features_train_silence = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_train_silence), total=len(file_paths_train_silence), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  (without VAD)\n",
    "    padding(wav_file, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_train_silence.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\silence_detection_training.pkl', 'wb') as f:\n",
    "    pickle.dump(features_train + features_train_silence, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train\\\\silence_validation_list.txt', 'r') as file:\n",
    "    lines = file.readlines() + file.readlines()\n",
    "file_paths_val_silence = [line.strip() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:03<00:00, 21.96it/s]\n"
     ]
    }
   ],
   "source": [
    "features_val_silence = []\n",
    "\n",
    "for i, file in tqdm(enumerate(file_paths_val_silence), total=len(file_paths_val_silence), leave=True):\n",
    "    wav_file = os.path.join(wav_dir,file.split('/')[0],file.split('/')[1])\n",
    "    wav_file2 = 'working_sample.wav'\n",
    "    label = file.split('/')[0]\n",
    "\n",
    "    # Preprocess the data  (without VAD)\n",
    "    padding(wav_file, wav_file2, 1000)\n",
    "    resample_wav(wav_file2, wav_file2, 8000)\n",
    "\n",
    "    # Extract features\n",
    "    features = extract_features(wav_file2)\n",
    "\n",
    "    # Add to the list\n",
    "    features_val_silence.append([features, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('extracted_features\\\\silence_detection_validation.pkl', 'wb') as f:\n",
    "    pickle.dump(features_val + features_val_silence, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
