{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import label_detection_training, label_detection_validation, silence_detection_training, silence_detection_validation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from dataset import TensorflowDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "# val = pd.read_pickle('extracted_features\\\\features_validation.pkl')\n",
    "\n",
    "# X_train = np.array([x[0] for x in train])\n",
    "# y_train = np.array([x[1] for x in train])\n",
    "# X_val = np.array([x[0] for x in val])\n",
    "# y_val = np.array([x[1] for x in val])\n",
    "\n",
    "# # Create a label encoder object\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# # Fit the label encoder using your labels (combine both train and val labels if they have different classes)\n",
    "# label_encoder.fit(np.concatenate((y_train, y_val)))\n",
    "\n",
    "# # Transform your string labels to integer labels for both training and validation sets\n",
    "# y_train_encoded = label_encoder.transform(y_train)\n",
    "# y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "y_train = np.array([x[1] for x in train])\n",
    "labels = list(np.unique(y_train))\n",
    "train = TensorflowDataset('extracted_features\\\\features_training.pkl', labels=labels).dataset\n",
    "train = train.shuffle(len(train), reshuffle_each_iteration=True)\n",
    "val = TensorflowDataset('extracted_features\\\\features_validation.pkl', labels=labels).dataset\n",
    "val = val.shuffle(len(val), reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 51s - loss: 2.1170 - accuracy: 0.4033 - val_loss: 1.3594 - val_accuracy: 0.5927 - 51s/epoch - 28ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 53s - loss: 1.3355 - accuracy: 0.6051 - val_loss: 0.9565 - val_accuracy: 0.7123 - 53s/epoch - 29ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 56s - loss: 1.1821 - accuracy: 0.6485 - val_loss: 0.8913 - val_accuracy: 0.7243 - 56s/epoch - 31ms/step\n",
      "Epoch 4/10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (MultiHeadAttention, Dropout, LayerNormalization, Dense, TimeDistributed,\n",
    "                                     BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "results = pd.DataFrame(columns=['num_heads', 'num_layers', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(embed_dim, activation='relu'),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "for num_heads in [2, 4]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for dropout_rate in [0.2, 0.4]:\n",
    "            for batch_size in [32, 64]:\n",
    "                for epochs in [10, 20]:\n",
    "                    num_classes = len(np.unique(labels))  # Number of unique classes in your dataset\n",
    "                    input_shape = (39, 44)\n",
    "                    embed_dim = 128\n",
    "\n",
    "                    inputs = tf.keras.Input(shape=input_shape)\n",
    "                    x = Conv1D(filters=128, kernel_size=3, activation='relu')(inputs)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = MaxPooling1D(pool_size=2)(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "                    for _ in range(num_layers):\n",
    "                        x = TransformerBlock(embed_dim, num_heads, dropout_rate)(x)\n",
    "\n",
    "                    x = GlobalMaxPooling1D()(x)\n",
    "                    x = Dense(512, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(256, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(128, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "                    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "                    # Compile the model\n",
    "                    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                    # Train the model with your training set and validate it with your validation set\n",
    "                    history = model.fit(train.batch(batch_size), epochs=epochs, batch_size=batch_size, validation_data=val.batch(batch_size), verbose=2)\n",
    "                                    # Record the results for the current configuration\n",
    "                    row = {\n",
    "                        'num_heads': num_heads,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'epoch': epochs,\n",
    "                        'batch': batch_size,\n",
    "                        'loss': history.history['loss'][-1],\n",
    "                        'loss_max': np.max(history.history['loss']),\n",
    "                        'accuracy': history.history['accuracy'][-1],\n",
    "                        'accuracy_max': np.max(history.history['accuracy']),\n",
    "                        'val_loss': history.history['val_loss'][-1],\n",
    "                        'val_loss_max': np.max(history.history['val_loss']),\n",
    "                        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "                        'val_accuracy_max': np.max(history.history['val_accuracy']),\n",
    "                    }\n",
    "\n",
    "                    results = results.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results\\\\model_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
