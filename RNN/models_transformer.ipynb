{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import labels_only_detection_training, labels_only_detection_validation, labels_only_detection_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.6999 - accuracy: 0.4385\n",
      "Epoch 1: val_accuracy improved from -inf to 0.67210, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 114s 161ms/step - loss: 1.6999 - accuracy: 0.4385 - val_loss: 0.9731 - val_accuracy: 0.6721\n",
      "Epoch 2/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 1.0030 - accuracy: 0.6522\n",
      "Epoch 2: val_accuracy improved from 0.67210 to 0.74117, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 39s 118ms/step - loss: 1.0030 - accuracy: 0.6522 - val_loss: 0.7559 - val_accuracy: 0.7412\n",
      "Epoch 3/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.8543 - accuracy: 0.7049\n",
      "Epoch 3: val_accuracy improved from 0.74117 to 0.75669, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 38s 115ms/step - loss: 0.8543 - accuracy: 0.7049 - val_loss: 0.7171 - val_accuracy: 0.7567\n",
      "Epoch 4/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7722 - accuracy: 0.7341\n",
      "Epoch 4: val_accuracy improved from 0.75669 to 0.76717, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 52s 158ms/step - loss: 0.7722 - accuracy: 0.7341 - val_loss: 0.6604 - val_accuracy: 0.7672\n",
      "Epoch 5/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.7536\n",
      "Epoch 5: val_accuracy improved from 0.76717 to 0.78580, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 72s 217ms/step - loss: 0.7141 - accuracy: 0.7536 - val_loss: 0.6121 - val_accuracy: 0.7858\n",
      "Epoch 6/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.7666\n",
      "Epoch 6: val_accuracy improved from 0.78580 to 0.78735, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 61s 185ms/step - loss: 0.6723 - accuracy: 0.7666 - val_loss: 0.5905 - val_accuracy: 0.7873\n",
      "Epoch 7/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6398 - accuracy: 0.7774\n",
      "Epoch 7: val_accuracy improved from 0.78735 to 0.80287, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 69s 206ms/step - loss: 0.6398 - accuracy: 0.7774 - val_loss: 0.5722 - val_accuracy: 0.8029\n",
      "Epoch 8/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.6099 - accuracy: 0.7864\n",
      "Epoch 8: val_accuracy did not improve from 0.80287\n",
      "330/330 [==============================] - 47s 143ms/step - loss: 0.6099 - accuracy: 0.7864 - val_loss: 0.5747 - val_accuracy: 0.7986\n",
      "Epoch 9/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5980 - accuracy: 0.7949\n",
      "Epoch 9: val_accuracy did not improve from 0.80287\n",
      "330/330 [==============================] - 67s 203ms/step - loss: 0.5980 - accuracy: 0.7949 - val_loss: 0.6280 - val_accuracy: 0.7866\n",
      "Epoch 10/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5697 - accuracy: 0.8011\n",
      "Epoch 10: val_accuracy improved from 0.80287 to 0.81801, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 71s 213ms/step - loss: 0.5697 - accuracy: 0.8011 - val_loss: 0.5257 - val_accuracy: 0.8180\n",
      "Epoch 11/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5536 - accuracy: 0.8071\n",
      "Epoch 11: val_accuracy did not improve from 0.81801\n",
      "330/330 [==============================] - 70s 211ms/step - loss: 0.5536 - accuracy: 0.8071 - val_loss: 0.5535 - val_accuracy: 0.8168\n",
      "Epoch 12/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5512 - accuracy: 0.8082\n",
      "Epoch 12: val_accuracy did not improve from 0.81801\n",
      "330/330 [==============================] - 76s 228ms/step - loss: 0.5512 - accuracy: 0.8082 - val_loss: 0.5492 - val_accuracy: 0.8157\n",
      "Epoch 13/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.8171\n",
      "Epoch 13: val_accuracy did not improve from 0.81801\n",
      "330/330 [==============================] - 54s 163ms/step - loss: 0.5337 - accuracy: 0.8171 - val_loss: 0.5238 - val_accuracy: 0.8118\n",
      "Epoch 14/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5101 - accuracy: 0.8260\n",
      "Epoch 14: val_accuracy did not improve from 0.81801\n",
      "330/330 [==============================] - 50s 150ms/step - loss: 0.5101 - accuracy: 0.8260 - val_loss: 0.5891 - val_accuracy: 0.7959\n",
      "Epoch 15/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.8216\n",
      "Epoch 15: val_accuracy improved from 0.81801 to 0.83159, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 50s 151ms/step - loss: 0.5133 - accuracy: 0.8216 - val_loss: 0.5035 - val_accuracy: 0.8316\n",
      "Epoch 16/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.5054 - accuracy: 0.8254\n",
      "Epoch 16: val_accuracy did not improve from 0.83159\n",
      "330/330 [==============================] - 37s 111ms/step - loss: 0.5054 - accuracy: 0.8254 - val_loss: 0.5386 - val_accuracy: 0.8196\n",
      "Epoch 17/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.8357\n",
      "Epoch 17: val_accuracy did not improve from 0.83159\n",
      "330/330 [==============================] - 34s 104ms/step - loss: 0.4826 - accuracy: 0.8357 - val_loss: 0.5223 - val_accuracy: 0.8265\n",
      "Epoch 18/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4843 - accuracy: 0.8338\n",
      "Epoch 18: val_accuracy did not improve from 0.83159\n",
      "330/330 [==============================] - 31s 93ms/step - loss: 0.4843 - accuracy: 0.8338 - val_loss: 0.5376 - val_accuracy: 0.8203\n",
      "Epoch 19/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4777 - accuracy: 0.8354\n",
      "Epoch 19: val_accuracy did not improve from 0.83159\n",
      "330/330 [==============================] - 31s 95ms/step - loss: 0.4777 - accuracy: 0.8354 - val_loss: 0.5131 - val_accuracy: 0.8316\n",
      "Epoch 20/20\n",
      "330/330 [==============================] - ETA: 0s - loss: 0.4646 - accuracy: 0.8387\n",
      "Epoch 20: val_accuracy improved from 0.83159 to 0.84323, saving model to models\\transformer.h5\n",
      "330/330 [==============================] - 33s 101ms/step - loss: 0.4646 - accuracy: 0.8387 - val_loss: 0.4882 - val_accuracy: 0.8432\n",
      "Epoch 1/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.8191 - accuracy: 0.4091\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62709, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 35s 193ms/step - loss: 1.8191 - accuracy: 0.4091 - val_loss: 1.1203 - val_accuracy: 0.6271\n",
      "Epoch 2/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 1.0297 - accuracy: 0.6447\n",
      "Epoch 2: val_accuracy improved from 0.62709 to 0.71013, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 31s 189ms/step - loss: 1.0297 - accuracy: 0.6447 - val_loss: 0.8577 - val_accuracy: 0.7101\n",
      "Epoch 3/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.8328 - accuracy: 0.7105\n",
      "Epoch 3: val_accuracy improved from 0.71013 to 0.76213, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 32s 196ms/step - loss: 0.8328 - accuracy: 0.7105 - val_loss: 0.6924 - val_accuracy: 0.7621\n",
      "Epoch 4/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.7517 - accuracy: 0.7411\n",
      "Epoch 4: val_accuracy improved from 0.76213 to 0.76872, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 34s 204ms/step - loss: 0.7517 - accuracy: 0.7411 - val_loss: 0.7007 - val_accuracy: 0.7687\n",
      "Epoch 5/20\n",
      "165/165 [==============================] - ETA: 0s - loss: 0.6821 - accuracy: 0.7633\n",
      "Epoch 5: val_accuracy improved from 0.76872 to 0.79162, saving model to models\\transformer.h5\n",
      "165/165 [==============================] - 38s 228ms/step - loss: 0.6821 - accuracy: 0.7633 - val_loss: 0.6185 - val_accuracy: 0.7916\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m [\u001b[39m64\u001b[39m,\u001b[39m128\u001b[39m]:\n\u001b[0;32m      8\u001b[0m     model \u001b[39m=\u001b[39m Transformer(num_heads\u001b[39m=\u001b[39mnum_heads, num_layers\u001b[39m=\u001b[39mnum_layer, dropout_rate\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, learning_rate\u001b[39m=\u001b[39mlr, num_classes\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m, batch_size\u001b[39m=\u001b[39mbatch, epoch\u001b[39m=\u001b[39mepoch)\n\u001b[1;32m----> 9\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(labels_only_detection_training, labels_only_detection_validation)\n\u001b[0;32m     10\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((res, pd\u001b[39m.\u001b[39mDataFrame([[num_heads,\u001b[39m0.2\u001b[39m, lr, epoch, batch,num_layer, model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], model\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]]], \n\u001b[0;32m     11\u001b[0m                                                             columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnum_layer\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39maccuracy_max\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mval_loss_max\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m])))\n",
      "File \u001b[1;32mc:\\Users\\jan20\\OneDrive\\Pulpit\\DS\\sem2\\Deep_learning\\Deep_Learning\\RNN\\models.py:234\u001b[0m, in \u001b[0;36mTransformer.train\u001b[1;34m(self, train_Dataset, val_Dataset)\u001b[0m\n\u001b[0;32m    232\u001b[0m early \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m    233\u001b[0m callbacks_list \u001b[39m=\u001b[39m [checkpoint, early]\n\u001b[1;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(train_Dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size), validation_data\u001b[39m=\u001b[39;49mval_Dataset\u001b[39m.\u001b[39;49mbatch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size), epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch, callbacks\u001b[39m=\u001b[39;49mcallbacks_list)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\jan20\\PycharmProjects\\PytorchProjects\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame(columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])\n",
    "# res.columns = ['gru_units', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max']\n",
    "for num_heads in [2,4]:\n",
    "    for num_layer in [2,4]:\n",
    "        for epoch in [20, 30]:\n",
    "            for lr in [0.001, 0.01, 0.1]:\n",
    "                for batch in [64,128]:\n",
    "                    model = Transformer(num_heads=num_heads, num_layers=num_layer, dropout_rate=0.2, learning_rate=lr, num_classes=10, batch_size=batch, epoch=epoch)\n",
    "                    model.train(labels_only_detection_training, labels_only_detection_validation)\n",
    "                    res = np.concatenate((res, pd.DataFrame([[num_heads,0.2, lr, epoch, batch,num_layer, model.history.history['loss'][-1], model.history.history['accuracy'][-1], model.history.history['val_loss'][-1], model.history.history['val_accuracy'][-1]]], \n",
    "                                                                            columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(res,columns=['num_heads', 'dropout_rate', 'learning_rate', 'epoch', 'batch', 'num_layer', 'loss_max','accuracy_max','val_loss_max', 'val_accuracy_max']).to_pickle('results/transformer_wyniki_only_known.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle('results\\\\model_transformer_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>2.120008</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.790722</td>\n",
       "      <td>0.600954</td>\n",
       "      <td>1.342593</td>\n",
       "      <td>0.812886</td>\n",
       "      <td>0.812886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_heads  num_layers  dropout_rate  epoch  batch      loss  loss_max  \\\n",
       "19        4.0         1.0           0.2   20.0   64.0  0.690125  2.120008   \n",
       "\n",
       "    accuracy  accuracy_max  val_loss  val_loss_max  val_accuracy  \\\n",
       "19  0.790032      0.790722  0.600954      1.342593      0.812886   \n",
       "\n",
       "    val_accuracy_max  \n",
       "19          0.812886  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[results.val_accuracy == results.val_accuracy.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(4, 1, 0.2, 20, 64, 0.001, (39,44), 11, model_path='models\\\\transformer_mod.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 1.4390 - accuracy: 0.6051\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62386, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 64ms/step - loss: 1.4361 - accuracy: 0.6059 - val_loss: 1.2966 - val_accuracy: 0.6239\n",
      "Epoch 2/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.2717 - accuracy: 0.6354\n",
      "Epoch 2: val_accuracy improved from 0.62386 to 0.68123, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.2717 - accuracy: 0.6354 - val_loss: 1.0730 - val_accuracy: 0.6812\n",
      "Epoch 3/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.1600 - accuracy: 0.6546\n",
      "Epoch 3: val_accuracy did not improve from 0.68123\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.1600 - accuracy: 0.6546 - val_loss: 1.2423 - val_accuracy: 0.6174\n",
      "Epoch 4/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 1.0674 - accuracy: 0.6781\n",
      "Epoch 4: val_accuracy did not improve from 0.68123\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 1.0674 - accuracy: 0.6781 - val_loss: 1.0686 - val_accuracy: 0.6736\n",
      "Epoch 5/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.9702 - accuracy: 0.6976\n",
      "Epoch 5: val_accuracy improved from 0.68123 to 0.70359, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 58ms/step - loss: 0.9702 - accuracy: 0.6976 - val_loss: 0.9470 - val_accuracy: 0.7036\n",
      "Epoch 6/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.9419 - accuracy: 0.7089\n",
      "Epoch 6: val_accuracy improved from 0.70359 to 0.72286, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 0.9403 - accuracy: 0.7090 - val_loss: 0.8850 - val_accuracy: 0.7229\n",
      "Epoch 7/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8779 - accuracy: 0.7252\n",
      "Epoch 7: val_accuracy improved from 0.72286 to 0.75081, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 59ms/step - loss: 0.8779 - accuracy: 0.7252 - val_loss: 0.8093 - val_accuracy: 0.7508\n",
      "Epoch 8/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.8337 - accuracy: 0.7391\n",
      "Epoch 8: val_accuracy improved from 0.75081 to 0.75919, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.8337 - accuracy: 0.7391 - val_loss: 0.7818 - val_accuracy: 0.7592\n",
      "Epoch 9/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.7511\n",
      "Epoch 9: val_accuracy did not improve from 0.75919\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.7798 - accuracy: 0.7511 - val_loss: 0.7994 - val_accuracy: 0.7470\n",
      "Epoch 10/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.7608 - accuracy: 0.7498\n",
      "Epoch 10: val_accuracy improved from 0.75919 to 0.76361, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 6s 61ms/step - loss: 0.7608 - accuracy: 0.7498 - val_loss: 0.7453 - val_accuracy: 0.7636\n",
      "Epoch 11/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.7196 - accuracy: 0.7644\n",
      "Epoch 11: val_accuracy did not improve from 0.76361\n",
      "107/107 [==============================] - 7s 63ms/step - loss: 0.7207 - accuracy: 0.7637 - val_loss: 0.7486 - val_accuracy: 0.7599\n",
      "Epoch 12/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6988 - accuracy: 0.7748\n",
      "Epoch 12: val_accuracy did not improve from 0.76361\n",
      "107/107 [==============================] - 6s 61ms/step - loss: 0.6979 - accuracy: 0.7753 - val_loss: 0.7384 - val_accuracy: 0.7633\n",
      "Epoch 13/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6861 - accuracy: 0.7761\n",
      "Epoch 13: val_accuracy improved from 0.76361 to 0.78155, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 64ms/step - loss: 0.6844 - accuracy: 0.7769 - val_loss: 0.6976 - val_accuracy: 0.7816\n",
      "Epoch 14/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7913\n",
      "Epoch 14: val_accuracy improved from 0.78155 to 0.78376, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 66ms/step - loss: 0.6378 - accuracy: 0.7909 - val_loss: 0.6855 - val_accuracy: 0.7838\n",
      "Epoch 15/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6264 - accuracy: 0.7941\n",
      "Epoch 15: val_accuracy did not improve from 0.78376\n",
      "107/107 [==============================] - 7s 62ms/step - loss: 0.6264 - accuracy: 0.7940 - val_loss: 0.7411 - val_accuracy: 0.7682\n",
      "Epoch 16/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.6216 - accuracy: 0.7957\n",
      "Epoch 16: val_accuracy did not improve from 0.78376\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.6217 - accuracy: 0.7958 - val_loss: 0.7235 - val_accuracy: 0.7780\n",
      "Epoch 17/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.6095 - accuracy: 0.8013\n",
      "Epoch 17: val_accuracy improved from 0.78376 to 0.78773, saving model to models\\transformer_mod.h5\n",
      "107/107 [==============================] - 7s 62ms/step - loss: 0.6095 - accuracy: 0.8013 - val_loss: 0.6695 - val_accuracy: 0.7877\n",
      "Epoch 18/20\n",
      "107/107 [==============================] - ETA: 0s - loss: 0.5933 - accuracy: 0.8050\n",
      "Epoch 18: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 6s 60ms/step - loss: 0.5933 - accuracy: 0.8050 - val_loss: 0.7116 - val_accuracy: 0.7721\n",
      "Epoch 19/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.5547 - accuracy: 0.8187\n",
      "Epoch 19: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 7s 61ms/step - loss: 0.5546 - accuracy: 0.8189 - val_loss: 0.6890 - val_accuracy: 0.7855\n",
      "Epoch 20/20\n",
      "106/107 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.8174\n",
      "Epoch 20: val_accuracy did not improve from 0.78773\n",
      "107/107 [==============================] - 6s 58ms/step - loss: 0.5425 - accuracy: 0.8173 - val_loss: 0.7654 - val_accuracy: 0.7638\n"
     ]
    }
   ],
   "source": [
    "model.train(label_detection_training, label_detection_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from models import Lstm, Gru, Transformer\n",
    "from dataset import LABELS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('extracted_features\\\\features_test.pkl')\n",
    "dataset = tf.data.Dataset.from_tensor_slices((test, np.zeros(len(test))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15854/15854 [==============================] - 58s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "x = model.predict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    97594\n",
       "7      9068\n",
       "2      8487\n",
       "5      7625\n",
       "4      7303\n",
       "9      6912\n",
       "0      6628\n",
       "8      4575\n",
       "6      4372\n",
       "1      4179\n",
       "3      1795\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed, BatchNormalization\n",
    "from dataset import LABELS\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataset import label_detection_training, label_detection_validation, silence_detection_training, silence_detection_validation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from dataset import TensorflowDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "# val = pd.read_pickle('extracted_features\\\\features_validation.pkl')\n",
    "\n",
    "# X_train = np.array([x[0] for x in train])\n",
    "# y_train = np.array([x[1] for x in train])\n",
    "# X_val = np.array([x[0] for x in val])\n",
    "# y_val = np.array([x[1] for x in val])\n",
    "\n",
    "# # Create a label encoder object\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# # Fit the label encoder using your labels (combine both train and val labels if they have different classes)\n",
    "# label_encoder.fit(np.concatenate((y_train, y_val)))\n",
    "\n",
    "# # Transform your string labels to integer labels for both training and validation sets\n",
    "# y_train_encoded = label_encoder.transform(y_train)\n",
    "# y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle('extracted_features\\\\features_training.pkl')\n",
    "y_train = np.array([x[1] for x in train])\n",
    "labels = list(np.unique(y_train))\n",
    "train = TensorflowDataset('extracted_features\\\\features_training.pkl', labels=labels).dataset\n",
    "train = train.shuffle(len(train), reshuffle_each_iteration=True)\n",
    "val = TensorflowDataset('extracted_features\\\\features_validation.pkl', labels=labels).dataset\n",
    "val = val.shuffle(len(val), reshuffle_each_iteration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 10\n",
      "Epoch 1/10\n",
      "1811/1811 - 49s - loss: 2.2341 - accuracy: 0.3708 - val_loss: 1.4084 - val_accuracy: 0.5799 - 49s/epoch - 27ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 45s - loss: 1.5168 - accuracy: 0.5522 - val_loss: 1.0448 - val_accuracy: 0.6921 - 45s/epoch - 25ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 48s - loss: 1.3047 - accuracy: 0.6113 - val_loss: 1.1216 - val_accuracy: 0.6602 - 48s/epoch - 27ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 49s - loss: 1.2991 - accuracy: 0.6142 - val_loss: 0.9259 - val_accuracy: 0.7234 - 49s/epoch - 27ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 49s - loss: 1.1588 - accuracy: 0.6563 - val_loss: 0.8972 - val_accuracy: 0.7286 - 49s/epoch - 27ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 48s - loss: 1.1611 - accuracy: 0.6545 - val_loss: 0.8835 - val_accuracy: 0.7343 - 48s/epoch - 26ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 50s - loss: 1.1062 - accuracy: 0.6720 - val_loss: 0.8617 - val_accuracy: 0.7398 - 50s/epoch - 28ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 51s - loss: 1.0632 - accuracy: 0.6827 - val_loss: 0.8325 - val_accuracy: 0.7505 - 51s/epoch - 28ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 50s - loss: 1.0183 - accuracy: 0.6955 - val_loss: 0.7855 - val_accuracy: 0.7580 - 50s/epoch - 28ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 52s - loss: 1.0201 - accuracy: 0.6949 - val_loss: 0.7494 - val_accuracy: 0.7652 - 52s/epoch - 29ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 66s - loss: 2.1068 - accuracy: 0.4024 - val_loss: 1.2203 - val_accuracy: 0.6320 - 66s/epoch - 37ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 60s - loss: 1.3975 - accuracy: 0.5842 - val_loss: 1.0267 - val_accuracy: 0.6958 - 60s/epoch - 33ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 61s - loss: 1.2557 - accuracy: 0.6253 - val_loss: 0.9898 - val_accuracy: 0.7030 - 61s/epoch - 34ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 60s - loss: 1.1790 - accuracy: 0.6498 - val_loss: 0.9405 - val_accuracy: 0.7162 - 60s/epoch - 33ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 59s - loss: 1.1051 - accuracy: 0.6706 - val_loss: 0.8465 - val_accuracy: 0.7433 - 59s/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 59s - loss: 1.0588 - accuracy: 0.6832 - val_loss: 0.8125 - val_accuracy: 0.7501 - 59s/epoch - 33ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 59s - loss: 1.0260 - accuracy: 0.6957 - val_loss: 0.7454 - val_accuracy: 0.7695 - 59s/epoch - 33ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 59s - loss: 1.0234 - accuracy: 0.6941 - val_loss: 0.8140 - val_accuracy: 0.7492 - 59s/epoch - 32ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 59s - loss: 0.9964 - accuracy: 0.7015 - val_loss: 0.7606 - val_accuracy: 0.7705 - 59s/epoch - 33ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 60s - loss: 0.9706 - accuracy: 0.7115 - val_loss: 0.7610 - val_accuracy: 0.7742 - 60s/epoch - 33ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 59s - loss: 0.9567 - accuracy: 0.7159 - val_loss: 0.7523 - val_accuracy: 0.7711 - 59s/epoch - 32ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 59s - loss: 0.9381 - accuracy: 0.7184 - val_loss: 0.7059 - val_accuracy: 0.7799 - 59s/epoch - 33ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 59s - loss: 0.9173 - accuracy: 0.7251 - val_loss: 0.7533 - val_accuracy: 0.7652 - 59s/epoch - 33ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 59s - loss: 0.9095 - accuracy: 0.7280 - val_loss: 0.6973 - val_accuracy: 0.7905 - 59s/epoch - 32ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 58s - loss: 0.8989 - accuracy: 0.7309 - val_loss: 0.7127 - val_accuracy: 0.7792 - 58s/epoch - 32ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 55s - loss: 0.8772 - accuracy: 0.7378 - val_loss: 0.6731 - val_accuracy: 0.7945 - 55s/epoch - 30ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 63s - loss: 0.8873 - accuracy: 0.7348 - val_loss: 0.6656 - val_accuracy: 0.7970 - 63s/epoch - 35ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 62s - loss: 0.8480 - accuracy: 0.7459 - val_loss: 0.6658 - val_accuracy: 0.7948 - 62s/epoch - 34ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 64s - loss: 0.8532 - accuracy: 0.7436 - val_loss: 0.6740 - val_accuracy: 0.7949 - 64s/epoch - 35ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 62s - loss: 0.8354 - accuracy: 0.7529 - val_loss: 0.6700 - val_accuracy: 0.7982 - 62s/epoch - 34ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 69s - loss: 2.1687 - accuracy: 0.3928 - val_loss: 1.2507 - val_accuracy: 0.6231 - 69s/epoch - 76ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 59s - loss: 1.3432 - accuracy: 0.6033 - val_loss: 0.9850 - val_accuracy: 0.6986 - 59s/epoch - 65ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 59s - loss: 1.1516 - accuracy: 0.6575 - val_loss: 0.9089 - val_accuracy: 0.7239 - 59s/epoch - 65ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 50s - loss: 1.0466 - accuracy: 0.6855 - val_loss: 0.8129 - val_accuracy: 0.7557 - 50s/epoch - 55ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 59s - loss: 0.9956 - accuracy: 0.7029 - val_loss: 0.8737 - val_accuracy: 0.7315 - 59s/epoch - 65ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 60s - loss: 0.9615 - accuracy: 0.7107 - val_loss: 0.8457 - val_accuracy: 0.7429 - 60s/epoch - 67ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 58s - loss: 0.9409 - accuracy: 0.7180 - val_loss: 0.7405 - val_accuracy: 0.7764 - 58s/epoch - 64ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 61s - loss: 0.8800 - accuracy: 0.7343 - val_loss: 0.7344 - val_accuracy: 0.7732 - 61s/epoch - 67ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 60s - loss: 0.8790 - accuracy: 0.7355 - val_loss: 0.7138 - val_accuracy: 0.7804 - 60s/epoch - 66ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 59s - loss: 0.8361 - accuracy: 0.7488 - val_loss: 0.6978 - val_accuracy: 0.7821 - 59s/epoch - 65ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 118s - loss: 2.1037 - accuracy: 0.4103 - val_loss: 1.1963 - val_accuracy: 0.6392 - 118s/epoch - 130ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 101s - loss: 1.3822 - accuracy: 0.5919 - val_loss: 1.0613 - val_accuracy: 0.6759 - 101s/epoch - 112ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 94s - loss: 1.1698 - accuracy: 0.6513 - val_loss: 0.8555 - val_accuracy: 0.7427 - 94s/epoch - 104ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 90s - loss: 1.0694 - accuracy: 0.6805 - val_loss: 0.8836 - val_accuracy: 0.7293 - 90s/epoch - 99ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 93s - loss: 1.0819 - accuracy: 0.6767 - val_loss: 0.8426 - val_accuracy: 0.7489 - 93s/epoch - 103ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 92s - loss: 0.9971 - accuracy: 0.6995 - val_loss: 0.7443 - val_accuracy: 0.7718 - 92s/epoch - 101ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 93s - loss: 0.9367 - accuracy: 0.7204 - val_loss: 0.7408 - val_accuracy: 0.7795 - 93s/epoch - 102ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 93s - loss: 0.9282 - accuracy: 0.7207 - val_loss: 0.6834 - val_accuracy: 0.7945 - 93s/epoch - 103ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 94s - loss: 0.9016 - accuracy: 0.7295 - val_loss: 0.6862 - val_accuracy: 0.7939 - 94s/epoch - 104ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 92s - loss: 0.8396 - accuracy: 0.7475 - val_loss: 0.6798 - val_accuracy: 0.7910 - 92s/epoch - 102ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 88s - loss: 0.8189 - accuracy: 0.7537 - val_loss: 0.6650 - val_accuracy: 0.7970 - 88s/epoch - 97ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 87s - loss: 0.8193 - accuracy: 0.7542 - val_loss: 0.6548 - val_accuracy: 0.8020 - 87s/epoch - 96ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 89s - loss: 0.7878 - accuracy: 0.7629 - val_loss: 0.6748 - val_accuracy: 0.7946 - 89s/epoch - 98ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 88s - loss: 0.7794 - accuracy: 0.7640 - val_loss: 0.6947 - val_accuracy: 0.7917 - 88s/epoch - 97ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 90s - loss: 0.7724 - accuracy: 0.7663 - val_loss: 0.6232 - val_accuracy: 0.8076 - 90s/epoch - 99ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 88s - loss: 0.7560 - accuracy: 0.7723 - val_loss: 0.6265 - val_accuracy: 0.8094 - 88s/epoch - 97ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 90s - loss: 0.7304 - accuracy: 0.7782 - val_loss: 0.6092 - val_accuracy: 0.8167 - 90s/epoch - 99ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 89s - loss: 0.7648 - accuracy: 0.7683 - val_loss: 0.6040 - val_accuracy: 0.8091 - 89s/epoch - 99ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 90s - loss: 0.7440 - accuracy: 0.7758 - val_loss: 0.5918 - val_accuracy: 0.8207 - 90s/epoch - 99ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 89s - loss: 0.7148 - accuracy: 0.7831 - val_loss: 0.6295 - val_accuracy: 0.8123 - 89s/epoch - 99ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 88s - loss: 2.8880 - accuracy: 0.2114 - val_loss: 1.9152 - val_accuracy: 0.4313 - 88s/epoch - 49ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 75s - loss: 1.9606 - accuracy: 0.4265 - val_loss: 1.5135 - val_accuracy: 0.5418 - 75s/epoch - 41ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 75s - loss: 1.7141 - accuracy: 0.4964 - val_loss: 1.3216 - val_accuracy: 0.6167 - 75s/epoch - 41ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 74s - loss: 1.6374 - accuracy: 0.5199 - val_loss: 1.2035 - val_accuracy: 0.6568 - 74s/epoch - 41ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 74s - loss: 1.4951 - accuracy: 0.5615 - val_loss: 1.1292 - val_accuracy: 0.6808 - 74s/epoch - 41ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 73s - loss: 1.4249 - accuracy: 0.5817 - val_loss: 1.1265 - val_accuracy: 0.6914 - 73s/epoch - 40ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 73s - loss: 1.3747 - accuracy: 0.5957 - val_loss: 1.0877 - val_accuracy: 0.6942 - 73s/epoch - 40ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 74s - loss: 1.3169 - accuracy: 0.6136 - val_loss: 1.0696 - val_accuracy: 0.6943 - 74s/epoch - 41ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 73s - loss: 1.2841 - accuracy: 0.6216 - val_loss: 1.0865 - val_accuracy: 0.6871 - 73s/epoch - 40ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 74s - loss: 1.2587 - accuracy: 0.6307 - val_loss: 1.0754 - val_accuracy: 0.6811 - 74s/epoch - 41ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 70s - loss: 2.9656 - accuracy: 0.1883 - val_loss: 1.9107 - val_accuracy: 0.4328 - 70s/epoch - 38ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 58s - loss: 2.1559 - accuracy: 0.3685 - val_loss: 1.5600 - val_accuracy: 0.5294 - 58s/epoch - 32ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 58s - loss: 1.9294 - accuracy: 0.4344 - val_loss: 1.6689 - val_accuracy: 0.5040 - 58s/epoch - 32ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 57s - loss: 1.8102 - accuracy: 0.4710 - val_loss: 1.3358 - val_accuracy: 0.5978 - 57s/epoch - 31ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 58s - loss: 1.6944 - accuracy: 0.5047 - val_loss: 1.3843 - val_accuracy: 0.5861 - 58s/epoch - 32ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 59s - loss: 1.7247 - accuracy: 0.4969 - val_loss: 1.2546 - val_accuracy: 0.6280 - 59s/epoch - 32ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 58s - loss: 1.5653 - accuracy: 0.5430 - val_loss: 1.1360 - val_accuracy: 0.6608 - 58s/epoch - 32ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 58s - loss: 1.5096 - accuracy: 0.5603 - val_loss: 1.1550 - val_accuracy: 0.6565 - 58s/epoch - 32ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 59s - loss: 1.4759 - accuracy: 0.5693 - val_loss: 1.1402 - val_accuracy: 0.6642 - 59s/epoch - 32ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 59s - loss: 1.4318 - accuracy: 0.5816 - val_loss: 1.0620 - val_accuracy: 0.6856 - 59s/epoch - 33ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 57s - loss: 1.4061 - accuracy: 0.5920 - val_loss: 1.1076 - val_accuracy: 0.6686 - 57s/epoch - 31ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 58s - loss: 1.3788 - accuracy: 0.6013 - val_loss: 1.0828 - val_accuracy: 0.6802 - 58s/epoch - 32ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 59s - loss: 1.3507 - accuracy: 0.6056 - val_loss: 1.1181 - val_accuracy: 0.6677 - 59s/epoch - 33ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 58s - loss: 1.3661 - accuracy: 0.6036 - val_loss: 1.0943 - val_accuracy: 0.6799 - 58s/epoch - 32ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 60s - loss: 1.3172 - accuracy: 0.6178 - val_loss: 1.0533 - val_accuracy: 0.6904 - 60s/epoch - 33ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 58s - loss: 1.2815 - accuracy: 0.6282 - val_loss: 1.2224 - val_accuracy: 0.6518 - 58s/epoch - 32ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 59s - loss: 1.2834 - accuracy: 0.6276 - val_loss: 1.0122 - val_accuracy: 0.7084 - 59s/epoch - 32ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 58s - loss: 1.2610 - accuracy: 0.6341 - val_loss: 1.0368 - val_accuracy: 0.7048 - 58s/epoch - 32ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 58s - loss: 1.2425 - accuracy: 0.6414 - val_loss: 1.0248 - val_accuracy: 0.7037 - 58s/epoch - 32ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 59s - loss: 1.2828 - accuracy: 0.6324 - val_loss: 1.1021 - val_accuracy: 0.6840 - 59s/epoch - 32ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 59s - loss: 3.0186 - accuracy: 0.1848 - val_loss: 1.9392 - val_accuracy: 0.4317 - 59s/epoch - 65ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 49s - loss: 1.9983 - accuracy: 0.4152 - val_loss: 1.3500 - val_accuracy: 0.6011 - 49s/epoch - 54ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 49s - loss: 1.7194 - accuracy: 0.4952 - val_loss: 1.3456 - val_accuracy: 0.5971 - 49s/epoch - 54ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 49s - loss: 1.5301 - accuracy: 0.5480 - val_loss: 1.1968 - val_accuracy: 0.6480 - 49s/epoch - 55ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 48s - loss: 1.6161 - accuracy: 0.5242 - val_loss: 1.0982 - val_accuracy: 0.6731 - 48s/epoch - 53ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 49s - loss: 1.4156 - accuracy: 0.5828 - val_loss: 1.0176 - val_accuracy: 0.6956 - 49s/epoch - 55ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 49s - loss: 1.3523 - accuracy: 0.6023 - val_loss: 0.9910 - val_accuracy: 0.7017 - 49s/epoch - 54ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 50s - loss: 1.3352 - accuracy: 0.6055 - val_loss: 0.9973 - val_accuracy: 0.6979 - 50s/epoch - 55ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 50s - loss: 1.2867 - accuracy: 0.6225 - val_loss: 1.0096 - val_accuracy: 0.6964 - 50s/epoch - 55ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 49s - loss: 1.2571 - accuracy: 0.6274 - val_loss: 0.9291 - val_accuracy: 0.7221 - 49s/epoch - 54ms/step\n",
      "num_heads: 2, num_layers: 1, dropout_rate: 0.4, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 55s - loss: 3.0289 - accuracy: 0.1875 - val_loss: 1.7118 - val_accuracy: 0.4871 - 55s/epoch - 61ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 49s - loss: 1.9964 - accuracy: 0.4133 - val_loss: 1.4481 - val_accuracy: 0.5818 - 49s/epoch - 54ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 48s - loss: 1.7583 - accuracy: 0.4846 - val_loss: 1.2385 - val_accuracy: 0.6255 - 48s/epoch - 53ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 48s - loss: 1.5753 - accuracy: 0.5354 - val_loss: 1.1838 - val_accuracy: 0.6458 - 48s/epoch - 53ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 48s - loss: 1.4876 - accuracy: 0.5596 - val_loss: 1.1234 - val_accuracy: 0.6578 - 48s/epoch - 53ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 48s - loss: 1.4348 - accuracy: 0.5775 - val_loss: 1.0618 - val_accuracy: 0.6806 - 48s/epoch - 53ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 48s - loss: 1.3826 - accuracy: 0.5938 - val_loss: 1.0658 - val_accuracy: 0.6767 - 48s/epoch - 53ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 48s - loss: 1.3360 - accuracy: 0.6068 - val_loss: 1.0519 - val_accuracy: 0.6902 - 48s/epoch - 53ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 47s - loss: 1.3375 - accuracy: 0.6056 - val_loss: 0.9634 - val_accuracy: 0.7123 - 47s/epoch - 52ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 48s - loss: 1.2639 - accuracy: 0.6273 - val_loss: 0.9261 - val_accuracy: 0.7207 - 48s/epoch - 53ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 48s - loss: 1.2710 - accuracy: 0.6247 - val_loss: 0.8898 - val_accuracy: 0.7296 - 48s/epoch - 53ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 47s - loss: 1.2072 - accuracy: 0.6434 - val_loss: 0.9147 - val_accuracy: 0.7280 - 47s/epoch - 52ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 47s - loss: 1.2005 - accuracy: 0.6451 - val_loss: 0.8638 - val_accuracy: 0.7392 - 47s/epoch - 52ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 46s - loss: 1.1800 - accuracy: 0.6514 - val_loss: 0.8215 - val_accuracy: 0.7518 - 46s/epoch - 51ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 44s - loss: 1.1668 - accuracy: 0.6561 - val_loss: 0.8747 - val_accuracy: 0.7360 - 44s/epoch - 48ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 49s - loss: 1.1667 - accuracy: 0.6550 - val_loss: 0.8772 - val_accuracy: 0.7352 - 49s/epoch - 54ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 50s - loss: 1.1722 - accuracy: 0.6541 - val_loss: 0.8474 - val_accuracy: 0.7543 - 50s/epoch - 55ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 49s - loss: 1.1178 - accuracy: 0.6718 - val_loss: 0.8514 - val_accuracy: 0.7404 - 49s/epoch - 54ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 49s - loss: 1.1298 - accuracy: 0.6689 - val_loss: 0.8762 - val_accuracy: 0.7339 - 49s/epoch - 54ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 49s - loss: 1.1906 - accuracy: 0.6485 - val_loss: 0.8647 - val_accuracy: 0.7433 - 49s/epoch - 54ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 179s - loss: 2.2579 - accuracy: 0.3634 - val_loss: 1.4015 - val_accuracy: 0.5800 - 179s/epoch - 99ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 162s - loss: 1.4520 - accuracy: 0.5709 - val_loss: 1.2543 - val_accuracy: 0.6231 - 162s/epoch - 89ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 159s - loss: 1.3048 - accuracy: 0.6151 - val_loss: 1.0973 - val_accuracy: 0.6667 - 159s/epoch - 88ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 166s - loss: 1.1778 - accuracy: 0.6499 - val_loss: 1.0563 - val_accuracy: 0.6786 - 166s/epoch - 91ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 166s - loss: 1.1349 - accuracy: 0.6622 - val_loss: 0.8829 - val_accuracy: 0.7346 - 166s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 166s - loss: 1.0741 - accuracy: 0.6824 - val_loss: 0.8584 - val_accuracy: 0.7411 - 166s/epoch - 92ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 165s - loss: 1.0262 - accuracy: 0.6956 - val_loss: 0.8829 - val_accuracy: 0.7345 - 165s/epoch - 91ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 165s - loss: 1.0391 - accuracy: 0.6915 - val_loss: 0.8633 - val_accuracy: 0.7407 - 165s/epoch - 91ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 167s - loss: 1.0289 - accuracy: 0.6960 - val_loss: 0.8151 - val_accuracy: 0.7588 - 167s/epoch - 92ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 166s - loss: 0.9544 - accuracy: 0.7156 - val_loss: 0.8180 - val_accuracy: 0.7571 - 166s/epoch - 91ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 151s - loss: 2.2424 - accuracy: 0.3632 - val_loss: 1.3726 - val_accuracy: 0.5940 - 151s/epoch - 83ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 127s - loss: 1.4735 - accuracy: 0.5626 - val_loss: 1.1462 - val_accuracy: 0.6574 - 127s/epoch - 70ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 126s - loss: 1.2847 - accuracy: 0.6187 - val_loss: 0.9925 - val_accuracy: 0.6899 - 126s/epoch - 70ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 125s - loss: 1.2017 - accuracy: 0.6431 - val_loss: 0.9956 - val_accuracy: 0.7020 - 125s/epoch - 69ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 124s - loss: 1.1085 - accuracy: 0.6695 - val_loss: 0.8904 - val_accuracy: 0.7258 - 124s/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 125s - loss: 1.0697 - accuracy: 0.6825 - val_loss: 0.9343 - val_accuracy: 0.7218 - 125s/epoch - 69ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 124s - loss: 1.0222 - accuracy: 0.6956 - val_loss: 0.8401 - val_accuracy: 0.7457 - 124s/epoch - 69ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 125s - loss: 1.0112 - accuracy: 0.6996 - val_loss: 0.7633 - val_accuracy: 0.7643 - 125s/epoch - 69ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 113s - loss: 0.9630 - accuracy: 0.7143 - val_loss: 0.7986 - val_accuracy: 0.7561 - 113s/epoch - 62ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 117s - loss: 0.9457 - accuracy: 0.7180 - val_loss: 0.7780 - val_accuracy: 0.7627 - 117s/epoch - 64ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 114s - loss: 0.9119 - accuracy: 0.7296 - val_loss: 0.7844 - val_accuracy: 0.7692 - 114s/epoch - 63ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 115s - loss: 0.8779 - accuracy: 0.7400 - val_loss: 0.7566 - val_accuracy: 0.7757 - 115s/epoch - 64ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 115s - loss: 0.8485 - accuracy: 0.7462 - val_loss: 0.7334 - val_accuracy: 0.7804 - 115s/epoch - 64ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 114s - loss: 0.8292 - accuracy: 0.7523 - val_loss: 0.7023 - val_accuracy: 0.7870 - 114s/epoch - 63ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 115s - loss: 0.8503 - accuracy: 0.7474 - val_loss: 0.7596 - val_accuracy: 0.7714 - 115s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 114s - loss: 0.8598 - accuracy: 0.7432 - val_loss: 0.6589 - val_accuracy: 0.7980 - 114s/epoch - 63ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 114s - loss: 0.8087 - accuracy: 0.7596 - val_loss: 0.8116 - val_accuracy: 0.7555 - 114s/epoch - 63ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 113s - loss: 0.8040 - accuracy: 0.7629 - val_loss: 0.6994 - val_accuracy: 0.7946 - 113s/epoch - 62ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 113s - loss: 0.7684 - accuracy: 0.7706 - val_loss: 0.7407 - val_accuracy: 0.7879 - 113s/epoch - 62ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 111s - loss: 0.7730 - accuracy: 0.7691 - val_loss: 0.6972 - val_accuracy: 0.7899 - 111s/epoch - 62ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 97s - loss: 2.2330 - accuracy: 0.3803 - val_loss: 1.3698 - val_accuracy: 0.5887 - 97s/epoch - 107ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 82s - loss: 1.3428 - accuracy: 0.6049 - val_loss: 0.9779 - val_accuracy: 0.7026 - 82s/epoch - 91ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 85s - loss: 1.1531 - accuracy: 0.6585 - val_loss: 0.8739 - val_accuracy: 0.7332 - 85s/epoch - 93ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 85s - loss: 1.0473 - accuracy: 0.6895 - val_loss: 0.8710 - val_accuracy: 0.7324 - 85s/epoch - 94ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.0117 - accuracy: 0.6992 - val_loss: 0.8407 - val_accuracy: 0.7429 - 83s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 83s - loss: 0.9324 - accuracy: 0.7220 - val_loss: 0.7856 - val_accuracy: 0.7610 - 83s/epoch - 91ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 84s - loss: 0.8938 - accuracy: 0.7318 - val_loss: 0.7594 - val_accuracy: 0.7658 - 84s/epoch - 92ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 79s - loss: 1.0309 - accuracy: 0.6950 - val_loss: 0.8318 - val_accuracy: 0.7436 - 79s/epoch - 87ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 79s - loss: 0.8684 - accuracy: 0.7410 - val_loss: 0.7589 - val_accuracy: 0.7732 - 79s/epoch - 87ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 77s - loss: 0.8645 - accuracy: 0.7411 - val_loss: 0.6869 - val_accuracy: 0.7891 - 77s/epoch - 85ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 124s - loss: 2.1900 - accuracy: 0.3838 - val_loss: 1.5130 - val_accuracy: 0.5768 - 124s/epoch - 137ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 114s - loss: 1.3573 - accuracy: 0.5967 - val_loss: 1.1052 - val_accuracy: 0.6712 - 114s/epoch - 126ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 115s - loss: 1.1634 - accuracy: 0.6522 - val_loss: 1.0279 - val_accuracy: 0.6877 - 115s/epoch - 127ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 116s - loss: 1.0644 - accuracy: 0.6838 - val_loss: 0.9607 - val_accuracy: 0.7045 - 116s/epoch - 128ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 116s - loss: 1.0069 - accuracy: 0.6996 - val_loss: 0.9109 - val_accuracy: 0.7255 - 116s/epoch - 128ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 114s - loss: 0.9642 - accuracy: 0.7115 - val_loss: 0.7714 - val_accuracy: 0.7632 - 114s/epoch - 126ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 113s - loss: 0.9625 - accuracy: 0.7128 - val_loss: 0.8002 - val_accuracy: 0.7583 - 113s/epoch - 125ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 113s - loss: 0.8855 - accuracy: 0.7346 - val_loss: 0.7384 - val_accuracy: 0.7766 - 113s/epoch - 125ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 116s - loss: 0.8643 - accuracy: 0.7434 - val_loss: 0.7466 - val_accuracy: 0.7754 - 116s/epoch - 128ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 120s - loss: 0.8752 - accuracy: 0.7362 - val_loss: 0.7177 - val_accuracy: 0.7833 - 120s/epoch - 132ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 122s - loss: 0.8409 - accuracy: 0.7489 - val_loss: 0.7037 - val_accuracy: 0.7901 - 122s/epoch - 135ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 120s - loss: 0.7961 - accuracy: 0.7611 - val_loss: 0.7923 - val_accuracy: 0.7646 - 120s/epoch - 133ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 121s - loss: 0.8429 - accuracy: 0.7467 - val_loss: 0.6841 - val_accuracy: 0.7924 - 121s/epoch - 133ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 122s - loss: 0.7736 - accuracy: 0.7683 - val_loss: 0.6431 - val_accuracy: 0.8033 - 122s/epoch - 135ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 120s - loss: 0.7438 - accuracy: 0.7769 - val_loss: 0.7293 - val_accuracy: 0.7818 - 120s/epoch - 133ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 120s - loss: 0.7844 - accuracy: 0.7632 - val_loss: 0.6508 - val_accuracy: 0.8011 - 120s/epoch - 132ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 122s - loss: 0.7223 - accuracy: 0.7830 - val_loss: 0.6574 - val_accuracy: 0.8032 - 122s/epoch - 135ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 121s - loss: 0.7328 - accuracy: 0.7800 - val_loss: 0.6249 - val_accuracy: 0.8086 - 121s/epoch - 133ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 123s - loss: 0.7363 - accuracy: 0.7790 - val_loss: 0.6709 - val_accuracy: 0.7954 - 123s/epoch - 136ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 124s - loss: 0.7655 - accuracy: 0.7711 - val_loss: 0.6317 - val_accuracy: 0.8082 - 124s/epoch - 137ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 117s - loss: 3.0535 - accuracy: 0.1625 - val_loss: 2.7620 - val_accuracy: 0.2698 - 117s/epoch - 64ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 108s - loss: 2.1239 - accuracy: 0.3713 - val_loss: 2.3919 - val_accuracy: 0.3658 - 108s/epoch - 60ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 112s - loss: 1.9530 - accuracy: 0.4224 - val_loss: 2.4522 - val_accuracy: 0.3970 - 112s/epoch - 62ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 98s - loss: 1.7526 - accuracy: 0.4826 - val_loss: 2.4380 - val_accuracy: 0.4485 - 98s/epoch - 54ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 100s - loss: 1.6485 - accuracy: 0.5162 - val_loss: 2.4183 - val_accuracy: 0.5046 - 100s/epoch - 55ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 101s - loss: 1.5742 - accuracy: 0.5394 - val_loss: 2.7948 - val_accuracy: 0.4716 - 101s/epoch - 56ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 99s - loss: 1.5039 - accuracy: 0.5608 - val_loss: 2.4881 - val_accuracy: 0.5284 - 99s/epoch - 55ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 99s - loss: 1.4862 - accuracy: 0.5687 - val_loss: 3.1226 - val_accuracy: 0.4710 - 99s/epoch - 55ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 99s - loss: 1.4460 - accuracy: 0.5825 - val_loss: 4.4008 - val_accuracy: 0.4173 - 99s/epoch - 54ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 100s - loss: 1.4243 - accuracy: 0.5884 - val_loss: 4.9663 - val_accuracy: 0.3601 - 100s/epoch - 55ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 94s - loss: 3.0813 - accuracy: 0.1578 - val_loss: 3.2880 - val_accuracy: 0.2062 - 94s/epoch - 52ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 83s - loss: 2.1317 - accuracy: 0.3713 - val_loss: 2.3476 - val_accuracy: 0.3644 - 83s/epoch - 46ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 83s - loss: 1.8721 - accuracy: 0.4501 - val_loss: 2.0342 - val_accuracy: 0.4794 - 83s/epoch - 46ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 82s - loss: 1.7157 - accuracy: 0.4986 - val_loss: 2.2179 - val_accuracy: 0.4548 - 82s/epoch - 45ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 83s - loss: 1.5992 - accuracy: 0.5313 - val_loss: 2.2584 - val_accuracy: 0.4891 - 83s/epoch - 46ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 82s - loss: 1.5171 - accuracy: 0.5571 - val_loss: 1.8729 - val_accuracy: 0.5357 - 82s/epoch - 45ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 83s - loss: 1.4528 - accuracy: 0.5796 - val_loss: 1.6923 - val_accuracy: 0.5993 - 83s/epoch - 46ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 84s - loss: 1.3931 - accuracy: 0.5973 - val_loss: 2.0158 - val_accuracy: 0.5949 - 84s/epoch - 46ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 84s - loss: 1.3533 - accuracy: 0.6103 - val_loss: 2.3544 - val_accuracy: 0.5563 - 84s/epoch - 46ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 83s - loss: 1.2931 - accuracy: 0.6259 - val_loss: 1.9286 - val_accuracy: 0.6159 - 83s/epoch - 46ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 83s - loss: 1.2578 - accuracy: 0.6364 - val_loss: 1.9597 - val_accuracy: 0.6339 - 83s/epoch - 46ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 83s - loss: 1.2317 - accuracy: 0.6415 - val_loss: 2.5244 - val_accuracy: 0.5783 - 83s/epoch - 46ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 83s - loss: 1.1952 - accuracy: 0.6567 - val_loss: 2.8713 - val_accuracy: 0.5465 - 83s/epoch - 46ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 83s - loss: 1.2054 - accuracy: 0.6555 - val_loss: 3.0115 - val_accuracy: 0.5147 - 83s/epoch - 46ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 83s - loss: 1.1550 - accuracy: 0.6689 - val_loss: 3.0945 - val_accuracy: 0.5024 - 83s/epoch - 46ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 83s - loss: 1.1417 - accuracy: 0.6716 - val_loss: 3.5205 - val_accuracy: 0.4940 - 83s/epoch - 46ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 86s - loss: 1.1388 - accuracy: 0.6729 - val_loss: 4.2846 - val_accuracy: 0.4020 - 86s/epoch - 48ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 86s - loss: 1.1270 - accuracy: 0.6772 - val_loss: 4.3633 - val_accuracy: 0.4061 - 86s/epoch - 47ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 86s - loss: 1.1022 - accuracy: 0.6836 - val_loss: 6.1380 - val_accuracy: 0.3235 - 86s/epoch - 48ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 85s - loss: 1.1654 - accuracy: 0.6679 - val_loss: 5.6617 - val_accuracy: 0.3666 - 85s/epoch - 47ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 91s - loss: 3.1123 - accuracy: 0.1646 - val_loss: 2.4138 - val_accuracy: 0.3257 - 91s/epoch - 100ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 84s - loss: 2.1049 - accuracy: 0.3760 - val_loss: 1.8780 - val_accuracy: 0.4726 - 84s/epoch - 92ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 84s - loss: 1.7657 - accuracy: 0.4753 - val_loss: 1.7721 - val_accuracy: 0.5051 - 84s/epoch - 93ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 84s - loss: 1.6150 - accuracy: 0.5218 - val_loss: 1.4239 - val_accuracy: 0.5933 - 84s/epoch - 92ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.5067 - accuracy: 0.5527 - val_loss: 1.5120 - val_accuracy: 0.5716 - 83s/epoch - 92ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 84s - loss: 1.4321 - accuracy: 0.5781 - val_loss: 1.2837 - val_accuracy: 0.6364 - 84s/epoch - 93ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 84s - loss: 1.3546 - accuracy: 0.6006 - val_loss: 1.2469 - val_accuracy: 0.6453 - 84s/epoch - 93ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 84s - loss: 1.3080 - accuracy: 0.6120 - val_loss: 1.2488 - val_accuracy: 0.6337 - 84s/epoch - 93ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 85s - loss: 1.2928 - accuracy: 0.6189 - val_loss: 1.2006 - val_accuracy: 0.6581 - 85s/epoch - 94ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 85s - loss: 1.2488 - accuracy: 0.6341 - val_loss: 1.1134 - val_accuracy: 0.6762 - 85s/epoch - 94ms/step\n",
      "num_heads: 2, num_layers: 2, dropout_rate: 0.4, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 156s - loss: 3.1456 - accuracy: 0.1565 - val_loss: 1.8228 - val_accuracy: 0.4700 - 156s/epoch - 172ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 142s - loss: 1.9936 - accuracy: 0.4098 - val_loss: 2.1167 - val_accuracy: 0.4406 - 142s/epoch - 157ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 142s - loss: 1.7639 - accuracy: 0.4768 - val_loss: 1.8694 - val_accuracy: 0.4994 - 142s/epoch - 156ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 143s - loss: 1.5857 - accuracy: 0.5288 - val_loss: 1.8316 - val_accuracy: 0.5278 - 143s/epoch - 157ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 145s - loss: 1.4678 - accuracy: 0.5672 - val_loss: 1.7980 - val_accuracy: 0.5446 - 145s/epoch - 160ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 144s - loss: 1.3794 - accuracy: 0.5925 - val_loss: 1.5471 - val_accuracy: 0.5908 - 144s/epoch - 159ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 145s - loss: 1.3246 - accuracy: 0.6120 - val_loss: 1.3590 - val_accuracy: 0.6355 - 145s/epoch - 160ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 145s - loss: 1.2829 - accuracy: 0.6223 - val_loss: 1.3267 - val_accuracy: 0.6478 - 145s/epoch - 160ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 145s - loss: 1.2494 - accuracy: 0.6312 - val_loss: 1.4039 - val_accuracy: 0.6322 - 145s/epoch - 160ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 146s - loss: 1.2262 - accuracy: 0.6406 - val_loss: 1.3439 - val_accuracy: 0.6400 - 146s/epoch - 161ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 143s - loss: 1.1947 - accuracy: 0.6506 - val_loss: 1.3316 - val_accuracy: 0.6648 - 143s/epoch - 158ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 146s - loss: 1.1621 - accuracy: 0.6603 - val_loss: 1.3027 - val_accuracy: 0.6598 - 146s/epoch - 161ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 144s - loss: 1.1500 - accuracy: 0.6644 - val_loss: 1.2143 - val_accuracy: 0.6842 - 144s/epoch - 159ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 144s - loss: 1.1411 - accuracy: 0.6677 - val_loss: 1.1371 - val_accuracy: 0.7079 - 144s/epoch - 159ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 144s - loss: 1.0896 - accuracy: 0.6828 - val_loss: 1.2513 - val_accuracy: 0.6977 - 144s/epoch - 159ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 140s - loss: 1.0583 - accuracy: 0.6912 - val_loss: 1.2410 - val_accuracy: 0.7005 - 140s/epoch - 154ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 142s - loss: 1.0568 - accuracy: 0.6904 - val_loss: 1.4178 - val_accuracy: 0.6845 - 142s/epoch - 157ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 141s - loss: 1.0535 - accuracy: 0.6928 - val_loss: 1.2665 - val_accuracy: 0.6883 - 141s/epoch - 156ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 140s - loss: 1.0699 - accuracy: 0.6881 - val_loss: 1.1796 - val_accuracy: 0.7157 - 140s/epoch - 155ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 140s - loss: 1.0292 - accuracy: 0.6980 - val_loss: 1.1563 - val_accuracy: 0.7167 - 140s/epoch - 154ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 116s - loss: 2.0913 - accuracy: 0.4069 - val_loss: 1.7231 - val_accuracy: 0.4875 - 116s/epoch - 64ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 110s - loss: 1.4050 - accuracy: 0.5861 - val_loss: 1.1407 - val_accuracy: 0.6573 - 110s/epoch - 61ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 112s - loss: 1.2536 - accuracy: 0.6258 - val_loss: 0.9992 - val_accuracy: 0.7074 - 112s/epoch - 62ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 111s - loss: 1.2020 - accuracy: 0.6442 - val_loss: 0.8997 - val_accuracy: 0.7268 - 111s/epoch - 61ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 110s - loss: 1.1406 - accuracy: 0.6608 - val_loss: 0.8691 - val_accuracy: 0.7342 - 110s/epoch - 61ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 112s - loss: 1.0528 - accuracy: 0.6893 - val_loss: 0.8170 - val_accuracy: 0.7496 - 112s/epoch - 62ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 111s - loss: 1.0217 - accuracy: 0.6946 - val_loss: 0.8714 - val_accuracy: 0.7292 - 111s/epoch - 61ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 110s - loss: 1.0073 - accuracy: 0.7004 - val_loss: 0.7972 - val_accuracy: 0.7583 - 110s/epoch - 61ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 111s - loss: 0.9558 - accuracy: 0.7143 - val_loss: 0.7493 - val_accuracy: 0.7680 - 111s/epoch - 61ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 111s - loss: 0.9411 - accuracy: 0.7189 - val_loss: 0.8242 - val_accuracy: 0.7474 - 111s/epoch - 61ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 108s - loss: 2.0844 - accuracy: 0.4110 - val_loss: 1.3543 - val_accuracy: 0.5866 - 108s/epoch - 60ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 102s - loss: 1.3399 - accuracy: 0.6043 - val_loss: 1.0398 - val_accuracy: 0.6852 - 102s/epoch - 56ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 103s - loss: 1.1925 - accuracy: 0.6460 - val_loss: 0.9425 - val_accuracy: 0.7124 - 103s/epoch - 57ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 104s - loss: 1.0934 - accuracy: 0.6750 - val_loss: 0.9392 - val_accuracy: 0.7142 - 104s/epoch - 58ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 102s - loss: 1.0317 - accuracy: 0.6914 - val_loss: 0.9200 - val_accuracy: 0.7171 - 102s/epoch - 56ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 110s - loss: 0.9809 - accuracy: 0.7061 - val_loss: 0.8001 - val_accuracy: 0.7563 - 110s/epoch - 61ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 109s - loss: 0.9275 - accuracy: 0.7221 - val_loss: 0.7608 - val_accuracy: 0.7676 - 109s/epoch - 60ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 111s - loss: 0.9197 - accuracy: 0.7261 - val_loss: 0.7782 - val_accuracy: 0.7632 - 111s/epoch - 61ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 109s - loss: 0.8878 - accuracy: 0.7348 - val_loss: 0.7798 - val_accuracy: 0.7523 - 109s/epoch - 60ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 112s - loss: 0.8541 - accuracy: 0.7459 - val_loss: 0.7193 - val_accuracy: 0.7788 - 112s/epoch - 62ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 109s - loss: 0.8355 - accuracy: 0.7503 - val_loss: 0.6982 - val_accuracy: 0.7876 - 109s/epoch - 60ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 109s - loss: 0.8194 - accuracy: 0.7553 - val_loss: 0.7066 - val_accuracy: 0.7852 - 109s/epoch - 60ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 112s - loss: 0.7894 - accuracy: 0.7653 - val_loss: 0.7040 - val_accuracy: 0.7842 - 112s/epoch - 62ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 109s - loss: 0.7944 - accuracy: 0.7629 - val_loss: 0.6959 - val_accuracy: 0.7896 - 109s/epoch - 60ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 111s - loss: 0.7945 - accuracy: 0.7625 - val_loss: 0.6345 - val_accuracy: 0.8132 - 111s/epoch - 61ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 109s - loss: 0.7555 - accuracy: 0.7728 - val_loss: 0.6470 - val_accuracy: 0.8070 - 109s/epoch - 60ms/step\n",
      "Epoch 17/20\n",
      "1811/1811 - 111s - loss: 0.7824 - accuracy: 0.7666 - val_loss: 0.6245 - val_accuracy: 0.8071 - 111s/epoch - 61ms/step\n",
      "Epoch 18/20\n",
      "1811/1811 - 111s - loss: 0.7727 - accuracy: 0.7673 - val_loss: 0.6770 - val_accuracy: 0.7971 - 111s/epoch - 61ms/step\n",
      "Epoch 19/20\n",
      "1811/1811 - 110s - loss: 0.7437 - accuracy: 0.7755 - val_loss: 0.6581 - val_accuracy: 0.7992 - 110s/epoch - 60ms/step\n",
      "Epoch 20/20\n",
      "1811/1811 - 112s - loss: 0.7234 - accuracy: 0.7852 - val_loss: 0.6973 - val_accuracy: 0.7870 - 112s/epoch - 62ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "906/906 - 91s - loss: 2.1364 - accuracy: 0.4027 - val_loss: 1.1991 - val_accuracy: 0.6402 - 91s/epoch - 100ms/step\n",
      "Epoch 2/10\n",
      "906/906 - 82s - loss: 1.3895 - accuracy: 0.5904 - val_loss: 1.0185 - val_accuracy: 0.6884 - 82s/epoch - 90ms/step\n",
      "Epoch 3/10\n",
      "906/906 - 79s - loss: 1.1817 - accuracy: 0.6511 - val_loss: 0.9145 - val_accuracy: 0.7164 - 79s/epoch - 88ms/step\n",
      "Epoch 4/10\n",
      "906/906 - 81s - loss: 1.0897 - accuracy: 0.6749 - val_loss: 0.8276 - val_accuracy: 0.7435 - 81s/epoch - 89ms/step\n",
      "Epoch 5/10\n",
      "906/906 - 83s - loss: 1.0082 - accuracy: 0.6996 - val_loss: 0.7773 - val_accuracy: 0.7643 - 83s/epoch - 91ms/step\n",
      "Epoch 6/10\n",
      "906/906 - 77s - loss: 0.9596 - accuracy: 0.7139 - val_loss: 0.7566 - val_accuracy: 0.7724 - 77s/epoch - 85ms/step\n",
      "Epoch 7/10\n",
      "906/906 - 80s - loss: 0.9627 - accuracy: 0.7117 - val_loss: 0.7643 - val_accuracy: 0.7661 - 80s/epoch - 88ms/step\n",
      "Epoch 8/10\n",
      "906/906 - 80s - loss: 0.8922 - accuracy: 0.7325 - val_loss: 0.7254 - val_accuracy: 0.7779 - 80s/epoch - 88ms/step\n",
      "Epoch 9/10\n",
      "906/906 - 77s - loss: 0.8604 - accuracy: 0.7412 - val_loss: 0.6892 - val_accuracy: 0.7901 - 77s/epoch - 85ms/step\n",
      "Epoch 10/10\n",
      "906/906 - 80s - loss: 0.8258 - accuracy: 0.7518 - val_loss: 0.6621 - val_accuracy: 0.7967 - 80s/epoch - 88ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.2, batch_size: 64, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "906/906 - 82s - loss: 2.1200 - accuracy: 0.4042 - val_loss: 1.3426 - val_accuracy: 0.6040 - 82s/epoch - 90ms/step\n",
      "Epoch 2/20\n",
      "906/906 - 78s - loss: 1.3128 - accuracy: 0.6113 - val_loss: 1.0047 - val_accuracy: 0.6995 - 78s/epoch - 86ms/step\n",
      "Epoch 3/20\n",
      "906/906 - 78s - loss: 1.1334 - accuracy: 0.6615 - val_loss: 0.9386 - val_accuracy: 0.7204 - 78s/epoch - 86ms/step\n",
      "Epoch 4/20\n",
      "906/906 - 74s - loss: 1.0405 - accuracy: 0.6899 - val_loss: 0.8517 - val_accuracy: 0.7457 - 74s/epoch - 82ms/step\n",
      "Epoch 5/20\n",
      "906/906 - 78s - loss: 1.0004 - accuracy: 0.7024 - val_loss: 0.7929 - val_accuracy: 0.7596 - 78s/epoch - 86ms/step\n",
      "Epoch 6/20\n",
      "906/906 - 79s - loss: 0.9272 - accuracy: 0.7219 - val_loss: 0.8056 - val_accuracy: 0.7515 - 79s/epoch - 87ms/step\n",
      "Epoch 7/20\n",
      "906/906 - 75s - loss: 0.9008 - accuracy: 0.7295 - val_loss: 0.7642 - val_accuracy: 0.7624 - 75s/epoch - 83ms/step\n",
      "Epoch 8/20\n",
      "906/906 - 78s - loss: 0.8635 - accuracy: 0.7394 - val_loss: 0.7446 - val_accuracy: 0.7729 - 78s/epoch - 86ms/step\n",
      "Epoch 9/20\n",
      "906/906 - 78s - loss: 0.8480 - accuracy: 0.7444 - val_loss: 0.7687 - val_accuracy: 0.7665 - 78s/epoch - 86ms/step\n",
      "Epoch 10/20\n",
      "906/906 - 75s - loss: 0.8852 - accuracy: 0.7358 - val_loss: 0.6602 - val_accuracy: 0.7988 - 75s/epoch - 83ms/step\n",
      "Epoch 11/20\n",
      "906/906 - 78s - loss: 0.7884 - accuracy: 0.7633 - val_loss: 0.6625 - val_accuracy: 0.7974 - 78s/epoch - 86ms/step\n",
      "Epoch 12/20\n",
      "906/906 - 78s - loss: 0.7569 - accuracy: 0.7724 - val_loss: 0.6631 - val_accuracy: 0.7983 - 78s/epoch - 86ms/step\n",
      "Epoch 13/20\n",
      "906/906 - 77s - loss: 0.7507 - accuracy: 0.7730 - val_loss: 0.6863 - val_accuracy: 0.7888 - 77s/epoch - 86ms/step\n",
      "Epoch 14/20\n",
      "906/906 - 79s - loss: 0.7365 - accuracy: 0.7759 - val_loss: 0.6411 - val_accuracy: 0.8049 - 79s/epoch - 88ms/step\n",
      "Epoch 15/20\n",
      "906/906 - 77s - loss: 0.7231 - accuracy: 0.7824 - val_loss: 0.6658 - val_accuracy: 0.7970 - 77s/epoch - 85ms/step\n",
      "Epoch 16/20\n",
      "906/906 - 79s - loss: 0.8101 - accuracy: 0.7572 - val_loss: 0.6893 - val_accuracy: 0.7891 - 79s/epoch - 87ms/step\n",
      "Epoch 17/20\n",
      "906/906 - 78s - loss: 0.7076 - accuracy: 0.7870 - val_loss: 0.6175 - val_accuracy: 0.8071 - 78s/epoch - 86ms/step\n",
      "Epoch 18/20\n",
      "906/906 - 76s - loss: 0.7199 - accuracy: 0.7820 - val_loss: 0.6488 - val_accuracy: 0.8071 - 76s/epoch - 84ms/step\n",
      "Epoch 19/20\n",
      "906/906 - 78s - loss: 0.6955 - accuracy: 0.7907 - val_loss: 0.6826 - val_accuracy: 0.7908 - 78s/epoch - 86ms/step\n",
      "Epoch 20/20\n",
      "906/906 - 77s - loss: 0.6901 - accuracy: 0.7900 - val_loss: 0.6010 - val_accuracy: 0.8129 - 77s/epoch - 85ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1811/1811 - 145s - loss: 3.0230 - accuracy: 0.1724 - val_loss: 2.2363 - val_accuracy: 0.3248 - 145s/epoch - 80ms/step\n",
      "Epoch 2/10\n",
      "1811/1811 - 144s - loss: 2.1690 - accuracy: 0.3603 - val_loss: 1.7557 - val_accuracy: 0.4821 - 144s/epoch - 79ms/step\n",
      "Epoch 3/10\n",
      "1811/1811 - 146s - loss: 1.8697 - accuracy: 0.4496 - val_loss: 1.5132 - val_accuracy: 0.5363 - 146s/epoch - 81ms/step\n",
      "Epoch 4/10\n",
      "1811/1811 - 147s - loss: 1.7377 - accuracy: 0.4873 - val_loss: 1.3509 - val_accuracy: 0.5971 - 147s/epoch - 81ms/step\n",
      "Epoch 5/10\n",
      "1811/1811 - 137s - loss: 1.6301 - accuracy: 0.5221 - val_loss: 1.2486 - val_accuracy: 0.6305 - 137s/epoch - 75ms/step\n",
      "Epoch 6/10\n",
      "1811/1811 - 146s - loss: 1.5492 - accuracy: 0.5439 - val_loss: 1.1941 - val_accuracy: 0.6427 - 146s/epoch - 81ms/step\n",
      "Epoch 7/10\n",
      "1811/1811 - 145s - loss: 1.4713 - accuracy: 0.5674 - val_loss: 1.0975 - val_accuracy: 0.6752 - 145s/epoch - 80ms/step\n",
      "Epoch 8/10\n",
      "1811/1811 - 144s - loss: 1.4047 - accuracy: 0.5864 - val_loss: 1.1295 - val_accuracy: 0.6639 - 144s/epoch - 80ms/step\n",
      "Epoch 9/10\n",
      "1811/1811 - 147s - loss: 1.3826 - accuracy: 0.5951 - val_loss: 1.0712 - val_accuracy: 0.6842 - 147s/epoch - 81ms/step\n",
      "Epoch 10/10\n",
      "1811/1811 - 139s - loss: 1.3538 - accuracy: 0.6026 - val_loss: 1.0238 - val_accuracy: 0.6970 - 139s/epoch - 77ms/step\n",
      "num_heads: 4, num_layers: 1, dropout_rate: 0.4, batch_size: 32, epochs: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\AppData\\Local\\Temp\\ipykernel_16556\\2367907417.py:87: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results = results.append(row, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1811/1811 - 143s - loss: 3.0259 - accuracy: 0.1751 - val_loss: 2.1750 - val_accuracy: 0.3420 - 143s/epoch - 79ms/step\n",
      "Epoch 2/20\n",
      "1811/1811 - 133s - loss: 2.1249 - accuracy: 0.3700 - val_loss: 1.6614 - val_accuracy: 0.4957 - 133s/epoch - 74ms/step\n",
      "Epoch 3/20\n",
      "1811/1811 - 132s - loss: 1.9152 - accuracy: 0.4344 - val_loss: 1.4920 - val_accuracy: 0.5559 - 132s/epoch - 73ms/step\n",
      "Epoch 4/20\n",
      "1811/1811 - 132s - loss: 1.7713 - accuracy: 0.4804 - val_loss: 1.3447 - val_accuracy: 0.6025 - 132s/epoch - 73ms/step\n",
      "Epoch 5/20\n",
      "1811/1811 - 125s - loss: 1.6620 - accuracy: 0.5112 - val_loss: 1.3313 - val_accuracy: 0.5989 - 125s/epoch - 69ms/step\n",
      "Epoch 6/20\n",
      "1811/1811 - 133s - loss: 1.5956 - accuracy: 0.5324 - val_loss: 1.2086 - val_accuracy: 0.6365 - 133s/epoch - 73ms/step\n",
      "Epoch 7/20\n",
      "1811/1811 - 132s - loss: 1.5860 - accuracy: 0.5338 - val_loss: 1.2962 - val_accuracy: 0.6162 - 132s/epoch - 73ms/step\n",
      "Epoch 8/20\n",
      "1811/1811 - 133s - loss: 1.5435 - accuracy: 0.5471 - val_loss: 1.2003 - val_accuracy: 0.6392 - 133s/epoch - 73ms/step\n",
      "Epoch 9/20\n",
      "1811/1811 - 131s - loss: 1.4947 - accuracy: 0.5631 - val_loss: 1.1578 - val_accuracy: 0.6558 - 131s/epoch - 73ms/step\n",
      "Epoch 10/20\n",
      "1811/1811 - 125s - loss: 1.4756 - accuracy: 0.5685 - val_loss: 1.1465 - val_accuracy: 0.6583 - 125s/epoch - 69ms/step\n",
      "Epoch 11/20\n",
      "1811/1811 - 132s - loss: 1.4358 - accuracy: 0.5818 - val_loss: 1.1074 - val_accuracy: 0.6671 - 132s/epoch - 73ms/step\n",
      "Epoch 12/20\n",
      "1811/1811 - 134s - loss: 1.4027 - accuracy: 0.5900 - val_loss: 1.0852 - val_accuracy: 0.6767 - 134s/epoch - 74ms/step\n",
      "Epoch 13/20\n",
      "1811/1811 - 133s - loss: 1.3661 - accuracy: 0.6016 - val_loss: 1.1139 - val_accuracy: 0.6646 - 133s/epoch - 73ms/step\n",
      "Epoch 14/20\n",
      "1811/1811 - 131s - loss: 1.3677 - accuracy: 0.6011 - val_loss: 1.0974 - val_accuracy: 0.6667 - 131s/epoch - 72ms/step\n",
      "Epoch 15/20\n",
      "1811/1811 - 116s - loss: 1.3811 - accuracy: 0.5963 - val_loss: 1.0212 - val_accuracy: 0.6929 - 116s/epoch - 64ms/step\n",
      "Epoch 16/20\n",
      "1811/1811 - 92s - loss: 1.3347 - accuracy: 0.6102 - val_loss: 1.0739 - val_accuracy: 0.6851 - 92s/epoch - 51ms/step\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\deep\\Deep_Learning\\RNN\\models_transformer.ipynb Cell 4\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(), loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Train the model with your training set and validate it with your validation set\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train\u001b[39m.\u001b[39;49mbatch(batch_size), epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size, validation_data\u001b[39m=\u001b[39;49mval\u001b[39m.\u001b[39;49mbatch(batch_size), verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m                 \u001b[39m# Record the results for the current configuration\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m row \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_heads\u001b[39m\u001b[39m'\u001b[39m: num_heads,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m: num_layers,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mval_accuracy_max\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mmax(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deep/Deep_Learning/RNN/models_transformer.ipynb#W3sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (MultiHeadAttention, Dropout, LayerNormalization, Dense, TimeDistributed,\n",
    "                                     BatchNormalization, Conv1D, MaxPooling1D, Flatten, GlobalMaxPooling1D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "results = pd.DataFrame(columns=['num_heads', 'num_layers', 'dropout_rate', 'epoch', 'batch', 'loss', 'loss_max', 'accuracy', 'accuracy_max', 'val_loss', 'val_loss_max', 'val_accuracy', 'val_accuracy_max'])\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential([\n",
    "            Dense(embed_dim, activation='relu'),\n",
    "            Dense(embed_dim)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "for num_heads in [2, 4]:\n",
    "    for num_layers in [1, 2]:\n",
    "        for dropout_rate in [0.2, 0.4]:\n",
    "            for batch_size in [32, 64]:\n",
    "                for epochs in [ 10, 20]:\n",
    "                    print(f'num_heads: {num_heads}, num_layers: {num_layers}, dropout_rate: {dropout_rate}, batch_size: {batch_size}, epochs: {epochs}')\n",
    "                    num_classes = len(np.unique(labels))  # Number of unique classes in your dataset\n",
    "                    input_shape = (39, 44)\n",
    "                    embed_dim = 128\n",
    "\n",
    "                    inputs = tf.keras.Input(shape=input_shape)\n",
    "                    x = Conv1D(filters=128, kernel_size=3, activation='relu')(inputs)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = MaxPooling1D(pool_size=2)(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "                    for _ in range(num_layers):\n",
    "                        x = TransformerBlock(embed_dim, num_heads, dropout_rate)(x)\n",
    "\n",
    "                    x = GlobalMaxPooling1D()(x)\n",
    "                    x = Dense(512, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(256, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    x = Dense(128, activation='relu')(x)\n",
    "                    x = BatchNormalization()(x)\n",
    "                    x = Dropout(dropout_rate)(x)\n",
    "                    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "                    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "                    # Compile the model\n",
    "                    model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "                    # Train the model with your training set and validate it with your validation set\n",
    "                    history = model.fit(train.batch(batch_size), epochs=epochs, batch_size=batch_size, validation_data=val.batch(batch_size), verbose=2)\n",
    "                                    # Record the results for the current configuration\n",
    "                    row = {\n",
    "                        'num_heads': num_heads,\n",
    "                        'num_layers': num_layers,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'epoch': epochs,\n",
    "                        'batch': batch_size,\n",
    "                        'loss': history.history['loss'][-1],\n",
    "                        'loss_max': np.max(history.history['loss']),\n",
    "                        'accuracy': history.history['accuracy'][-1],\n",
    "                        'accuracy_max': np.max(history.history['accuracy']),\n",
    "                        'val_loss': history.history['val_loss'][-1],\n",
    "                        'val_loss_max': np.max(history.history['val_loss']),\n",
    "                        'val_accuracy': history.history['val_accuracy'][-1],\n",
    "                        'val_accuracy_max': np.max(history.history['val_accuracy']),\n",
    "                    }\n",
    "\n",
    "                    results = results.append(row, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_heads</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch</th>\n",
       "      <th>loss</th>\n",
       "      <th>loss_max</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accuracy_max</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_loss_max</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_accuracy_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.020137</td>\n",
       "      <td>2.234147</td>\n",
       "      <td>0.694923</td>\n",
       "      <td>0.695544</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>1.408424</td>\n",
       "      <td>0.765225</td>\n",
       "      <td>0.765225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.835371</td>\n",
       "      <td>2.106825</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.752862</td>\n",
       "      <td>0.670023</td>\n",
       "      <td>1.220319</td>\n",
       "      <td>0.798176</td>\n",
       "      <td>0.798176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.836061</td>\n",
       "      <td>2.168659</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.748770</td>\n",
       "      <td>0.697845</td>\n",
       "      <td>1.250656</td>\n",
       "      <td>0.782142</td>\n",
       "      <td>0.782142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.714789</td>\n",
       "      <td>2.103729</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.783126</td>\n",
       "      <td>0.629499</td>\n",
       "      <td>1.196326</td>\n",
       "      <td>0.812298</td>\n",
       "      <td>0.820683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.258716</td>\n",
       "      <td>2.888041</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>0.630682</td>\n",
       "      <td>1.075352</td>\n",
       "      <td>1.915189</td>\n",
       "      <td>0.681083</td>\n",
       "      <td>0.694322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.282760</td>\n",
       "      <td>2.965580</td>\n",
       "      <td>0.632391</td>\n",
       "      <td>0.641438</td>\n",
       "      <td>1.102142</td>\n",
       "      <td>1.910666</td>\n",
       "      <td>0.684025</td>\n",
       "      <td>0.708444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.257138</td>\n",
       "      <td>3.018563</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.627350</td>\n",
       "      <td>0.929122</td>\n",
       "      <td>1.939173</td>\n",
       "      <td>0.722124</td>\n",
       "      <td>0.722124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.190626</td>\n",
       "      <td>3.028895</td>\n",
       "      <td>0.648482</td>\n",
       "      <td>0.671771</td>\n",
       "      <td>0.864657</td>\n",
       "      <td>1.711817</td>\n",
       "      <td>0.743307</td>\n",
       "      <td>0.754340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>2.257917</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.715571</td>\n",
       "      <td>0.818004</td>\n",
       "      <td>1.401505</td>\n",
       "      <td>0.757134</td>\n",
       "      <td>0.758753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.772999</td>\n",
       "      <td>2.242363</td>\n",
       "      <td>0.769142</td>\n",
       "      <td>0.770644</td>\n",
       "      <td>0.697234</td>\n",
       "      <td>1.372575</td>\n",
       "      <td>0.789938</td>\n",
       "      <td>0.798029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.864500</td>\n",
       "      <td>2.232953</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.741070</td>\n",
       "      <td>0.686920</td>\n",
       "      <td>1.369754</td>\n",
       "      <td>0.789056</td>\n",
       "      <td>0.789056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.765530</td>\n",
       "      <td>2.189961</td>\n",
       "      <td>0.771058</td>\n",
       "      <td>0.783022</td>\n",
       "      <td>0.631657</td>\n",
       "      <td>1.512975</td>\n",
       "      <td>0.808179</td>\n",
       "      <td>0.808620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.424297</td>\n",
       "      <td>3.053465</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>0.588350</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>4.966305</td>\n",
       "      <td>0.360106</td>\n",
       "      <td>0.528391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.165354</td>\n",
       "      <td>3.081251</td>\n",
       "      <td>0.667938</td>\n",
       "      <td>0.683632</td>\n",
       "      <td>5.661652</td>\n",
       "      <td>6.137986</td>\n",
       "      <td>0.366578</td>\n",
       "      <td>0.633863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.248828</td>\n",
       "      <td>3.112274</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>0.634100</td>\n",
       "      <td>1.113379</td>\n",
       "      <td>2.413799</td>\n",
       "      <td>0.676228</td>\n",
       "      <td>0.676228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.029160</td>\n",
       "      <td>3.145577</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>0.698030</td>\n",
       "      <td>1.156267</td>\n",
       "      <td>2.116711</td>\n",
       "      <td>0.716681</td>\n",
       "      <td>0.716681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.941052</td>\n",
       "      <td>2.091297</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.718885</td>\n",
       "      <td>0.824226</td>\n",
       "      <td>1.723114</td>\n",
       "      <td>0.747426</td>\n",
       "      <td>0.768020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.723400</td>\n",
       "      <td>2.084395</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.785215</td>\n",
       "      <td>0.697317</td>\n",
       "      <td>1.354331</td>\n",
       "      <td>0.786996</td>\n",
       "      <td>0.813180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.825751</td>\n",
       "      <td>2.136443</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.751774</td>\n",
       "      <td>0.662067</td>\n",
       "      <td>1.199146</td>\n",
       "      <td>0.796705</td>\n",
       "      <td>0.796705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.690125</td>\n",
       "      <td>2.120008</td>\n",
       "      <td>0.790032</td>\n",
       "      <td>0.790722</td>\n",
       "      <td>0.600954</td>\n",
       "      <td>1.342593</td>\n",
       "      <td>0.812886</td>\n",
       "      <td>0.812886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.353759</td>\n",
       "      <td>3.022973</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>0.602645</td>\n",
       "      <td>1.023833</td>\n",
       "      <td>2.236273</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.696970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    num_heads  num_layers  dropout_rate  epoch  batch      loss  loss_max  \\\n",
       "0         2.0         1.0           0.2   10.0   32.0  1.020137  2.234147   \n",
       "1         2.0         1.0           0.2   20.0   32.0  0.835371  2.106825   \n",
       "2         2.0         1.0           0.2   10.0   64.0  0.836061  2.168659   \n",
       "3         2.0         1.0           0.2   20.0   64.0  0.714789  2.103729   \n",
       "4         2.0         1.0           0.4   10.0   32.0  1.258716  2.888041   \n",
       "5         2.0         1.0           0.4   20.0   32.0  1.282760  2.965580   \n",
       "6         2.0         1.0           0.4   10.0   64.0  1.257138  3.018563   \n",
       "7         2.0         1.0           0.4   20.0   64.0  1.190626  3.028895   \n",
       "8         2.0         2.0           0.2   10.0   32.0  0.954365  2.257917   \n",
       "9         2.0         2.0           0.2   20.0   32.0  0.772999  2.242363   \n",
       "10        2.0         2.0           0.2   10.0   64.0  0.864500  2.232953   \n",
       "11        2.0         2.0           0.2   20.0   64.0  0.765530  2.189961   \n",
       "12        2.0         2.0           0.4   10.0   32.0  1.424297  3.053465   \n",
       "13        2.0         2.0           0.4   20.0   32.0  1.165354  3.081251   \n",
       "14        2.0         2.0           0.4   10.0   64.0  1.248828  3.112274   \n",
       "15        2.0         2.0           0.4   20.0   64.0  1.029160  3.145577   \n",
       "16        4.0         1.0           0.2   10.0   32.0  0.941052  2.091297   \n",
       "17        4.0         1.0           0.2   20.0   32.0  0.723400  2.084395   \n",
       "18        4.0         1.0           0.2   10.0   64.0  0.825751  2.136443   \n",
       "19        4.0         1.0           0.2   20.0   64.0  0.690125  2.120008   \n",
       "20        4.0         1.0           0.4   10.0   32.0  1.353759  3.022973   \n",
       "\n",
       "    accuracy  accuracy_max  val_loss  val_loss_max  val_accuracy  \\\n",
       "0   0.694923      0.695544  0.749380      1.408424      0.765225   \n",
       "1   0.752862      0.752862  0.670023      1.220319      0.798176   \n",
       "2   0.748770      0.748770  0.697845      1.250656      0.782142   \n",
       "3   0.783126      0.783126  0.629499      1.196326      0.812298   \n",
       "4   0.630682      0.630682  1.075352      1.915189      0.681083   \n",
       "5   0.632391      0.641438  1.102142      1.910666      0.684025   \n",
       "6   0.627350      0.627350  0.929122      1.939173      0.722124   \n",
       "7   0.648482      0.671771  0.864657      1.711817      0.743307   \n",
       "8   0.715571      0.715571  0.818004      1.401505      0.757134   \n",
       "9   0.769142      0.770644  0.697234      1.372575      0.789938   \n",
       "10  0.741070      0.741070  0.686920      1.369754      0.789056   \n",
       "11  0.771058      0.783022  0.631657      1.512975      0.808179   \n",
       "12  0.588350      0.588350  4.966305      4.966305      0.360106   \n",
       "13  0.667938      0.683632  5.661652      6.137986      0.366578   \n",
       "14  0.634100      0.634100  1.113379      2.413799      0.676228   \n",
       "15  0.698030      0.698030  1.156267      2.116711      0.716681   \n",
       "16  0.718885      0.718885  0.824226      1.723114      0.747426   \n",
       "17  0.785215      0.785215  0.697317      1.354331      0.786996   \n",
       "18  0.751774      0.751774  0.662067      1.199146      0.796705   \n",
       "19  0.790032      0.790722  0.600954      1.342593      0.812886   \n",
       "20  0.602645      0.602645  1.023833      2.236273      0.696970   \n",
       "\n",
       "    val_accuracy_max  \n",
       "0           0.765225  \n",
       "1           0.798176  \n",
       "2           0.782142  \n",
       "3           0.820683  \n",
       "4           0.694322  \n",
       "5           0.708444  \n",
       "6           0.722124  \n",
       "7           0.754340  \n",
       "8           0.758753  \n",
       "9           0.798029  \n",
       "10          0.789056  \n",
       "11          0.808620  \n",
       "12          0.528391  \n",
       "13          0.633863  \n",
       "14          0.676228  \n",
       "15          0.716681  \n",
       "16          0.768020  \n",
       "17          0.813180  \n",
       "18          0.796705  \n",
       "19          0.812886  \n",
       "20          0.696970  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_pickle('results\\\\model_transformer_final_version.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
