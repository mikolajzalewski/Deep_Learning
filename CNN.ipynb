{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datasets as ds_own\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork():\n",
    "    \n",
    "    def train_step(self, data, optimizer, criterion):\n",
    "        x, y = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy = (logits.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "    \n",
    "    def test_step(self, data, criterion):\n",
    "        x, y = data\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = criterion(logits, y)\n",
    "        accuracy = (logits.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "        return {'loss': loss, 'accuracy': accuracy}\n",
    "    \n",
    "    def Conv2d_output_size(self, w, k, s, p):\n",
    "        '''\n",
    "        w - width of input image\n",
    "        k - kernel size\n",
    "        s - stride\n",
    "        p - padding\n",
    "        '''\n",
    "        return np.floor((w - k + 2 * p) / s + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_3_class(nn.Module, ConvolutionalNeuralNetwork):\n",
    "    def __init__(self, num_classes = 10\n",
    "                ,kernel_size1=11\n",
    "                ,kernel_size2=2\n",
    "                ,stride=1\n",
    "                ,padding=1\n",
    "                ,number_of_filters0=32\n",
    "                ,number_of_filters1=32\n",
    "                ,length_of_input0=32\n",
    "                ,no_neurons = 128\n",
    "                ,dr=nn.Dropout(p=0)\n",
    "                ,activation_function=torch.relu):\n",
    "        super(CNN_3_class, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, number_of_filters0, kernel_size1, stride, padding)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        length0 = self.Conv2d_output_size(length_of_input0, kernel_size1, stride, padding)//2\n",
    "        self.conv2 = nn.Conv2d(number_of_filters0, number_of_filters1, kernel_size2, stride, padding)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        length1 = self.Conv2d_output_size(length0, kernel_size2, stride, padding)//2\n",
    "        self.fc1 = nn.Linear(int(length1*length1*number_of_filters1), no_neurons)\n",
    "        self.fc2 = nn.Linear(no_neurons, num_classes)\n",
    "        self.dr = dr\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.activation_function(self.conv1(x)))\n",
    "        x = self.pool2(self.activation_function(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation_function(self.fc1(x))\n",
    "        x = self.dr(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar10 datasets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ds_own.cifar_train\n",
    "val_dataet = ds_own.cifar_val\n",
    "test_loader = ds_own.val_loader\n",
    "train_loader = ds_own.train_loader\n",
    "\n",
    "cifar_basic_aug = ds_own.cifar_basic_aug\n",
    "cifar_mixup = ds_own.cifar_mixup\n",
    "cifar_cutout = ds_own.cifar_cutout\n",
    "\n",
    "basic_aug_loader = ds_own.basic_aug_loader\n",
    "mixup_loader = ds_own.mixup_loader\n",
    "cutout_loader = ds_own.cutout_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 68/1250 [00:02<00:47, 24.95it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mikol\\Desktop\\Deep_Learning\\CNN.ipynb Cell 7\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Deep_Learning/CNN.ipynb#Y143sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m train_losses \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Deep_Learning/CNN.ipynb#Y143sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m train_accuracies \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Deep_Learning/CNN.ipynb#Y143sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Deep_Learning/CNN.ipynb#Y143sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_step(data, optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mikol/Desktop/Deep_Learning/CNN.ipynb#Y143sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(results[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\mikol\\Desktop\\Deep_Learning\\datasets.py:62\u001b[0m, in \u001b[0;36mCifarDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     59\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels\u001b[39m.\u001b[39miloc[index, \u001b[39m1\u001b[39m]]\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     64\u001b[0m \u001b[39mreturn\u001b[39;00m (img, label)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "# batch_size = 64 TEGO NIE UZYWAMY A JESLI CHCEMY TO W LOADERZE TRZEBA DODAC\n",
    "num_epochs = 2\n",
    "optimizer = optim.Adam\n",
    "activation_func = nn.ReLU()\n",
    "# dropout_rate = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create model, criterion, and optimizer\n",
    "model = CNN_3_class(num_classes = 10\n",
    "                    ,kernel_size1=7\n",
    "                    ,kernel_size2=3\n",
    "                    ,stride=1\n",
    "                    ,padding=1\n",
    "                    ,number_of_filters0=32\n",
    "                    ,number_of_filters1=32\n",
    "                    ,length_of_input0=32\n",
    "                    ,no_neurons = 128\n",
    "                    ,dr=nn.Dropout(p=0)\n",
    "                    ,activation_function=torch.relu)\n",
    "                    \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "epochs = num_epochs\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    for data in tqdm(train_loader):\n",
    "        results = model.train_step(data, optimizer, criterion)\n",
    "        train_losses.append(results['loss'].item())\n",
    "        train_accuracies.append(results['accuracy'].item())\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_train_accuracy = sum(train_accuracies) / len(train_accuracies)\n",
    "    print(f'Train loss: {avg_train_loss:.4f}, Train accuracy: {avg_train_accuracy:.4f}')\n",
    "\n",
    "    # Test the model\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(test_loader):\n",
    "            results = model.test_step(data, criterion)\n",
    "            val_losses.append(results['loss'].item())\n",
    "            val_accuracies.append(results['accuracy'].item())\n",
    "\n",
    "    # Calculate average test loss and accuracy for the epoch\n",
    "    avg_validation_loss = sum(val_losses) / len(val_losses)\n",
    "    avg_validation_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
    "    print(f'Validation loss: {avg_validation_loss:.4f}, Validation accuracy: {avg_validation_accuracy:.4f}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN from article about weighted random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MyCNN(nn.Module, ConvolutionalNeuralNetwork):\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=736, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=736, out_channels=508, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=508, out_channels=664, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=664, out_channels=916, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv5 = nn.Conv2d(in_channels=916, out_channels=186, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(in_channels=186, out_channels=352, kernel_size=3, padding=1)\n",
    "        self.linear = nn.Linear(in_features=22528, out_features=1229)\n",
    "        self.output = nn.Linear(in_features=1229, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv3(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.conv5(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another aproach to alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PretrainedAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.alexnet(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the last fully connected layer to output num_classes\n",
    "        self.model.classifier[-1] = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import CifarDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and validation directories\n",
    "TRAIN_DIR = 'Cifar10\\\\train'\n",
    "VAL_DIR = 'Cifar10\\\\val'\n",
    "\n",
    "# Path to dataframe with labels for training and validation data\n",
    "TRAIN_LABELS = 'Cifar10\\\\trainLabels.csv'\n",
    "VAL_LABELS = 'Cifar10\\\\valLabels.csv'\n",
    "\n",
    "# List of class names\n",
    "CLASS_NAMES = ['frog', 'truck', 'deer', 'automobile', 'bird', 'horse', 'ship', 'cat', 'dog',\n",
    " 'airplane']\n",
    "\n",
    "# Dictionary for encoding class names\n",
    "CLASS_DICT = {CLASS_NAMES[i]: i for i in range(len(CLASS_NAMES))}\n",
    "\n",
    "# Size in pixels of single image\n",
    "IMG_SIZE=32\n",
    "\n",
    "# import transforms\n",
    "import torchvision.transforms as transforms\n",
    "transformer = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "\n",
    "# Create pyTorch Dataset instances of training and validation data\n",
    "cifar_train = CifarDataset(root_dir = TRAIN_DIR, labels=TRAIN_LABELS, \n",
    "                           transform=transformer, class_dict=CLASS_DICT)\n",
    "cifar_val = CifarDataset(root_dir = VAL_DIR, labels=VAL_LABELS, \n",
    "                         transform=transformer, class_dict=CLASS_DICT)\n",
    "\n",
    "train_loader = DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(cifar_val, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Prepare CIFAR10 dataset and dataloaders\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = PretrainedAlexNet(num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 651/1250 [07:19<07:05,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# print('1')\n",
    "a = 0\n",
    "# Train the model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    a = 0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        except:\n",
    "            a += 1\n",
    "            continue\n",
    "    print('Epoch {} loss: {:.3f}'.format(epoch+1, running_loss/len(train_loader)))\n",
    "    print('Finished Training, {} errors'.format(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "    print('Test accuracy: {:.2f}%'.format(100 * total_correct / total_samples))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PretrainedVGG16(nn.Module):\n",
    "    def __init__(self, num_classes=10, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = models.vgg16(pretrained=pretrained)\n",
    "        \n",
    "        # Modify the last fully connected layer to output num_classes\n",
    "        self.model.classifier[-1] = nn.Linear(4096, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import CifarDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to training and validation directories\n",
    "TRAIN_DIR = 'Cifar10\\\\train'\n",
    "VAL_DIR = 'Cifar10\\\\val'\n",
    "\n",
    "# Path to dataframe with labels for training and validation data\n",
    "TRAIN_LABELS = 'Cifar10\\\\trainLabels.csv'\n",
    "VAL_LABELS = 'Cifar10\\\\valLabels.csv'\n",
    "\n",
    "# List of class names\n",
    "CLASS_NAMES = ['frog', 'truck', 'deer', 'automobile', 'bird', 'horse', 'ship', 'cat', 'dog',\n",
    " 'airplane']\n",
    "\n",
    "# Dictionary for encoding class names\n",
    "CLASS_DICT = {CLASS_NAMES[i]: i for i in range(len(CLASS_NAMES))}\n",
    "\n",
    "# Size in pixels of single image\n",
    "IMG_SIZE=32\n",
    "\n",
    "# import transforms\n",
    "import torchvision.transforms as transforms\n",
    "transformer = transforms.Compose([\n",
    "    transforms.Resize(256),               # Resize the input image to 256x256\n",
    "    transforms.CenterCrop(224),           # Crop the center 224x224 region of the image\n",
    "    transforms.ToTensor(),                # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(                 # Normalize the image\n",
    "        mean=[0.485, 0.456, 0.406],        #   using the standard ImageNet mean and std\n",
    "        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create pyTorch Dataset instances of training and validation data\n",
    "cifar_train = CifarDataset(root_dir = TRAIN_DIR, labels=TRAIN_LABELS, \n",
    "                           transform=transformer, class_dict=CLASS_DICT)\n",
    "cifar_val = CifarDataset(root_dir = VAL_DIR, labels=VAL_LABELS, \n",
    "                         transform=transformer, class_dict=CLASS_DICT)\n",
    "\n",
    "train_loader = DataLoader(cifar_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(cifar_val, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mikol\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\mikol/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n",
      "100%|██████████| 528M/528M [00:11<00:00, 47.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize model and move to device\n",
    "model = PretrainedVGG16(num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 53/1250 [13:45<3:19:25, 10.00s/it]  "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# print('1')\n",
    "a = 0\n",
    "# Train the model\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    a = 0\n",
    "    for i, data in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        try:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        except:\n",
    "            a += 1\n",
    "            continue\n",
    "    print('Epoch {} loss: {:.3f}'.format(epoch+1, running_loss/len(train_loader)))\n",
    "    print('Finished Training, {} errors'.format(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
